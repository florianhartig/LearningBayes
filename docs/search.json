[
  {
    "objectID": "2B-PosteriorEstimation.html#fitting-a-linear-regression-with-different-mcmcs",
    "href": "2B-PosteriorEstimation.html#fitting-a-linear-regression-with-different-mcmcs",
    "title": "3  Posterior estimation",
    "section": "3.2 Fitting a linear regression with different MCMCs",
    "text": "3.2 Fitting a linear regression with different MCMCs\nWe will use the dataset airquality, just removing NAs and scaling all variables for convenience\n\nairqualityCleaned = airquality[complete.cases(airquality),]\nairqualityCleaned = data.frame(scale(airqualityCleaned))\n\nOur goal here is to fit the relationship between Ozone and Temperature with a linear regression\n\nplot(Ozone ~ Temp, data = airqualityCleaned)\n\n\n\n\nAs a frequentist, you would do this via\n\nfit &lt;- lm(Ozone ~ Temp, data = airqualityCleaned)\n\nwhich would calculate the MLE an p-values for this model, and you could evaluate and summarize the results of this via\n\nsummary(fit)\nlibrary(effects)\nplot(allEffects(fit, partial.residuals = T))\npar(mfrow = c(2,2))\nplot(fit) # residuals\n\nAs discussed above, as Bayesian, we have will estimate our models usually via MCMC sampling. Below, I show you three appraoches through which you can do this.\n\n3.2.1 Bayesian analysis with brms\nbrms is a package that allows you to specify regression models in the formular syntax that is familiar to you from standard frequentist R function and packages such as lm, lme4, etc. The application is straightforward\nIn the background, brms will translate you command into a STAN model (see below), fit this model, and return the results!\n\nsummary(fit)\n\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: Ozone ~ Temp \n   Data: airqualityCleaned (Number of observations: 111) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nPopulation-Level Effects: \n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept    -0.00      0.07    -0.14     0.14 1.00     3725     2409\nTemp          0.70      0.07     0.56     0.83 1.00     4106     2791\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.73      0.05     0.64     0.84 1.00     3308     2682\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(fit, ask = FALSE)\n\n\n\nplot(conditional_effects(fit), ask = FALSE)\n\n\n\nfit$model # model that is actually fit via \n\n// generated with brms 2.17.0\nfunctions {\n}\ndata {\n  int&lt;lower=1&gt; N;  // total number of observations\n  vector[N] Y;  // response variable\n  int&lt;lower=1&gt; K;  // number of population-level effects\n  matrix[N, K] X;  // population-level design matrix\n  int prior_only;  // should the likelihood be ignored?\n}\ntransformed data {\n  int Kc = K - 1;\n  matrix[N, Kc] Xc;  // centered version of X without an intercept\n  vector[Kc] means_X;  // column means of X before centering\n  for (i in 2:K) {\n    means_X[i - 1] = mean(X[, i]);\n    Xc[, i - 1] = X[, i] - means_X[i - 1];\n  }\n}\nparameters {\n  vector[Kc] b;  // population-level effects\n  real Intercept;  // temporary intercept for centered predictors\n  real&lt;lower=0&gt; sigma;  // dispersion parameter\n}\ntransformed parameters {\n  real lprior = 0;  // prior contributions to the log posterior\n  lprior += student_t_lpdf(Intercept | 3, -0.3, 2.5);\n  lprior += student_t_lpdf(sigma | 3, 0, 2.5)\n    - 1 * student_t_lccdf(0 | 3, 0, 2.5);\n}\nmodel {\n  // likelihood including constants\n  if (!prior_only) {\n    target += normal_id_glm_lpdf(Y | Xc, Intercept, b, sigma);\n  }\n  // priors including constants\n  target += lprior;\n}\ngenerated quantities {\n  // actual population-level intercept\n  real b_Intercept = Intercept - dot_product(means_X, b);\n}\n\npp_check(fit) # residual checks\n\nUsing 10 posterior draws for ppc type 'dens_overlay' by default.\n\n\n\n\n\n\n\n3.2.2 Bayesian analysis with JAGS\nThe general approach in JAGS is to\n\nSet up a list that contains all the necessary data\nWrite the model as a string in the JAGS specific BUGS dialect\nCompile the model and run the MCMC for an adaptation (burn-in) phase\n\n\nlibrary(rjags)\nData = list(y = airqualityCleaned$Ozone, x = airqualityCleaned$Temp, nobs = nrow(airqualityCleaned))\n\nmodelCode = \"\nmodel{\n\n  # Likelihood\n  for(i in 1:nobs){\n    mu[i] &lt;- a*x[i]+ b\n    y[i] ~ dnorm(mu[i],tau) # dnorm in jags parameterizes via precision = 1/sd^2\n  }\n\n  # Prior distributions\n  \n  # For location parameters, normal choice is wide normal\n  a ~ dnorm(0,0.0001)\n  b ~ dnorm(0,0.0001)\n\n  # For scale parameters, normal choice is decaying\n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau) # this line is optional, just in case you want to observe sigma or set sigma (e.g. for inits)\n\n}\n\"\n\n# Specify a function to generate inital values for the parameters \n# (optional, if not provided, will start with the mean of the prior )\ninits.fn &lt;- function() list(a = rnorm(1), b = rnorm(1), tau = 1/runif(1,1,100))\n\n# sets up the model\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, init = inits.fn, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 111\n   Unobserved stochastic nodes: 3\n   Total graph size: 310\n\nInitializing model\n\n# MCMC sample from model\nSamples &lt;- coda.samples(jagsModel, variable.names = c(\"a\",\"b\",\"sigma\"), n.iter = 5000)\n\n# Plot the mcmc chain and the posterior sample \nplot(Samples)\n\n\n\nsummary(Samples)\n\n\nIterations = 1:5000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean      SD  Naive SE Time-series SE\na     0.698556 0.07055 0.0005760      0.0005714\nb     0.001087 0.06872 0.0005611      0.0005611\nsigma 0.724221 0.05136 0.0004194      0.0004313\n\n2. Quantiles for each variable:\n\n         2.5%      25%       50%     75%  97.5%\na      0.5635  0.65237 0.6986556 0.74458 0.8345\nb     -0.1305 -0.04479 0.0006133 0.04712 0.1333\nsigma  0.6350  0.68940 0.7210357 0.75549 0.8301\n\n\n\n\n3.2.3 Bayesian analysis with STAN\nApproach is identical to JAGS just that we have to define all variables in the section data\n\nlibrary(rstan)\n\nstanmodelcode &lt;- \"\n  data {\n    int&lt;lower=0&gt; N;\n    vector[N] x;\n    vector[N] y;\n  }\n  parameters {\n    real alpha;\n    real beta;\n    real&lt;lower=0&gt; sigma;\n  }\n  model {\n    y ~ normal(alpha + beta * x, sigma);\n  }\n\"\n\ndat = list(y = airqualityCleaned$Ozone, x = airqualityCleaned$Temp, N = nrow(airqualityCleaned))\n\nfit &lt;- stan(model_code = stanmodelcode, model_name = \"example\", \n            data = dat, iter = 2012, chains = 3, verbose = TRUE,\n            sample_file = file.path(tempdir(), 'norm.csv')) \n\n\nprint(fit)\n\nInference for Stan model: example.\n3 chains, each with iter=2012; warmup=1006; thin=1; \npost-warmup draws per chain=1006, total post-warmup draws=3018.\n\n        mean se_mean   sd   2.5%    25%    50%    75%  97.5% n_eff Rhat\nalpha   0.00    0.00 0.07  -0.14  -0.05   0.00   0.05   0.14  2759    1\nbeta    0.70    0.00 0.07   0.55   0.65   0.70   0.75   0.84  3024    1\nsigma   0.73    0.00 0.05   0.63   0.69   0.73   0.76   0.84  3339    1\nlp__  -19.79    0.04 1.33 -23.41 -20.31 -19.42 -18.84 -18.30  1346    1\n\nSamples were drawn using NUTS(diag_e) at Tue Jul  2 11:25:51 2024.\nFor each parameter, n_eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor on split chains (at \nconvergence, Rhat=1).\n\nplot(fit)\n\nci_level: 0.8 (80% intervals)\n\n\nouter_level: 0.95 (95% intervals)\n\n\n\n\nrstan::traceplot(fit)\n\n\n\n\n\n\n3.2.4 Bayesian analysis via BayesianTools\nHere, we don’t use a model specification language, but just write out the likelihood as an standard R function. The same can be done for the prior. For simplicity, in this case I just used flat priors using the lower / upper arguments.\n\nlibrary(BayesianTools)\n\nlikelihood &lt;- function(par){\n  a0 = par[1]\n  a1 = par[2]\n  sigma &lt;- par[3]  \n  logLikel = sum(dnorm(a0 + a1 * airqualityCleaned$Temp  - airqualityCleaned$Ozone , sd = sigma, log = T))\n  return(logLikel)\n}\n\nsetup &lt;- createBayesianSetup(likelihood = likelihood, lower = c(-10,-10,0.01), upper = c(10,10,10), names = c(\"a0\", \"a1\", \"sigma\"))\n\nout &lt;- runMCMC(setup)\n\n\nplot(out)\n\n\n\nsummary(out, start = 1000)\n\n# # # # # # # # # # # # # # # # # # # # # # # # # \n## MCMC chain summary ## \n# # # # # # # # # # # # # # # # # # # # # # # # # \n \n# MCMC sampler:  DEzs \n# Nr. Chains:  3 \n# Iterations per chain:  2335 \n# Rejection rate:  0.777 \n# Effective sample size:  454 \n# Runtime:  0.929  sec. \n \n# Parameters\n        psf   MAP   2.5% median 97.5%\na0    1.006 0.003 -0.143  0.003 0.144\na1    1.012 0.702  0.560  0.692 0.822\nsigma 1.005 0.709  0.635  0.722 0.828\n\n## DIC:  270.119 \n## Convergence \n Gelman Rubin multivariate psrf:"
  },
  {
    "objectID": "2B-PosteriorEstimation.html#checking-and-expecting-the-results",
    "href": "2B-PosteriorEstimation.html#checking-and-expecting-the-results",
    "title": "3  Posterior estimation",
    "section": "3.3 Checking and expecting the results",
    "text": "3.3 Checking and expecting the results\nRunning the sampler again\n\nSamples &lt;- coda.samples(jagsModel, variable.names = c(\"a\",\"b\",\"sigma\"), n.iter = 5000)\n\n\n3.3.1 Convergence checks\nExcept for details in the syntax, the following is more or less the same for all samplers.\nFirst thing should always be convergence checks. Visual look at the trace plots,\n\nplot(Samples)\n\n\n\n\nWe want to look at\n\nConvergence to the right parameter area (seems immediate, else you will see a slow move of the parameters in the traceplot). You should set burn-in after you have converged to the right area\nMixing: low autocorrelation in the chain after convergence to target area (seems excellent in this case)\n\nFurther convergence checks should be done AFTER removing burn-in\n\ncoda::acfplot(Samples)\n\n\n\n\nFormal convergence diagnostics via\n\ncoda::gelman.diag(Samples)\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\na              1          1\nb              1          1\nsigma          1          1\n\nMultivariate psrf\n\n1\n\ncoda::gelman.plot(Samples)\n\n\n\n\nNo fixed rule but typically people require univariate psrf &lt; 1.05 or &lt; 1.1 and multivariate psrf &lt; 1.1 or 1.2\n\n\n\n\n\n\nCaution\n\n\n\nNote that the msrf rule was made for estimating the mean / median. If you want to estimate more unstable statistics, e.g. higher quantiles or other values such as the MAP or the DIC (see section on model selection), you may have to run the MCMC chain much longer to get stable outputs.\n\n  library(BayesianTools)\n  bayesianSetup &lt;- createBayesianSetup(likelihood = testDensityNormal, \n                                       prior = createUniformPrior(lower = -10,\n                                                                  upper = 10))\n  out = runMCMC(bayesianSetup = bayesianSetup, settings = list(iterations = 3000))\n\nThe plotDiagnostics function in package BT shows us how statistics develop over time\n\nplotDiagnostic(out)\n\n\n\n\n\n\n\n\n3.3.2 Summary Table\n\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n            Mean      SD  Naive SE Time-series SE\na      6.994e-01 0.06955 0.0005678      0.0005723\nb     -3.372e-05 0.06891 0.0005626      0.0005626\nsigma  7.253e-01 0.05011 0.0004091      0.0004212\n\n2. Quantiles for each variable:\n\n         2.5%      25%        50%     75%  97.5%\na      0.5635  0.65303  0.6989581 0.74574 0.8366\nb     -0.1337 -0.04608 -0.0004285 0.04616 0.1370\nsigma  0.6355  0.68992  0.7219566 0.75761 0.8316\n\n\nHighest Posterior Density intervals\n\nHPDinterval(Samples)\n\n[[1]]\n           lower     upper\na      0.5684561 0.8435704\nb     -0.1378257 0.1334005\nsigma  0.6342108 0.8268713\nattr(,\"Probability\")\n[1] 0.95\n\n[[2]]\n           lower     upper\na      0.5636098 0.8309431\nb     -0.1365890 0.1344200\nsigma  0.6303167 0.8242163\nattr(,\"Probability\")\n[1] 0.95\n\n[[3]]\n           lower     upper\na      0.5657504 0.8386521\nb     -0.1290830 0.1373207\nsigma  0.6260861 0.8223699\nattr(,\"Probability\")\n[1] 0.95\n\n\n\n\n3.3.3 Plots\nMarginal plots show the parameter distribution (these were also created in the standard coda traceplots)\n\nBayesianTools::marginalPlot(Samples)\n\n\n\n\nPair correlation plots show 2nd order correlations\n\n# coda\ncoda::crosscorr.plot(Samples)\n\n\n\n#BayesianTools\ncorrelationPlot(Samples)\n\n\n\n\n\n\n3.3.4 Posterior predictive distribution\n\ndat = as.data.frame(Data)[,1:2]\ndat = dat[order(dat$x),]\n# raw data\nplot(dat[,2], dat[,1])\n\n# extract 1000 parameters from posterior from package BayesianTools\nx = getSample(Samples, start = 300)\npred = x[,2] + dat[,2] %o% x[,1] \nlines(dat[,2], apply(pred, 1, median))\nlines(dat[,2], apply(pred, 1, quantile, probs = 0.2), \n      lty = 2, col = \"red\")\nlines(dat[,2], apply(pred, 1, quantile, probs = 0.8), \n      lty = 2, col = \"red\")\n\n\n\n# alternative: plot all 1000 predictions in transparent color\nplot(dat[,2], dat[,1])\nfor(i in 1:nrow(x)) lines(dat[,2], pred[,i], col = \"#0000EE03\")\n\n\n\n# important point - so far, we have plotted \n# in frequentist, this is know as the confidence vs. the prediction distribution\n# in the second case, we add th\n\n\npred = x[,2] + dat[,2] %o% x[,1] \nfor(i in 1:nrow(x))  {\n  pred[,i] = pred[,i] + rnorm(length(pred[,i]), 0, sd = x[i,3])\n}\n\nplot(dat[,2], dat[,1])\nlines(dat[,2], apply(pred, 1, median))\nlines(dat[,2], apply(pred, 1, quantile, probs = 0.2), lty = 2, col = \"red\")\nlines(dat[,2], apply(pred, 1, quantile, probs = 0.8), lty = 2, col = \"red\")\n\n#alternative plotting\npolygon(x = c(dat[,2], rev(dat[,2])), \n        y = c(apply(pred, 1, quantile, probs = 0.2), \n              rev(apply(pred, 1, quantile, probs = 0.8))), \n        col = \"#EE000020\")"
  },
  {
    "objectID": "2B-PosteriorEstimation.html#playing-around-with-the-pipeline",
    "href": "2B-PosteriorEstimation.html#playing-around-with-the-pipeline",
    "title": "3  Posterior estimation",
    "section": "3.4 Playing around with the pipeline",
    "text": "3.4 Playing around with the pipeline\n\n3.4.1 Prior choice\nPriors are not scale-free. What that means: dnorm(0,0.0001) might not be an uninformative prior, if the data scale is extremely small so that you might expect huge effect sizes - scaling all variables makes sure we have a good intuition of what “uninformative means”.\nTask: play with the following minimal script for a linear regression to understand how scaling parameter affects priors and thus posterior shapes. In particular, change\n\nMultiply Ozone by 1000000 -&gt; will push sd estimates high\nMultiply Temp by 0.0000001 -&gt; will push parameter estimates high\n\nThen compare Bayesian parameter estimates and their uncertainty to Bayesian estimates. How would you have to change the priors to fix this problem and keep them uninformative?\nTask: 2 implement mildly informative priors as well as strong shrinkage priors in the regression. Question to discuss: should you put the shrinkage also in the intercept? Why should you center center variables if you include a shrinkage prior on the intercept?\n\nlibrary(rjags)\n\ndat = airquality[complete.cases(airquality),] \n# scaling happens here - change \ndat$Ozone = as.vector(scale(dat$Ozone))\ndat$Temp = as.vector(scale(dat$Temp)) \n\n\nData = list(y = dat$Ozone, \n            x = dat$Temp, \n            i.max = nrow(dat))\n\n# Model\nmodelCode = \"\nmodel{\n\n  # Likelihood\n  for(i in 1:i.max){\n    mu[i] &lt;- Temp*x[i]+ intercept\n    y[i] ~ dnorm(mu[i],tau)\n  }\n\n  # Prior distributions\n  \n  # For location parameters, typical choice is wide normal\n  intercept ~ dnorm(0,0.0001)\n  Temp ~ dnorm(0,0.0001)\n\n  # For scale parameters, typical choice is decaying\n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau) # this line is optional, just in case you want to observe sigma or set sigma (e.g. for inits)\n\n}\n\"\n\n# Specify a function to generate inital values for the parameters (optional, if not provided, will start with the mean of the prior )\ninits.fn &lt;- function() list(a = rnorm(1), b = rnorm(1), \n                            tau = 1/runif(1,1,100))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, init = inits.fn, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 111\n   Unobserved stochastic nodes: 3\n   Total graph size: 310\n\n\nWarning in jags.model(file = textConnection(modelCode), data = Data, init =\ninits.fn, : Unused initial value for \"a\" in chain 1\n\n\nWarning in jags.model(file = textConnection(modelCode), data = Data, init =\ninits.fn, : Unused initial value for \"b\" in chain 1\n\n\nWarning in jags.model(file = textConnection(modelCode), data = Data, init =\ninits.fn, : Unused initial value for \"a\" in chain 2\n\n\nWarning in jags.model(file = textConnection(modelCode), data = Data, init =\ninits.fn, : Unused initial value for \"b\" in chain 2\n\n\nWarning in jags.model(file = textConnection(modelCode), data = Data, init =\ninits.fn, : Unused initial value for \"a\" in chain 3\n\n\nWarning in jags.model(file = textConnection(modelCode), data = Data, init =\ninits.fn, : Unused initial value for \"b\" in chain 3\n\n\nInitializing model\n\n# Run a bit to have a burn-in\nupdate(jagsModel, n.iter = 1000)\n\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel, variable.names = c(\"intercept\",\"Temp\",\"sigma\"), n.iter = 5000)\n\n\n# Bayesian results\nsummary(Samples)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                Mean      SD  Naive SE Time-series SE\nTemp       0.6984643 0.06891 0.0005627      0.0005679\nintercept -0.0002989 0.06872 0.0005611      0.0005611\nsigma      0.7243984 0.04956 0.0004046      0.0004088\n\n2. Quantiles for each variable:\n\n             2.5%      25%       50%     75%  97.5%\nTemp       0.5628  0.65241 0.6987757 0.74428 0.8339\nintercept -0.1363 -0.04603 0.0003719 0.04624 0.1342\nsigma      0.6358  0.69009 0.7212351 0.75629 0.8286\n\n# MCMC results\nfit &lt;- lm(Ozone ~ Temp, data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1.2298 -0.5247 -0.0263  0.3138  3.5485 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 1.232e-17  6.823e-02    0.00        1    \nTemp        6.985e-01  6.854e-02   10.19   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7188 on 109 degrees of freedom\nMultiple R-squared:  0.488, Adjusted R-squared:  0.4833 \nF-statistic: 103.9 on 1 and 109 DF,  p-value: &lt; 2.2e-16\n\n\n\n\n3.4.2 Missing data\nIn the analysis above, we removed missing data. What happens if you are leaving the missing data in in a Jags model? Try it out and discuss what happens"
  },
  {
    "objectID": "1A-GettingStarted.html#organization-of-this-book",
    "href": "1A-GettingStarted.html#organization-of-this-book",
    "title": "1  Getting Started",
    "section": "1.1 Organization of this book",
    "text": "1.1 Organization of this book\nThe aim of this book is to introduce you to the main techniques and concepts that are used when performing regression analyses in applied settings. This book is organized in three parts:\n\nIntroduction and philosophy: The first part of this book provides a general introduction to Bayesian inference, starting with the internal logic (likelihood, prior, posterior), a short introduction on posterior estimation and interpretation, a section on Bayesian model selection and a overview of the Bayesian workflow\nBayesian GLMMs: The second part covers how standard GLMMs (which could also be fit in R packages lme4 or glmmTMB) would be implemented in a Bayesian worklow\nHierarchical models: The third part of the book shows examples of popular hierarchical model structures that may be the reason why you want to use Bayesian inference."
  },
  {
    "objectID": "1A-GettingStarted.html#your-r-system",
    "href": "1A-GettingStarted.html#your-r-system",
    "title": "1  Getting Started",
    "section": "1.1 Your R System",
    "text": "1.1 Your R System\nIn this course, we work with the combination of R + RStudio.\n\n[R](https://www.r-project.org) is the calculation engine that performs the computations.\n[RStudio](https://posit.co/download/rstudio-desktop/) is the editor that helps you sending inputs to R and collect outputs.\n\nMake sure you have a recent version of R + RStudio installed on your computer. If you have never used RStudio, here is a good video introducing the basic system and how R and RStudio interact."
  },
  {
    "objectID": "1A-GettingStarted.html#libraries-that-you-will-need",
    "href": "1A-GettingStarted.html#libraries-that-you-will-need",
    "title": "1  Getting Started",
    "section": "1.2 Libraries that you will need",
    "text": "1.2 Libraries that you will need\nThe R engine comes with a number of base functions, but one of the great things about R is that you can extend these base functions by libraries that can be programmed by anyone. In principle, you can install libraries from any website or file. In practice, however, most commonly used libraries are distributed via two major repositories. For statistical methods, this is CRAN, and for bioinformatics, this is Bioconductor.\n\n\n\n\n\n\nClick to see more on installing libraries in R\n\n\n\n\n\nTo install a package from a library, use the command\n\ninstall.packages(LIBRARY)\n\nExchange “LIBRARY” with the name of the library you want to install. The default is to search the package in CRAN, but you can specify other repositories or file locations in the function. For Windows / Mac, R should work out of the box. For other UNIX based systems, may also need to install\nbuild-essential\ngfortran\nlibmagick++-dev\nr-base-dev\ncmake\nIf you are new to installing packages on Debian / Ubuntu, etc., type the following:\nsudo apt update && sudo apt install -y --install-recommends build-essential gfortran libmagick++-dev r-base-dev cmake\n\n\n\nIn this book, we will often use data sets from the EcoData package, which is not on CRAN, but on a GitHub page. To install the package from github, first install devtools package (unless you have the devtools package installed already) by running\n\ninstall.packages(\"devtools\")\n\nThen you can use the devtools::install_github function to install the EcoData package via\n\ndevtools::install_github(repo = \"TheoreticalEcology/EcoData\",\n                         dependencies = T, build_vignettes = T)\n\nBesides providing data, the EcoData installation also forces the installation of most of the packages that we need in this book, so this may take a while. If you want to load only the EcoData package (without installing all the other packages), or if you encounter problems during the install, set dependencies = F, build_vignettes = F.\nIn addition to the packages provided in EcoData, to be able to run all examples in the book, please install the following additional packages:\n\ninstall.packages(BayesianTools)\ninstall.packages(rjags)\n\nAdditionally, you have to install the JAGS MCMC sampler, which is a program independent from R. You can find downloads of Jags versions for different operating systems here."
  },
  {
    "objectID": "1A-GettingStarted.html#assumed-r-knowledge",
    "href": "1A-GettingStarted.html#assumed-r-knowledge",
    "title": "1  Getting Started",
    "section": "1.4 Assumed R knowledge",
    "text": "1.4 Assumed R knowledge\nAs mentioned in the preface, this book assumes that you have basic knowledge about data manipulation (reading in data, removing or selecting columns or rows, calculating means per group etc.) and plotting in R. Note that for both purposes, there are currently two main schools in the R environment which do the same things, but with very different syntax:\n\nbase R, which uses functions such as plot(), apply(), aggregate()\ntidyverse, with packages such as dplyr and ggplot2, which provide functions such as mutate(), filter() and heavily rely on the %&gt;% pipe operator.\n\nThere are many opinions about advantages and disadvantages of the two schools. I’m agnostic about this, or more precisely, I think you should get to know both schools and then decide based on the purpose. I see advantages of tidyverse in particular for data manipulation, while I often prefer baseR plots over ggplot2. To keep it simple, however, all code in this course uses base R.\n\n\n\n\n\n\nNote\n\n\n\nThe tidyverse framework is currently trying to expand to the tasks of statistical / machine learning models as well, trying to streamline statistical workflows. While this certainly has a lot of potential, I don’t see it as general / mature enough to recommend it as a default for the statistical workflow.\n\n\nIn the following box, you will find an exercise that asks you to perform basic plots and data manipulations. To text yourself, please check that you can perform these operations. If you have problems, you should study an introductory R course (for example here) before continuing with this text.\n\n\n\n\n\n\nExercise - Data wrangling\n\n\n\nWe work with the airquality dataset:\n\ndat = airquality\n\n\nBefore working with a dataset, you should always get an overview of it. Helpful functions for this are str(), View(), summary(), head(), and tail(). Apply them to dat and make sure to understand what they do.\nWhat is the data type of the variable ‘Month’? Transform it to a factor\nScale the variable Wind and save it as a new variable in dat\nTransform the variable ‘Temp’ (log-transform) and save it as a new variable in dat\nExtract the first 100 rows of dat and remove the NAs from the subsetted dataset\nPlot the variables of the dataset, and against each other (e.g. Wind, Wind vs Temp, Temp vs Month, all simultaneously)\nCalculate correlation indices between the numerical variables (e.g. Wind and Temp, Temp and Ozone). What is the difference between Spearman and Pearson correlation?\n\n\n\n\n\n\n\n\n\nSolution\n\n\n\n\n\n\nstr() helps us to check the data types of the variables, ensure that they are correct, e.g. categorical variables should be factors and continuous variables should be either num (numeric) or int (integer). summary()returns important summary statistics of our variables and informs us about NAs in the data\n\nstr(dat)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : int  5 5 5 5 5 5 5 5 5 5 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\nsummary(dat)\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n\n\n\nThere are NAs in Ozone and Solar.R! Also, Month is not a factor!\nWe have to transform Month into a factor:\n\ndat$Month = as.factor(dat$Month)\nstr(dat)\n\n'data.frame':   153 obs. of  6 variables:\n $ Ozone  : int  41 36 12 18 NA 28 23 19 8 NA ...\n $ Solar.R: int  190 118 149 313 NA NA 299 99 19 194 ...\n $ Wind   : num  7.4 8 12.6 11.5 14.3 14.9 8.6 13.8 20.1 8.6 ...\n $ Temp   : int  67 72 74 62 56 66 65 59 61 69 ...\n $ Month  : Factor w/ 5 levels \"5\",\"6\",\"7\",\"8\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ Day    : int  1 2 3 4 5 6 7 8 9 10 ...\n\n\nScaling means that the variables are centered and standardized (divided by their standard deviation):\n\ndat$sWind = scale(dat$Wind)\nsummary(dat)\n\n     Ozone           Solar.R           Wind             Temp       Month \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00   5:31  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00   6:30  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00   7:31  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88   8:31  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00   9:30  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00         \n NA's   :37       NA's   :7                                              \n      Day             sWind.V1      \n Min.   : 1.0   Min.   :-2.3438868  \n 1st Qu.: 8.0   1st Qu.:-0.7259482  \n Median :16.0   Median :-0.0730957  \n Mean   :15.8   Mean   : 0.0000000  \n 3rd Qu.:23.0   3rd Qu.: 0.4378323  \n Max.   :31.0   Max.   : 3.0492420  \n\n\n\nUse logfunction to transform the variable (be aware of NAs!)\n\ndat$logTemp = log(dat$Temp)\n\nUse [rows, cols] to subset the data and complete.cases() to remove observations with NAs\n\ndat_sub = dat[1:100,]\nsummary(dat_sub)\n\n     Ozone           Solar.R           Wind            Temp       Month \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.70   Min.   :56.00   5:31  \n 1st Qu.: 16.00   1st Qu.:101.0   1st Qu.: 7.40   1st Qu.:69.00   6:30  \n Median : 34.00   Median :223.0   Median : 9.70   Median :79.50   7:31  \n Mean   : 41.59   Mean   :193.3   Mean   :10.07   Mean   :76.87   8: 8  \n 3rd Qu.: 63.00   3rd Qu.:274.0   3rd Qu.:12.00   3rd Qu.:84.00   9: 0  \n Max.   :135.00   Max.   :334.0   Max.   :20.70   Max.   :93.00         \n NA's   :31       NA's   :7                                             \n      Day              sWind.V1          logTemp     \n Min.   : 1.00   Min.   :-2.3438868   Min.   :4.025  \n 1st Qu.: 7.00   1st Qu.:-0.7259482   1st Qu.:4.234  \n Median :14.50   Median :-0.0730957   Median :4.376  \n Mean   :14.93   Mean   : 0.0313607   Mean   :4.334  \n 3rd Qu.:23.00   3rd Qu.: 0.5797567   3rd Qu.:4.431  \n Max.   :31.00   Max.   : 3.0492420   Max.   :4.533  \n\n\ndat_sub = dat_sub[complete.cases(dat_sub),]\nsummary(dat_sub)\n\n     Ozone          Solar.R            Wind            Temp       Month \n Min.   :  1.0   Min.   :  7.00   Min.   : 4.00   Min.   :57.00   5:24  \n 1st Qu.: 16.0   1st Qu.: 97.25   1st Qu.: 7.40   1st Qu.:67.75   6: 9  \n Median : 33.0   Median :223.00   Median : 9.70   Median :81.00   7:26  \n Mean   : 41.5   Mean   :192.53   Mean   :10.15   Mean   :76.61   8: 5  \n 3rd Qu.: 61.5   3rd Qu.:274.25   3rd Qu.:12.00   3rd Qu.:84.25   9: 0  \n Max.   :135.0   Max.   :334.00   Max.   :20.70   Max.   :92.00         \n      Day              sWind.V1          logTemp     \n Min.   : 1.00   Min.   :-1.6910344   Min.   :4.043  \n 1st Qu.: 7.75   1st Qu.:-0.7259482   1st Qu.:4.216  \n Median :15.50   Median :-0.0730957   Median :4.394  \n Mean   :14.97   Mean   : 0.0550798   Mean   :4.330  \n 3rd Qu.:21.00   3rd Qu.: 0.5797567   3rd Qu.:4.434  \n Max.   :31.00   Max.   : 3.0492420   Max.   :4.522  \n\n\nSingle continuous variables can be visualized using a histogram (hist) , for two variables, it depends on their data types:\n\n\n\n\n\n\n\n\nScenario\nWhich plot\nR command\n\n\n\n\nNumeric\nHistogram or boxplot\nhist() andboxplot\n\n\nNumeric with numeric\nScatterplot\nplot\n\n\nNumeric with categorical\nBoxplot\nboxplot(numeric~categorical)\n\n\nCategorical with categorical\nmosaicplot or grouped barplot\nmosaicplot(table(categorical, categorical)) or barplot(data,        beside=TRUE)\n\n\n\n\n# Numeric\nhist(dat$Wind, main = \"Wind\")\n\n\n\n# Numeric vs numeric\nplot(dat$Wind, dat$Solar.R)\n\n\n\n# Numeric with categorical\nboxplot(Wind~Month, data = dat)\n\n\n\n# All with all\npairs(dat)\n\n\n\n\nSpearman is a rank correlation factor, less sensitive against outliers and non-linearity:\n\n# Pearson\ncor(dat$Wind, dat$Temp, use = \"complete.obs\")\n\n[1] -0.4579879\n\n# Spearman\ncor(dat$Wind, dat$Temp, use = \"complete.obs\", method = \"spearman\")\n\n[1] -0.4465408"
  },
  {
    "objectID": "2A-BayesianLogic.html#frequentist-and-bayesian-coin-flip",
    "href": "2A-BayesianLogic.html#frequentist-and-bayesian-coin-flip",
    "title": "2  The Bayesian Logic",
    "section": "2.1 Frequentist and Bayesian coin flip",
    "text": "2.1 Frequentist and Bayesian coin flip\nAssume I’m trying to guess if a coin comes up heads or tails. I have had 10 trials, and 8x success in guessing the side.\n\ntrials = 10\nsuccess = 8\n\nWhat we want to know now is what my properties are regarding correctly guessing the outcome of the coin flip experiment, with the three inferential methods in statistics: MLE, NHST and Bayes.\nFor all three statistical methods, we use the same statistical model which is the binomial model. The probability density is available in R throught he function dbinom. For exammple, dbinom(6,10,0.9) gives you the probability of obtaining 6/10 successes when the true probability of heads is 0.9\n\ndbinom(6,10, 0.0)\n\n[1] 0\n\n\n\n2.1.1 The ML estimator\nThe idea of maximum likelihood estimation (MLE) is to look for the set of parameters that would, under the given model assumption, lead to the highest probability of obtaining the observed data.\nIn our case we have only one parameter, the probability of success per flip, so you can basically read off the MLE by plotting the for different values of this parameter.\n\nparametervalues &lt;- seq(0,1,0.001) # parameters to check\nlikelihood &lt;- dbinom(success,trials,parametervalues) # p(D|oarameters)\n\n# plot results\nplot(parametervalues, likelihood, type = \"l\")\nlegend(\"topleft\", legend = c(\"Likelihood\", \"maximum\"), col = c(\"black\", \"red\"), lwd = 1)\nMLEEstimate &lt;- parametervalues[which.max(likelihood)]\nabline(v=MLEEstimate, col = \"red\")\n\n\n\n\n\n2.1.1.1 Constructing confidence intervals\nOK, the MLE is the best value, but what is often more interesting is the uncertainty around this value. Frequentist CIs are constructed according to the following idea: If I would do the experiment many times, how would the estimate scatter, and how wide would I have to take the interval so that the true value is contained in the interval x% (typically 95%) under repeatedly performing the experiment?\nThe test statistics that can be used to do this are discussed, e.g., in https://onlinecourses.science.psu.edu/stat504/node/39. The result for a 1-parameter model is that the CI is at a log likelihood difference of 1.92\n\nplot(parametervalues, likelihood, type = \"l\")\nlegend(\"topleft\", legend = c(\"Likelihood\", \"maximum\", \"CI\"), col = c(\"black\", \"red\", \"green\"), lwd = 1)\nMLEEstimate &lt;- parametervalues[which.max(likelihood)]\nabline(v=MLEEstimate, col = \"red\")\n\nconfidence.level &lt;- log(max(likelihood)) -1.92\nleftCI &lt;- parametervalues[which.min(abs(log(likelihood[1:which.max(likelihood)]) - confidence.level))]\nabline(v=leftCI, col = \"green\")\nrightCI &lt;- parametervalues[which.min(abs(log(likelihood[which.max(likelihood):length(likelihood)]) - confidence.level)) + which.max(likelihood) -1]\nabline(v=rightCI, col = \"green\")\n\n\n\n\nNote: there are also other methods to look at uncertainty with likelihoods, e.g. the profile likelihood, see discussion here\n\n\n\n\n\n\nSummary MLE\n\n\n\n\nBest estimate (MLE)\n95% CI –&gt; if we would do the experiment over and over again, 95% of the CIs would contain the true value. NOTE: this is != saying: for a given dataset, the true value is in the CI with 95% probability!\n\n\n\n\n\n\n2.1.2 Getting the p-value for a fair coin\nwant to get p-value for a smaller or equal result (1-tailed) given a fair coin p(k&lt;=kobs|H0:p=0.5). Basically, we want the sum over the red bars\n\nbarplot(dbinom(0:10, 10, 0.5), col = c(rep(\"grey\", success ), rep(\"red\", 11-success)))\n\n\n\nline(pbinom(0:10,trials,prob = 0.5, lower.tail = F))\n\n\nCall:\nline(pbinom(0:10, trials, prob = 0.5, lower.tail = F))\n\nCoefficients:\n[1]   1.2640  -0.1373\n\n\nWe can get this with the cummulative distribution function in R\n\npValue &lt;- pbinom(success,trials,prob = 0.5, lower.tail = F)\n\nbut it is a bit tricky, because depeding on which side one wants to test, you have to add a -1 to the successes becaues of the discrete nature of teh data and the definition of the cummulative in R. You can try, but it’s safer in practice to use the binom.test, which calculates the same values\n\nbinom.test(7,trials,0.5) # two sided \n\n\n    Exact binomial test\n\ndata:  7 and trials\nnumber of successes = 7, number of trials = 10, p-value = 0.3438\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.3475471 0.9332605\nsample estimates:\nprobability of success \n                   0.7 \n\n\nAlternatively:\n\nbinom.test(7,trials,0.5, alternative=\"greater\") # testing for greater\nbinom.test(7,trials,0.5, alternative=\"less\") # testing for less\n\n\n\n\n\n\n\nSide-note: multiple testing\n\n\n\n\n\nImagine there is no effect, but we keep on repeating the test 100 times. How often do you think will we find a significant effect?\n\ndata= rbinom(100,10,0.5)\npValue &lt;- pbinom(data,trials,prob = 0.5, lower.tail = F)\nsum(pValue &lt; 0.05)\n\n[1] 7\n\n\nYes, 5 is what you expect. To be exact, in the case of discrete random distributions, the value doesn’t have to be exactly 5%, but that is a side topic, and here it works.\nThe message here is: if you do repeated tests, and you want to maintain a fixed overall type I error rate, you need to adjust the p-values, e.g. by\n\npValueAdjusted &lt;- p.adjust(pValue, method = \"hochberg\")\nsum(pValueAdjusted &lt; 0.05)\n\n[1] 0\n\n\nRemember: in general, if you choose an alpha level of 5%, and you have absolutely random data, you should get 5% false positives (type I error) asymptotically, and the distribution of p-values in repeated experiments will be flat.\n\n\n\n\n\n\n\n\n\nSummary NHST\n\n\n\n\np-value –&gt; probability to see the observed or more extreme data given the null hypothesis\nrejection H0 if p &lt; alpha. If p &gt; alpha, the test ist inconclusive\nif you do multiple tests, you may want to adjust the p-values\n\nAlso note: we are free to choose the null-hypothesis as we want. What would you do if you null hypothesis is that a coin should have an 0.8 probability of head?\n\n\n\n\n2.1.3 The Bayesian posterior estimate\nRemember for Bayes p(M|D) = p(D|M) * p(M) / P(D), and we can show that p(D) is just the integral over p(D|M) * p(M)\nWe had already calculated p(D|M), so we just need to define p(M), the prior. For the moment, we will use a flat prior, but see the comments on prior choice later in the book - for a bernoulli trial often other priors, in particular the beta distribution, are used.\n\nprior &lt;- rep(1,1001)\nposterior &lt;- likelihood * prior / sum(likelihood * prior) * length(parametervalues)\n\nplot(parametervalues, posterior, col = \"darkgreen\", type = \"l\")\nlines(parametervalues, likelihood)\nlines(parametervalues, prior, col = \"red\" )\nlegend(\"topright\", c(\"likelihood\", \"prior\", \"posterior\"), col = c(\"black\", \"red\", \"green\"), lwd = 1 )\n\n\n\n\nyou see that likelihood and posterior have the same shape. However, this is only because I chose a flat prior. There is still a difference, however, namely that the posterior is normalized, i.e. will integrate to one. It has to be, because we want to interpret it as a pdf, while the likelihood is not a pdf. Let’s look at the same example for an informative prior\n\nprior &lt;- dnorm(parametervalues, mean = 0.5, sd = 0.1)\nposterior &lt;- likelihood * prior / sum(likelihood * prior) * length(parametervalues)\n\nplot(parametervalues, posterior, col = \"darkgreen\", type = \"l\")\nlines(parametervalues, likelihood)\nlines(parametervalues, prior, col = \"red\" )\nlegend(\"topright\", c(\"likelihood\", \"prior\", \"posterior\"), col = c(\"black\", \"red\", \"green\"), lwd = 1 )\n\n\n\n\nyou can see that the likelihood moves the posterior away from the prior, but not by much. try the same think with more data, but the same ratio, i.e. change to 30 trials, 9 success\n\n\n\n\n\n\nSummary Bayes\n\n\n\nDistribution in, distribution out: prior * likelihood = posterior"
  },
  {
    "objectID": "2A-BayesianLogic.html#interpreting-the-posterior",
    "href": "2A-BayesianLogic.html#interpreting-the-posterior",
    "title": "2  The Bayesian Logic",
    "section": "2.2 Interpreting the Posterior",
    "text": "2.2 Interpreting the Posterior\nIn standard statistics, we are used to interpret search for the point that maximizes p(D|phi), and interpret this as the most likely value.\n\nparameter = seq(-5,5,len=500)\nlikelihood = dnorm(parameter) + dnorm(parameter, mean = 2.5, sd=0.5)\n\nplot(parameter,likelihood, type = \"l\")\n\nMLEEstimate &lt;- parameter[which.max(likelihood)]\nabline(v=MLEEstimate, col = \"red\")\ntext(2.5,0.8, \"MLE\", col = \"red\")\n\n\n\n\nAssume the prior is flat, then we get the posterior simply by normalization\n\nunnormalizedPosterior = likelihood * 1 \nposterior = unnormalizedPosterior / sum(unnormalizedPosterior/50) \n\nIn Bayesian statistics, the primary outcome of the inference is the whole distribution.\n\nplot(parameter,posterior, type = \"l\")\npolygon(parameter, posterior, border=NA, col=\"darksalmon\")\n\n\n\n\nIf we don’t have to, this is what we should interpret and forecast with. However, in many cases, people what to summarize this distribution by particular values. Here is what you typically use for different situations\n\n2.2.1 The MAP (mode of posterior)\nIf you want to have the most probable parameter value, what you can do is to use the mode of the posterior distribution. It is called the maximum a posteriori probability (MAP) estimate.\n\n\n2.2.2 Posterior median or mean\nAlthough the MAP is very intutive, it has several problems, which is the reason why it is rarely used in practice.\nThe first reason is computational: with the MCMC methods that are used in practice to estimate posterior distributions (see next section), it is very hard to locate the exact position of the MAP in posterior space.\nThe second reason is that Bayesian methods are often used for complicated models and in non-assymptotic cases. In such a situation, the likelihood or posterior often has weird, non-normal shapes. If the posterior distribution is very skewed as in our example, it may well be that the MAP doesn’t really give a good idea of where most probability mass is.\nFor both reasons, mean or median posterior values are the most common choices if the goal is to summarize the posterior distribution by one value.\n\nplot(parameter,posterior, type = \"l\")\npolygon(parameter, posterior, border=NA, col=\"darksalmon\")\n\nmedianPosterior &lt;- parameter[min(which(cumsum(posterior) &gt; 0.5 * 50))]\nabline(v=medianPosterior, col = \"blue\")\ntext(2.9,0.3, \"Median\", col = \"blue\")\n\nmean &lt;- mean(posterior * parameter) / mean(posterior)\nabline(v=mean, col = \"darkgreen\")\ntext(0.4,0.25, \"Mean\", col = \"darkgreen\")\n\n\n\n\n\n\n2.2.3 Bayesian credibile intervals\nTypically, one also wants uncertainties. There basic option to do this is the Bayesian credible interval, which is the analogue to the frequentist confidence interval. The 95 % Bayesian Credibility interval is the centra 95% of the posterior distribution\n\nplot(parameter,posterior, type = \"l\")\n\n\nlowerCI &lt;- min(which(cumsum(posterior) &gt; 0.025 * 50))\nupperCI &lt;- min(which(cumsum(posterior) &gt; 0.975 * 50))\n\npar = parameter[c(lowerCI, lowerCI:upperCI, upperCI)]\npost = c(0, posterior[lowerCI:upperCI], 0)\n\npolygon(par, post, border=NA, col=\"darksalmon\")\n\ntext(0.75,0.07, \"95 % Credibile\\n Interval\")\n\n\n\n\n\n\n2.2.4 HPD and LPL\nThere are two alternatives to the credibility interval that is particularly useful if the posterior has weird correlation structres.\n\nThe Highest Posterior Density (HPD). The HPD is the x% highest posterior density interval is the shortest interval in parameter space that contains x% of the posterior probability. It would be a bit cumbersome to calculate this in this example, but if you have an MCMC sample, you get the HPD with the package coda via\n\n\nHPDinterval(obj, prob = 0.95, ...)\n\n\nThe Lowest Posterior Loss (LPL) interval, which considers also the prior.\n\nMore on both alternatives here.\nMore options to plot HPD in 2-d here http://www.sumsar.net/blog/2014/11/how-to-summarize-a-2d-posterior-using-a-highest-density-ellipse/\n\n\n2.2.5 Multivariate issues\nThings are always getting more difficult if you move to more dimensions, and Bayesian analysis is no exception.\n\n2.2.5.1 Marginal values hide correlations\nA problem that often occurs when we have more than one parameter are correlations between parameters. In this case, the marginal posterior distributions that are reported in the summary() or plot functions of coda can be VERY misleading.\nLook at the situation below, where we have two parameters that are highly correlated. The marginal posteriors look basically flat, and looking only at them you may think there is no information in the likelihood.\nHowever, if you look at the correlation, you see that the likelihood has excluded vast areas of the prior space (assuming we have had flat uncorrelated likelihoods in this case).\n\nlibrary(psych)\npar1= runif(1000,0,1)\npar2 =par1 + rnorm(1000,sd = 0.05)\nscatter.hist(par1,par2)\n\n\n\n\nIt is therefore vital to plot the correlation plots as well to be able to judge the extent to which parameters are uncertaint.\nIf you have more parameters, however, you may still miss things here, because there could be higher-order correlations between the parameters that look random in the two-dimensional plot. A good proxy to get an overall reduction of uncertainy across all parameters, including all these higher-order correlations, is to compare the prior predictive distribution with the posterior predictive distribution.\n\n\n2.2.5.2 Nonlinear correlations\nA further issue that many people are not aware of is that the marginal mode (maximum) does not need to coincide with the global mode if correlations in parameter space are nonlinear. Assume we have a posterior with 2 parameters, which are in a complcated, banana-shaped correlation. Assume we are able to sample from this poterior. Here is an example from Meng and Barnard, code from the bayesm package (see Rmd source file for code of this function).\nIf we plot the correlation, as well as the marginal distributions (i.e. the histograms for each parameter), you see that the mode of the marginal distributions will not conincide with the multivariate mode (red, solid lines).\n\nset.seed(124)\nsample=banana(A=0.5,B=0,C1=3,C2=3,50000)\nscatterhist(sample[,1], sample[,2])\n\n\n\n#abline(h = 0.22, col = \"green\", lwd = 3, lty =2)\n#abline(v = 0.295, col = \"green\", lwd = 3, lty =2)\n\nHence, it’s important to note that the marginal distributions are not suited to calculate the MAP, CIs, HPDs or any other summary statistics if the posterior distribution is not symmetric in multivariate space. This is a real point of confusion for many people, so keep it in mind!"
  },
  {
    "objectID": "2A-BayesianLogic.html#prior-choice",
    "href": "2A-BayesianLogic.html#prior-choice",
    "title": "2  The Bayesian Logic",
    "section": "2.3 Prior Choice",
    "text": "2.3 Prior Choice\nThe choice of prior (prior elicitation) is key to Bayesian analysis, and it is arguably the most contentious step in the whole procedure, as it supposedly contains “subjective” judgement. I disagree with this notion. The choice of a prior is not necessarily subjective. It simply means that, unlike in a frequentist analysis, we should generally collect everything that is known about a parameter in advance, which may be done in an objective way. Also, we can try to avoid the inclusion of prior knowledge by choosing so-called uninformative (aka vague, reference) priors. So, a first thing to note about priors is that we have\n\nInformative priors that express prior knowledge about an inferential question\nUninformative priors that express no prior knowledge about an inferential question\n\nMore about the choice of uninformative priors below. But first some other statements:\n\nIn the limit if infinitely many data, the likelihood gets infinitely sharp, and therefore the prior choice irrelevant (as long as the prior is not 0 anywhere there is likelihood)\nPriors are therefore most important if you have a small dataset\nPriors are changed by rescaling parameters (see below)\nUninformative priors are not always flat (see below). For common problems, people have developed recommendations for which priors should be used in an uninformative setting\n\n\n2.3.1 Scaling and scale-invariance of prior choices\nScaling is key to understand why uninformative priors can’t always be flat. Imagine the following situation: we have a dataset on average tree diameters, and we want to infer the average with a Bayesian method. We shouldn’t really look at the data before we specify our prior, so let’s just specify the prior, and assume we choose a flat prior between 1 and 10 because we don’t want to bias our data in any way\n\nvalues = 1:5\npriorWeight = rep(1/5, 5)\nbarplot(priorWeight, names.arg = values, xlab = \"size [cm]\", \n        ylab = \"priorProbability\", col = \"darkseagreen\")\n\n\n\n\nNow, let’s assume that we decide do change the analysis slightly, and measure average size in the basal area, which scales to diameter as x^2. We have already specified our prior knowledge about diameter, so for each cm of diameter we have specified the same weight.\nIf we rescale the x-axis to basal area, the length of each bar on the x-axis changes - large values are getting broader, short values are getting more narrow. If the probability weight is to stay the same, we get the following picture:\n\nbarplot(priorWeight/values^2, width = values^2, names.arg = values^2, \n        xlab = \"size [cm^2]\", ylab = \"priorProbability\", col = \"darkseagreen\")\n\n\n\n\nThe message here is that if we are free to rescale predictors as we want (which is generally true), the prior cannot be flat for all possible parameter transformations. A key for any rule about finding uninformative priors is therefore that the rule must be invariant under parameter transformation. For more on this, see (George and McCulloch 1993).\nA second message is that in Bayesian statistics, you have to be a bit careful about parameter transformations, because we don’t just look at one value, but at a whole distribution, and the shape of this distribution will change of we reparameterize.\n\n\n2.3.2 Default choices for uniformative priors\nSo, what is the right choice for uninformative priors? The somewhat disturbing answer is that there is no generally accepted solution for this problem. One famous proposal that contains many of the desirable properties is Jeffrey’s prior which is defined as\np(phi) ~ sqrt ( det ( F(phi)))\nwhere F(phi) is the Fisher information matrix, which basically tells you how strongly the likelihood changes if parameters change. It is easy to see that the prior choice will then be\n\ninvariant under rescaling parameters\nproportional to how strongly parameters affect the likelihood\n\nTo me, this seems to cover the main agreements about prior choice. Unfortunately, Jeffrey’s prior seems to have some problems for multivariate and hierarchical models, so it’s not a general panacea. However, partly based on the intuition gained from Jeffrey’s prior, a few general default prior choices have emerged:\n\nFor scale parameters (something that affects the output linearly, like slope or intercept in a regression), use flat or quasi flat priors such as a bounded uniform distribution or (most common choice) a wide normal distribution. Note that, people often modify this priors by having a bit more probability mass around a neutral value (usually 0) to get the Bayesian analogue of Lasso or Ridge regression, see Park, T. & Casella, G. (2008), Kyung, M.; Gill, J.; Ghosh, M.; Casella, G. et al. (2010) Penalized regression, standard errors, and Bayesian lassos. Bayesian Analysis, 5, 369-411. If this effect is small, we speak about mildly regularizing priors. If the effect is strong, we speak about shrinkage priors. Shrinkage priors can be designed with a fixed or adaptive shrinkage, were fixed means that that the strength of the shrinkage (e.g. controlled by the sd in a normal prior) is fixed, whereas adaptive shrinkage priors fit the shrinkage via a hyperprior.\nFor variance parameters (something like the standard deviation in a linear regression), use decaying parameters such as 1/x (standard choice according to Jeffrey’s prior) or inverse-gamma (very common choice because of conjugacy, see next subjsection)\nFor variance hyperparameters in hierarchical models, use again decaying priors such as inverse-gamma or half-t family (suggested by Gelman, 2006)\nFor binomial distribution, Jeffrey’s prior is a beta(1/2,1/2) - this is a good default choice.\n\nIn doubt, prior effects can be examined by varying the prior in a sensitivity analysis.\nSee also\n\nhttp://stats.stackexchange.com/questions/6493/weakly-informative-prior-distributions-for-scale-parameters\nhttp://stats.stackexchange.com/questions/61928/choosing-non-informative-priors?rq=1\n\n\n\n2.3.3 Conjugacy\nAnother issue that is often important is conjugacy. In Bayesian statistics, if the posterior distributions p(θ|x) are in the same family as the prior probability distribution p(θ), the prior and posterior are then called conjugate distributions, and the prior is called a conjugate prior for the likelihood function.\nConjugacy has two main advantages:\n\nThe shape of the posterior is known, which allows approximating it parameterically\nMany sampling methods work more efficiently\n\nOne therefore traditionally preferred to specify conjugate priors if possible, although the advantages of this depend on the samplers that are used. Most modern samplers do not really require conjugacy to work well.\n\n\n2.3.4 Readings\nUninformative priors\nKass, R. E. & Wasserman, L. (1996) The selection of prior distributions by formal rules. J. Am. Stat. Assoc., American Statistical Association, 91, 1343-1370.\nJeffreys, H. (1946) An Invariant Form for the Prior Probability in Estimation Problems. Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences, The Royal Society, 186, 453-461.\nJaynes, E. (1968) Prior probabilities. Systems Science and Cybernetics, IEEE Transactions on, IEEE, 4, 227-241.\nTibshirani, R. (1989) Noninformative priors for one parameter of many. Biometrika, 76, 604-608.\nPark, T. & Casella, G. (2008) The Bayesian Lasso. Journal of the American Statistical Association, 103, 681-686.\nIrony, T. Z. & Singpurwalla, N. D. (1997) Non-informative priors do not exist – a dialogue with José M. Bernardo. J. Stat. Plan. Infer., 65, 159-177.\nGelman, A.; Jakulin, A.; Pittau, M. G. & Su, Y.-S. (2008) A weakly informative default prior distribution for logistic and other regression models. The Annals of Applied Statistics, JSTOR, , 1360-1383.\nGelman, A. (2006) Prior distributions for variance parameters in hierarchical models. Bayesian Analysis, Citeseer, 1, 515-533.\nFong, Y.; Rue, H. & Wakefield, J. (2010) Bayesian inference for generalized linear mixed models. Biostatistics, 11, 397-412.\nFerguson, T. (1974) Prior distributions on spaces of probability measures. The Annals of Statistics, JSTOR, 2, 615-629.\nJeffrey’s prior\nJeffreys priors for mixture estimation http://arxiv.org/abs/1511.03145\nInformative priors\nChoy, S. L.; O’Leary, R. & Mengersen, K. (2009) Elicitation by design in ecology: using expert opinion to inform priors for Bayesian statistical models. Ecology, 90, 265-277\n\n\n\n\nGeorge, Edward I., and Robert McCulloch. 1993. “On Obtaining Invariant Prior Distributions.” Journal of Statistical Planning and Inference 37 (2): 169–79. https://doi.org/10.1016/0378-3758(93)90086-L."
  },
  {
    "objectID": "2C-LMM.html#lm",
    "href": "2C-LMM.html#lm",
    "title": "4  Linear and linear mixed models",
    "section": "4.1 LM",
    "text": "4.1 LM\nTo introduce the typical options in a linear model, we use an example that was originally prepared by Jörn Pagel. In the example, we want to analyze predictors of Body mass in the snake Vipera aspis.\n\nDat = read.table(\"https://raw.githubusercontent.com/florianhartig/LearningBayes/master/data/Aspis_data.txt\", stringsAsFactors = T)\n\n# Inspect relationship between body mass and total body lenght\nplot(Dat$TL, Dat$BM,\n     xlab = 'Total length [mm]',\n     ylab = 'Body mass [g]')\n\n\n\n# For the analysis we use log-transformed body masses\n# and log-transformed and scaled total body lenght (TL)\nplot(Dat$log_TL.sc, Dat$log_BM)\n\n\n\n\n\n4.1.1 LM with continous predictor\nLinear regression with lm()\n\nLM &lt;- lm(log_BM ~ log_TL.sc, data = Dat)\nsummary(LM)\n\n\nCall:\nlm(formula = log_BM ~ log_TL.sc, data = Dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.60635 -0.28199  0.01219  0.21303  0.83099 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  4.95347    0.03219  153.89   &lt;2e-16 ***\nlog_TL.sc    0.32626    0.03233   10.09   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3452 on 113 degrees of freedom\nMultiple R-squared:  0.474, Adjusted R-squared:  0.4694 \nF-statistic: 101.8 on 1 and 113 DF,  p-value: &lt; 2.2e-16\n\n\nAnalysis in JAGS\n\nlibrary(rjags)\n\n# 1) Save a description of the model in JAGS syntax \n# to the working directory\nmodel =\"\nmodel{\n  # Likelihood\n  for(i in 1:n.dat){\n    y[i] ~ dnorm(mu[i],tau)\n    mu[i] &lt;- alpha + beta.TL * TL[i]\n    }\n  \n  # Prior distributions\n  alpha ~ dnorm(0,0.001)\n  beta.TL ~ dnorm(0,0.001)\n  tau &lt;- 1/(sigma*sigma)\n  sigma ~ dunif(0,100)\n  }\n\"\n\n# s = function(x) dgamma(x, shape = 0.0001, rate = 0.001)\n# curve(s, 0, 5)\n\n\n# 2) Set up a list that contains all the necessary data\nData = list(y = Dat$log_BM, \n            TL = Dat$log_TL.sc,\n            n.dat = nrow(Dat))\n\n# 3) Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1), \n                            beta.TL = rnorm(1),\n                            sigma = runif(1,1,100))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file = textConnection(model), data=Data, \n                        init = inits.fn, n.chains = 3, \n                        n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 115\n   Unobserved stochastic nodes: 3\n   Total graph size: 470\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c(\"alpha\",\"beta.TL\",\"sigma\")\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, \n                        n.iter = 5000)\n\n# Statistical summaries of the (marginal) posterior \n# distribution for each parameter\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean      SD  Naive SE Time-series SE\nalpha   4.9532 0.03272 0.0002671      0.0002671\nbeta.TL 0.3259 0.03276 0.0002674      0.0002655\nsigma   0.3491 0.02360 0.0001927      0.0002524\n\n2. Quantiles for each variable:\n\n          2.5%    25%    50%    75%  97.5%\nalpha   4.8881 4.9310 4.9537 4.9751 5.0171\nbeta.TL 0.2619 0.3039 0.3259 0.3477 0.3905\nsigma   0.3066 0.3327 0.3476 0.3639 0.3992\n\n# If we were interested only in point estimates,\n# we could extract posterior means\nPostMeans &lt;- summary(Samples)$statistics[,'Mean']\n\n# Graphical overview of the samples from the MCMC chains\nplot(Samples)\n\n\n\n\nCompare this to the lm() results\n\nplot(Dat$log_TL.sc, Dat$log_BM)\ncoef(LM)\n\n(Intercept)   log_TL.sc \n  4.9534727   0.3262571 \n\n# and the two regression lines\nabline(LM, col = 'red')\nabline(PostMeans[1:2], col = 'blue')\n\n\n\n\n\n\n4.1.2 LM with categorical predictor\nInspect relationship between body mass and total body lenght but now seperately for the two sexes\n\npoint.symbols &lt;- c(f = 1, m = 4)\nplot(Dat$TL, Dat$BM,\n     pch = point.symbols[Dat$Sex],\n     xlab = 'Total length [mm]',\n     ylab = 'Body mass [g]')\n\n\n\n# For the analysis we use log-transformed body masses\n# and log-transformed and scaled total body lenght (TL)\nplot(Dat$log_TL.sc, Dat$log_BM, \n     pch = point.symbols[Dat$Sex])\n\n\n\n\nLinear regression with lm()\n\nLM &lt;- lm(log_BM ~ log_TL.sc + Sex, data = Dat)\nsummary(LM)\n\n\nCall:\nlm(formula = log_BM ~ log_TL.sc + Sex, data = Dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.41822 -0.18030 -0.02969  0.16075  0.50885 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  5.27831    0.03127  168.81   &lt;2e-16 ***\nlog_TL.sc    0.44765    0.02197   20.38   &lt;2e-16 ***\nSexm        -0.59296    0.04394  -13.49   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.214 on 112 degrees of freedom\nMultiple R-squared:  0.7997,    Adjusted R-squared:  0.7961 \nF-statistic: 223.6 on 2 and 112 DF,  p-value: &lt; 2.2e-16\n\n\nAnalysis in JAGS\n\nmodel = \n  \"model{\n  # Likelihood\n  for(i in 1:n.dat){\n    y[i] ~ dnorm(mu[i],tau)\n    mu[i] &lt;- alpha + beta.TL * TL[i] + beta.m * Sexm[i]\n    }\n  \n  # Prior distributions\n  alpha ~ dnorm(0,0.001)\n  beta.TL ~ dnorm(0,0.001)\n  beta.m ~ dnorm(0,0.001)\n  tau &lt;- 1/(sigma*sigma)\n  sigma ~ dunif(0,100)\n  }\n  \"\n\n# 2) Set up a list that contains all the necessary data\nData = list(y = Dat$log_BM, \n            TL = Dat$log_TL.sc,\n            Sexm = ifelse(Dat$Sex == 'm', 1, 0),\n            n.dat = nrow(Dat))\n\n# 3) Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1), \n                            beta.TL = rnorm(1),\n                            beta.m = rnorm(1),\n                            sigma = runif(1,1,100))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file = textConnection(model), data=Data, \n                        init = inits.fn, n.chains = 3, \n                        n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 115\n   Unobserved stochastic nodes: 4\n   Total graph size: 588\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c(\"alpha\",\"beta.TL\",\"beta.m\",\"sigma\")\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, \n                        n.iter = 5000)\n\n# Statistical summaries of the (marginal) posterior distribution\n# for each parameter\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n           Mean      SD  Naive SE Time-series SE\nalpha    5.2796 0.03207 0.0002619      0.0005402\nbeta.TL  0.4480 0.02231 0.0001821      0.0002520\nbeta.m  -0.5948 0.04525 0.0003694      0.0007766\nsigma    0.2164 0.01467 0.0001198      0.0001537\n\n2. Quantiles for each variable:\n\n           2.5%     25%     50%     75%   97.5%\nalpha    5.2165  5.2582  5.2793  5.3008  5.3439\nbeta.TL  0.4044  0.4330  0.4479  0.4631  0.4919\nbeta.m  -0.6847 -0.6246 -0.5946 -0.5647 -0.5065\nsigma    0.1894  0.2061  0.2158  0.2258  0.2465\n\n# Compare this to the lm() results\ncoef(LM)\n\n(Intercept)   log_TL.sc        Sexm \n  5.2783125   0.4476472  -0.5929615 \n\n# Graphical overview of the samples from the MCMC chains\nplot(Samples)\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n        Point est. Upper C.I.\nalpha            1          1\nbeta.TL          1          1\nbeta.m           1          1\nsigma            1          1\n\nMultivariate psrf\n\n1\n\n# Correlation plot\nBayesianTools::correlationPlot(Samples)\n\n\n\n\n\n\n4.1.3 LM with interaction\nFrom the previous plot, it´s obvious that we could also consider an interaction between sex and body mass\nLinear regression with lm()\n\nLM &lt;- lm(log_BM ~ log_TL.sc + Sex + Sex:log_TL.sc, data = Dat)\nsummary(LM)\n\n\nCall:\nlm(formula = log_BM ~ log_TL.sc + Sex + Sex:log_TL.sc, data = Dat)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.40757 -0.16446 -0.01407  0.14330  0.51672 \n\nCoefficients:\n               Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)     5.29416    0.03247 163.059   &lt;2e-16 ***\nlog_TL.sc       0.48297    0.03049  15.841   &lt;2e-16 ***\nSexm           -0.59513    0.04362 -13.642   &lt;2e-16 ***\nlog_TL.sc:Sexm -0.07225    0.04361  -1.657      0.1    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.2123 on 111 degrees of freedom\nMultiple R-squared:  0.8045,    Adjusted R-squared:  0.7992 \nF-statistic: 152.3 on 3 and 111 DF,  p-value: &lt; 2.2e-16\n\n\nAnalysis in JAGS\n\nmodel =\n  \"model{\n  # Likelihood\n  for(i in 1:n.dat){\n    y[i] ~ dnorm(mu[i],tau)\n    mu[i] &lt;- alpha + beta.TL[Sex[i]] * TL[i] + beta.m * Sexm[i]\n    }\n  \n  # Prior distributions\n  alpha ~ dnorm(0,0.001)\n  for(s in 1:2){\n    beta.TL[s] ~ dnorm(0,0.001)\n    }\n  beta.m ~ dnorm(0,0.001)\n  tau &lt;- 1/(sigma*sigma)\n  sigma ~ dunif(0,100)\n  }\n  \"\n\n# 2) Set up a list that contains all the necessary data\nData = list(y = Dat$log_BM, \n            TL = Dat$log_TL.sc,\n            Sexm = ifelse(Dat$Sex == 'm', 1, 0),\n            Sex = as.numeric(Dat$Sex),\n            n.dat = nrow(Dat))\n\n# 3) Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1), \n                            beta.TL = rnorm(2),\n                            beta.m = rnorm(1),\n                            sigma = runif(1,1,100))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file = textConnection(model), data=Data, \n                        init = inits.fn, n.chains = 3, \n                        n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 115\n   Unobserved stochastic nodes: 5\n   Total graph size: 704\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c(\"alpha\",\"beta.TL\",\"beta.m\",\"sigma\")\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, \n                        n.iter = 5000)\n\n# Statistical summaries of the (marginal) posterior distribution\n# for each parameter\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean      SD  Naive SE Time-series SE\nalpha       5.2948 0.03231 0.0002638      0.0005270\nbeta.TL[1]  0.4833 0.03038 0.0002480      0.0003388\nbeta.TL[2]  0.4108 0.03124 0.0002551      0.0002972\nbeta.m     -0.5958 0.04369 0.0003567      0.0007299\nsigma       0.2147 0.01465 0.0001196      0.0001593\n\n2. Quantiles for each variable:\n\n              2.5%     25%     50%     75%   97.5%\nalpha       5.2316  5.2733  5.2953  5.3162  5.3577\nbeta.TL[1]  0.4238  0.4631  0.4831  0.5036  0.5436\nbeta.TL[2]  0.3496  0.3898  0.4109  0.4318  0.4717\nbeta.m     -0.6808 -0.6251 -0.5960 -0.5670 -0.5088\nsigma       0.1884  0.2045  0.2139  0.2239  0.2456\n\n# Compare this to the lm() results\ncoef(LM)\n\n   (Intercept)      log_TL.sc           Sexm log_TL.sc:Sexm \n    5.29416337     0.48296513    -0.59513246    -0.07224671 \n\n# Graphical overview of the samples from the MCMC chains\nplot(Samples)\n\n\n\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n           Point est. Upper C.I.\nalpha               1          1\nbeta.TL[1]          1          1\nbeta.TL[2]          1          1\nbeta.m              1          1\nsigma               1          1\n\nMultivariate psrf\n\n1\n\n# Correlation plot\nBayesianTools::correlationPlot(Samples)"
  },
  {
    "objectID": "2C-LMM.html#lmm-mixed-effects",
    "href": "2C-LMM.html#lmm-mixed-effects",
    "title": "4  Linear and linear mixed models",
    "section": "4.2 LMM (mixed effects)",
    "text": "4.2 LMM (mixed effects)\n\n####################################################################\n# Linear regression with lm()\nlibrary(lme4)\n\nLoading required package: Matrix\n\nLME &lt;- lmer(log_BM ~ log_TL.sc + Sex + Sex:log_TL.sc\n         + (1|Pop), data = Dat)\nsummary(LME)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_BM ~ log_TL.sc + Sex + Sex:log_TL.sc + (1 | Pop)\n   Data: Dat\n\nREML criterion at convergence: -135.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.62996 -0.80341 -0.05736  0.71521  2.79946 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Pop      (Intercept) 0.03569  0.1889  \n Residual             0.01152  0.1073  \nNumber of obs: 115, groups:  Pop, 9\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     5.30770    0.06515  81.468\nlog_TL.sc       0.50259    0.01676  29.996\nSexm           -0.59752    0.02243 -26.643\nlog_TL.sc:Sexm -0.04369    0.02291  -1.907\n\nCorrelation of Fixed Effects:\n            (Intr) lg_TL. Sexm  \nlog_TL.sc    0.102              \nSexm        -0.191 -0.297       \nlg_TL.sc:Sx -0.071 -0.656  0.016\n\n#############################################################\n# Analysis in JAGS                                          #\n#############################################################\n\nmodel = \n  \"model{\n  # Likelihood\n  for(i in 1:n.dat){\n    y[i] ~ dnorm(mu[i],tau)\n    mu[i] &lt;- alpha[Pop[i]] + beta.TL[Sex[i]] * TL[i] + beta.m * Sexm[i]\n    }\n  \n  # Prior distributions\n  for(p in 1:n.pop){\n    alpha[p] ~ dnorm(mu.alpha, tau.pop)\n    }\n  for(s in 1:2){\n    beta.TL[s] ~ dnorm(0,0.001)\n    }\n  mu.alpha ~ dnorm(0,0.001)\n  beta.m ~ dnorm(0,0.001)\n  tau &lt;- 1/(sigma*sigma)\n  sigma ~ dunif(0,100)\n  tau.pop &lt;- 1/(sigma.pop*sigma.pop)\n  sigma.pop ~ dunif(0,100)\n  }\n  \"\n\n# 2) Set up a list that contains all the necessary data\nData = list(y = Dat$log_BM, \n            TL = Dat$log_TL.sc,\n            Sexm = ifelse(Dat$Sex == 'm', 1, 0),\n            Sex = as.numeric(Dat$Sex),\n            n.dat = nrow(Dat),\n            Pop = Dat$Pop,\n            n.pop = max(Dat$Pop))\n\n# 3) Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(mu.alpha = rnorm(1), \n                            beta.TL = rnorm(2),\n                            beta.m = rnorm(1),\n                            sigma = runif(1,1,100),\n                            sigma.pop = runif(1,1,100))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file = textConnection(model), data=Data, \n                        init = inits.fn, n.chains = 3, \n                        n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 115\n   Unobserved stochastic nodes: 15\n   Total graph size: 832\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c(\"mu.alpha\",\"beta.TL\",\"beta.m\",\"sigma\",\"sigma.pop\")\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, \n                        n.iter = 5000)\n\n# Statistical summaries of the (marginal) posterior distribution\n# for each parameter\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n              Mean       SD  Naive SE Time-series SE\nbeta.TL[1]  0.5032 0.016849 0.0001376      2.026e-04\nbeta.TL[2]  0.4590 0.017565 0.0001434      1.980e-04\nbeta.m     -0.5974 0.022529 0.0001839      3.829e-04\nmu.alpha    5.3083 0.080623 0.0006583      7.273e-04\nsigma       0.1087 0.007728 0.0000631      8.905e-05\nsigma.pop   0.2261 0.070652 0.0005769      1.074e-03\n\n2. Quantiles for each variable:\n\n               2.5%     25%     50%     75%   97.5%\nbeta.TL[1]  0.47004  0.4920  0.5032  0.5145  0.5362\nbeta.TL[2]  0.42407  0.4474  0.4591  0.4709  0.4930\nbeta.m     -0.64159 -0.6125 -0.5975 -0.5822 -0.5528\nmu.alpha    5.14824  5.2592  5.3084  5.3570  5.4702\nsigma       0.09469  0.1033  0.1082  0.1136  0.1251\nsigma.pop   0.13155  0.1773  0.2126  0.2585  0.4013\n\n# Compare this to the lmer() results\nsummary(LME)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: log_BM ~ log_TL.sc + Sex + Sex:log_TL.sc + (1 | Pop)\n   Data: Dat\n\nREML criterion at convergence: -135.6\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-2.62996 -0.80341 -0.05736  0.71521  2.79946 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n Pop      (Intercept) 0.03569  0.1889  \n Residual             0.01152  0.1073  \nNumber of obs: 115, groups:  Pop, 9\n\nFixed effects:\n               Estimate Std. Error t value\n(Intercept)     5.30770    0.06515  81.468\nlog_TL.sc       0.50259    0.01676  29.996\nSexm           -0.59752    0.02243 -26.643\nlog_TL.sc:Sexm -0.04369    0.02291  -1.907\n\nCorrelation of Fixed Effects:\n            (Intr) lg_TL. Sexm  \nlog_TL.sc    0.102              \nSexm        -0.191 -0.297       \nlg_TL.sc:Sx -0.071 -0.656  0.016\n\n# Graphical overview of the samples from the MCMC chains\nplot(Samples)\n\n\n\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n           Point est. Upper C.I.\nbeta.TL[1]          1          1\nbeta.TL[2]          1          1\nbeta.m              1          1\nmu.alpha            1          1\nsigma               1          1\nsigma.pop           1          1\n\nMultivariate psrf\n\n1\n\n# Correlation plot\nBayesianTools::correlationPlot(Samples)"
  },
  {
    "objectID": "3A-ModelSelection.html#regularization",
    "href": "3A-ModelSelection.html#regularization",
    "title": "5  Bayesian model selection",
    "section": "5.1 Regularization",
    "text": "5.1 Regularization\nThe first option is not really a model selection method, but it replaces model selection in many cases. What I mean by this is the following: in many, maybe most cases where model selection is applied in frequentist analysis, the goal is NOT to find out if one of several alternative hypotheses are better supported by the data, but the goal is rather to deal with the problem that we do not have enough data to to fit the large model that we actually want to fit. The underlying problem is the bias-variance trade-off.\nIn this case, many frequentists use information theoretical (IT) model selection approaches to simplify the model. These approaches are also available to Bayesians (see next subsection on Bayesian IT approaches), but in many cases prior regularization is a better option to deal with this problem. The idea of this approaches is to control model complexity via priors that are deliberately more restrictive than what we would set when expressing our “pure” uncertainty about the respective parameter.\n\nTo show how these approaches works, I will create a dataset with 100 predictors, of which the first 10 have an effect with slope 1, and the 90 remaining predictors have no effect, i.e. an effect with a slope of zero.\n\nset.seed(1)\ndat = data.frame(matrix(runif(20000, -0.5,0.5), ncol = 100))\ndat$y = rnorm(200)\ndat$y = dat$y + rowSums(dat[,1:10]) \n# Preparing data list for Jags \nData = list(y = dat$y, x = as.matrix(dat)[,1:100], i.max = nrow(dat))\n\nWe can look at the results of a standard frequentist regression with all predictors\n\nfullModel = lm(y ~ . , data = dat)\n#summary(fullModel)\n\nWe can calculate the mean squared error (MSE) of the estimates via\n\ntrue = c(rep(1,10), rep(0,90))\nestimated = coef(fullModel)[-1]\nMSE = var(true - estimated)\n\nFor our convenience later, I will create a small function to plot this\n\nplotEstimates &lt;- function(estimates, ...){\n  MSE = round(var(true - estimates), digits = 4)\n  out &lt;- barplot(estimates, las = 2, ylim = c(-0.5, 1.5), ...)\n  text(60, 1, paste(\"MSE\", MSE))\n  lines(x = c(0,12), y = c(1,1), lwd = 4)\n  lines(x = c(12,120), y = c(0,0), lwd = 4)\n}\nplotEstimates(estimated)\n\n\n\n\n\n5.1.1 Uninformative priors\nObviously, the Bayesian equivalent of the model with wide uninformative priors would yield the same result.\n\nlibrary(rjags)\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.0\n\n\nLoaded modules: basemod,bugs\n\nmodelCode0 = \"model{\n  # Likelihood\n  for(i in 1:i.max){\n    mu[i] &lt;- inprod(a , x[i,]) + b\n    y[i] ~ dnorm(mu[i],tau)\n  }\n  \n  # Prior distributions\n  for(i in 1:100){\n    a[i] ~ dnorm(0,0.0001)\n  }\n  b ~ dnorm(0,0.0001) # usually no need and safer not to regularize intercept\n\n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau)\n}\n\"\n  \n\njagsModel0 &lt;- jags.model(file= textConnection(modelCode0), \n                        data=Data, \n                        n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 102\n   Total graph size: 20910\n\nInitializing model\n\nSamples0 &lt;- coda.samples(jagsModel0, \n                        variable.names = c(\"a\",\"b\",\"sigma\"), \n                        n.iter = 5000)\n\n\nx0&lt;- summary(Samples0)\nest0 &lt;- x0$quantiles[1:100,3] \nplotEstimates(est0)\n\n\n\n\n\n\n5.1.2 Mildly regularizing priors\nThe idea of mildly regularizing priors is that we give the regression slopes a small push towards zero. The strength of a mildly regularizing prior should be such that it doesn’t strongly influence parameter estimates, but still regularizes the problem. If predictors and responses are scaled and centered, we would expect strong effects to have effect sizes of 1. A typical prior width for a mildly regularizing prior would thus be something in the order of 1 to 10. In the code below, this is implemented by setting medium wide normal priors on the slope.\n\n  a[i] ~ dnorm(0,0.5)\n\n\n\n\n\n\n\nCaution\n\n\n\nAs discussed, when working with informative priors, we should be aware of the scale of the predictors and the reponse, as the priors have to be viewed in relation to the data scale to say what is a wide and what is a narrow prior.\nHowever, as we created the data in this case in with a centered uniform distribution and a unit scale, we can skip this step\n\n\nIn my practical work, I have often found that these priors will often help to avoid parameter identifiability problems, in particular in GLMs, thus making model selection or simplification unnecessary, while having very little influence on the parameter estimates\n\nlibrary(rjags)\nmodelCode1 = \"model{\n  # Likelihood\n  for(i in 1:i.max){\n    mu[i] &lt;- inprod(a , x[i,]) + b\n    y[i] ~ dnorm(mu[i],tau)\n  }\n  \n  # Prior distributions\n  for(i in 1:100){\n    a[i] ~ dnorm(0,0.5)\n  }\n  b ~ dnorm(0,0.0001) # usually no need and safer not to regularize intercept\n\n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau)\n}\n\"\n  \n\njagsModel1 &lt;- jags.model(file= textConnection(modelCode1), \n                        data=Data, \n                        n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 102\n   Total graph size: 20911\n\nInitializing model\n\nSamples1 &lt;- coda.samples(jagsModel1, \n                        variable.names = c(\"a\",\"b\",\"sigma\"), \n                        n.iter = 5000)\n\n#gelman.diag(Samples)\n#summary(Samples)\n\nx1&lt;- summary(Samples1)\nest1 &lt;- x1$quantiles[1:100,3] \nplotEstimates(est1)\n\n\n\n\nIn the result, we see a slightly lower MSE as before, but virtually no influence on the effects of on the parameter estimates.\n\n\n5.1.3 Fixed or adaptive shrinkage priors\nShrinkage priors are basically the same as before, just stronger.\nThere are two options to set the shrinkage:\n\nAs common in frequentist L1 / L2 shrikage, you could set the the shrinkage via cross-validation or similar approaches\nYou can treat the shrinkage as a parameter to be estimated in the model and set a hyperprior on it.\n\nI use the latter example in the code below. The structure added to the model is\n\n  for(i in 1:100){\n    a[i] ~ dnorm(0,tauShrinkage)\n  }\n\n  tauShrinkage ~ dgamma(0.001, 0.001)\n  sdShrinkage &lt;- 1/sqrt(tauShrinkage)\n\nNote that this structure is very similar to a mixed model, so in some sense we treat the slopes for the different predictors as “random effects” that come from a common normal distribution, whose SD is estimated.\nRegularization via prior - Lasso and Ridge equivalents. Idea is that we put a kind of “random effect” directly on the parameter values\nKyung, M.; Gill, J.; Ghosh, M.; Casella, G. et al. (2010) Penalized regression, standard errors, and Bayesian lassos. Bayesian Analysis, 5, 369-411.\nhttp://stats.stackexchange.com/questions/95395/ridge-regression-bayesian-interpretation?rq=1\nhttp://stats.stackexchange.com/questions/28609/regularized-bayesian-logistic-regression-in-jags\nhttp://doingbayesiandataanalysis.blogspot.de/2014/01/bayesian-variable-selection-in-multiple.html\n\nmodelCode2 = \"model{\n\n  # Likelihood\n  for(i in 1:i.max){\n    mu[i] &lt;- inprod(a , x[i,]) + b\n    y[i] ~ dnorm(mu[i],tau)\n  }\n  \n  # Prior distributions\n  for(i in 1:100){\n    a[i] ~ dnorm(0,tauShrinkage)\n  }\n  b ~ dnorm(0,0.001)\n\n  tauShrinkage ~ dgamma(0.001, 0.001)\n  sdShrinkage &lt;- 1/sqrt(tauShrinkage)\n  \n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau)\n}\n\"\n\njagsModel2 &lt;- jags.model(file= textConnection(modelCode2), data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 103\n   Total graph size: 20912\n\nInitializing model\n\npara.names &lt;- \nSamples2 &lt;- coda.samples(jagsModel2, \n                         variable.names = c(\"a\",\"b\",\"sigma\", \"sdShrinkage\"), \n                         n.iter = 5000)\n\n#gelman.diag(Samples)\n#summary(Samples)\n\nx2&lt;- summary(Samples2)\nest2 &lt;- x2$quantiles[1:100,3] \nplotEstimates(est2)\n\n\n\n\nNote in the results that we now have a notable bias towards zero on the parameter estimates, but also overall a strongly reduced MSE on the estimates.\n\n\n5.1.4 Spike and slab\nSpike and slab priors, also known as stochastic search variable selection (SSVS),\nGeorge, E. I., & McCulloch, R. E. (1993). Variable selection via Gibbs sampling. Journal of the American Statistical Association, 88(423), 881-889.\nidentifies promising subsets of multiple regression covariates via Gibbs sampling (George and McCulloch 1993). Here’s a short SSVS demo with JAGS and R.\nThe idea of the spike and slab\nIshwaran, H. & Rao, J. S. (2005) Spike and Slab Variable Selection: Frequentist and Bayesian Strategies. The Annals of Statistics, Institute of Mathematical Statistics, 33, pp. 730-773.\nHemant Ishwaran. J. Sunil Rao. “Spike and slab variable selection: Frequentist and Bayesian strategies.” Ann. Statist. 33 (2) 730 - 773, April 2005. https://doi.org/10.1214/009053604000001147\nThe structure added to the model used here follows Kuo, L., & Mallick, B. (1998). Variable selection for regression models. Sankhyā: The Indian Journal of Statistics, Series B, 65-81.\n\n  pind ~ dbeta(5,5)\n  for(j in 1:100){\n    a_raw[j] ~ dnorm(0,0.01)\n    ind[j] ~ dbern(pind)\n    a[j] = ind[j] * a_raw[j]\n  }\n\nIn this code, the effective slope ind[j] * a_raw[j] consists of two components. The variable ind (zero or one) controls the spike - value of 0 switches off the parameter estimate and thus forces the effective slope towards zero. Ind is drawn from a Bernoulli distribution with a beta prior, which is a standard choice for the Bernoull because it is conjugage and Jeffrey’s prior. The variable a_raw[j] is the slab, which can be interpreted as the estimate of the parameter conditional on the probability ind that it is in the model.\nHere the full model\n\nmodelCode3 = \"model{\n  # Likelihood\n  for(i in 1:i.max){\n    mu[i] &lt;- inprod(a , x[i,]) + b\n    y[i] ~ dnorm(mu[i],tau)\n  }\n\n  # Prior distributions\n  pind ~ dbeta(5,5)\n  for(j in 1:100){\n    a_raw[j] ~ dnorm(0,0.01)\n    ind[j] ~ dbern(pind)\n    a[j] = ind[j] * a_raw[j]\n  }\n  b ~ dnorm(0,0.01)\n  \n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau)\n  }\n\"\n\njagsModel3 &lt;- jags.model(file= textConnection(modelCode3), data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 203\n   Total graph size: 21112\n\nInitializing model\n\nSamples3 &lt;- coda.samples(jagsModel3, \n                        variable.names = c(\"a_raw\", \"ind\",\"sigma\"), \n                        n.iter = 5000)\n\nx3&lt;- summary(Samples3)\n\nIn this model, we get two estimates for each predictor: the probability that a variable is in the model (ind) and the estimate for the variable, conditional on its inclusion in the model. Let’s first look at the probability to be in the model\n\nincl &lt;- x3$quantiles[101:200,3] \nbarplot(incl)\n\n\n\n\nPretty good recovery. Here are the conditional estimates\n\ncondEst &lt;- x3$quantiles[1:100,3] \nplotEstimates(condEst)\n\n\n\n\nAs a final estimator, I plot the probability to be included times the conditional estimate:\n\nest3 &lt;- condEst * incl\nplotEstimates(est3)\n\n\n\n\n\n\n5.1.5 Comparison of the methods\nHere the comparison of the unregularized model with all 3 prior regularization\n\npar(mfrow = c(2,2))\nplotEstimates(est0, main = \"Uninformative\")\nplotEstimates(est1, main = \"Mildly regularizing\")\nplotEstimates(est2, main = \"Adaptive shrinkage\")\nplotEstimates(est3, main = \"Spike and slap\")\n\n\n\n\nPractical examples of ecological papers using this approach\n\nhttps://www.sciencedirect.com/science/article/pii/S1574954124001031"
  },
  {
    "objectID": "3A-ModelSelection.html#bayes-factors",
    "href": "3A-ModelSelection.html#bayes-factors",
    "title": "5  Bayesian model selection",
    "section": "5.3 Bayes Factors",
    "text": "5.3 Bayes Factors\nBayes’ formula makes no fundamental difference between models and parameters. Hence, to perform inference with multiple models, we can simply write down the joint posterior \\(P(M_i, \\Theta_i| D)\\) of different models \\(M_i\\) with parameter vectors \\(\\Theta_i\\) as\n\\[\nP(M_i, \\Theta_i| D) \\sim L(D|M_i , \\Theta_i) \\cdot p(\\Theta_i) \\cdot p(M_i)\n\\]\nwhere \\(L(D|M_i , \\Theta_i)\\) is the likelihood of model \\(M_i\\), \\(p(\\Theta_i)\\) is the prior distribution of model \\(M_i\\), \\(p(M_i)\\) is the prior weight on model \\(M_i\\), and the ~ indicates that this the equation still needs to be normalized (i.e. divided by the integral over all models and parameters). The figure below provides a graphical illustration of this situation, assuming that we jump between three models with an increasing number of parameters.\n\nWhen looking at this picture as a matrix, we see that we have two options to average (marginalize) across this space of models * parameters\n\nWe look at the posterior for each model parameter marginalized across models (parameter averaging)\nWe can look at the marginal probability of each model (posterior weight aka Bayes factor)\n\n\n5.3.0.1 Marginal likelihood\nWe are mainly interested in the second option here. When looking at the previous equation, we see that the posterior propability of each model can be obtained by the integral of the likelihood over the prior space of each model\n\\[\nML_i = P(D|M_i) = \\int L(D|M_i , \\Theta_i) \\cdot p(\\Theta_i) d \\Theta_i\n\\]\nwhich is known as the marginal likelihood (ML). The marginal likelihood can be interpreted similar as the to the Likelihood at MLE of a fitted model - it provides you with an idea about the average fit of the model across the prior. The ratio of the MLs of two models is known as the Bayes factor.\n\\[\nBF_{i,j} = \\frac{ML_i}{ML_j}\n\\]\nNote that this is analogue to the likelihood ratio in frequentist statistics. However, there is one important difference: as we will discuss in more detail below, the BF is affected by the prior choice as well as the uncertainty around the parameter estimates. More complex models typically have more uncertainties, i.e. more prior volume in areas areas with lower likelihood (as opposed to the MLE).\nThe following image explains why this is: imagine our posterior falls off such that always 1/3 of the prior space has high likelihood and 2/3 of the parameter space has low likelihood. Then, you see that the space with high likelihood is getting relatively smaller the higher the dimension of your data.\n\nThis has two consequences\n\nThus, the BF implicitly penalizes for model complexity (or, maybe more precisely, it penalizes for unconstrained parameters) and can thus directly be used to compare models. (Kass, R. E. And Raftery 1995) without having to add a complexity penalty\nBF calculations are sensitive to the choice of model priors (see also example below). If you would increase the prior width of all models above, the 2d model would be penalized more strongly than the 1d model. Many authors therefore warn that BF should not be calculated on uninformative priors.\n\n\n\n\n\n\n\nBayesian Hypothesis tests using the BF package\n\n\n\n\n\nFor simple models, BF can be calculated using the BayesFactor package\n\nlibrary(BayesFactor)\n\ndata(sleep)\n\n## Compute difference scores\ndiffScores = sleep$extra[1:10] - sleep$extra[11:20]\n\n## Traditional two-tailed t test\nt.test(diffScores)\n\n\n    One Sample t-test\n\ndata:  diffScores\nt = -4.0621, df = 9, p-value = 0.002833\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n -2.4598858 -0.7001142\nsample estimates:\nmean of x \n    -1.58 \n\nbf = ttestBF(x = diffScores, rscale = 10)\nbf\n\nBayes factor analysis\n--------------\n[1] Alt., r=10 : 4.521117 ±0%\n\nAgainst denominator:\n  Null, mu = 0 \n---\nBayes factor type: BFoneSample, JZS\n\n\nThe BF package circumvents the prior problem by scaling the prior to the data. This is known under the label “empirical Bayes”. Empirical Bayesian methods estimate or scale prior probability distributions to the data, thus recovering some of the properties of classical frequentist inference.\n\n\n\n\n\n5.3.0.2 Posterior model weights\nWhile the BF compares two models. For more than 2 models, it is more useful to standardize the ML multiplied with the model priors across all models in question, calculating a Bayesian posterior model weight as\n\\[\nBMW_i = \\frac{P(D|M_i)}{\\sum_j P(D|M_j)} \\cdot \\frac{p(M_i)}{\\sum_j p(M_j)}\n\\]\nBesides inference about the right model, posterior model weights can also be used to make predictions with a weighted ensemble of models (see, e.g., (Dormann Carsten et al. 2018)).\n\n\n5.3.0.3 Model-averaged parameters\nThe second route we can take is to marginalize across models to obtain averaged parameters. To obtain the averaged parameters, we simply marginalize across model space\n\\[\nP(\\Theta | D) = \\sum_i L(D|M_i , \\Theta_i) \\cdot p(\\Theta_i) \\cdot p(M_i)\n\\]\nresulting in averaged distributions for the parameters that are shared between models (thus, this only works for models that are at least partly nested). The approach is analogue to parameter averaging in frequentist models (e.g. (Burnham and Anderson 2002)) and similarly, it is controversially discussed. The main objection is that under collinearity of predictors x1 and x2, the models M1: y ~ x1 and M1: y ~ x1 + x2 do not really estimate the same quantity when looking at the parameter estimate of x1: in M1, x1 is measuring the raw correlation, while in M2, x1 is measuring y ~ x1|x2. Whether it is desirable to average these quantities into a single distribution should be carefully considered.\n\n\n5.3.1 Estimation of the marginal likelihood in practice\nWhile the definition of the Bayesian model weights and averaged parameters is straightforward, the estimation of these quantities is often not. In princile, there are two options to estimate the quantities defined above numerically, both with a number of caveats.\nThe first option is to sample directly from the joint posterior of the models and the parameters . Basic algorithms such as rejection sampling can do that without any modification , but they are inefficient for higher-dimensional parameter spaces. More sophisticated algorithms such as MCMC and SMC require modifications to deal with the issue of a changing number of parameters when changing between models, as well as with the issue of a changed meaning of the parameters. Such modifications (the most common class are the reversible-jump MCMCs (RJ-MCMC)) are often difficult to program, tune and generalize, which is the reason why they are typically only applied in specialized, well defined settings with a large number of models to be compared.\nThe second option is to approximate the marginal likelihood each model independently, and then average parameters or predictions based on the resulting weights. To approximate the marginal likelihood, one has to cover the parameter space with of each single model, e.g. with random sampling or MCMC, store the likelihood, and then compute the marginal likelihood from that. The challenge here is to get a stable approximation of the marginal likelihood, which can be connected with considerable problems (Weinberg 2012). Nevertheless, because of the comparably easier implementation, this approach the more common choice in situations where the number of models to be compared is low.\nFor Jags model, the ML can be estimated using the BayesTools package, which, unfortunately, has a very similar name as the BayesianTools package.\nThe following example shows a comparison of two regression models using the BayesianTools package\n\nlibrary(BayesianTools)\n?BayesianTools::marginalLikelihood\n\n# Creating test data with quadratic relationship\nsampleSize = 30\nx &lt;- (-(sampleSize-1)/2):((sampleSize-1)/2)\ny &lt;-  1 * x + 1*x^2 + rnorm(n=sampleSize,mean=0,sd=10)\n# plot(x,y, main=\"Test Data\")\n\n# likelihoods for linear and quadratic model \nlikelihood1 &lt;- function(param){\n  pred = param[1] + param[2]*x + param[3] * x^2\n  singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[4]^2), log = TRUE)\n  return(sum(singlelikelihoods))  \n}\nlikelihood2 &lt;- function(param){\n  pred = param[1] + param[2]*x \n  singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[3]^2), log = TRUE)\n  return(sum(singlelikelihoods))  \n}\n\nsetUp1 &lt;- createBayesianSetup(likelihood1, \n                              lower = c(-5,-5,-5,0.01), \n                              upper = c(5,5,5,30))\nsetUp2 &lt;- createBayesianSetup(likelihood2, \n                              lower = c(-5,-5,0.01), \n                              upper = c(5,5,30))\n\nout1 &lt;- runMCMC(bayesianSetup = setUp1)\nM1 = marginalLikelihood(out1, start = 1000)\n\nout2 &lt;- runMCMC(bayesianSetup = setUp2)\nM2 = marginalLikelihood(out2, start = 1000)\n\n\n### Calculating Bayes factor\n\nexp(M1$ln.ML - M2$ln.ML)\n\n[1] 4.057075e+29\n\n# BF &gt; 1 means the evidence is in favor of M1. See Kass, R. E. & Raftery, A. E. \n# (1995) Bayes Factors. J. Am. Stat. Assoc., Amer Statist Assn, 90, 773-795.\n\n### Calculating Posterior weights\n\nexp(M1$ln.ML) / ( exp(M1$ln.ML) + exp(M2$ln.ML))\n\n[1] 1\n\n# If models have different model priors, multiply with the prior probabilities of each model. \n\n\n\n5.3.2 Prior influence on the marginal likelihood\nA problem that is often not sufficiently appreciated when performing Bayesian inference across multiple models is the influence of the choice of uninformative parameter priors on the model weight.\nThe issue arises because the prior density enters the marginal likelihood multiplicative. Although this follows directly from the joint posterior definition and is logically completely consistent, it has the somewhat intuitive consequence that increasing the width width of an uninformative prior will linearly decrease the marginal likelihood (Sinharay-SensitivityBayesFactors-2002). In particular, for an infinitely wide (inproper) uninformative prior, the model weight goes to zero, regardless of the fit. This behavior is surprising to many practitioners of Bayesian analysis, because they are used to the fact that the influence of increasing prior width on uninformative priors is minimal for fitting parameters.\n\n# Motivation: ML is very dependent on the prior, which is a problem if you \n# have uninformative priors. you can see this via rerunning the upper  \n# example with changed priors - suddenly, support for M1 is gone\n\n\nsetUp1 &lt;- createBayesianSetup(likelihood1, \n                              lower = c(-500,-500,-500,0.01), \n                              upper = c(500,500,500,3000))\nsetUp2 &lt;- createBayesianSetup(likelihood2, \n                              lower = c(-500,-500,0.01), \n                              upper = c(500,500,3000))\n\nout1 &lt;- runMCMC(bayesianSetup = setUp1)\nM1 = marginalLikelihood(out1, start = 1000)\n\nout2 &lt;- runMCMC(bayesianSetup = setUp2)\nM2 = marginalLikelihood(out2, start = 1000)\n\nCalculating Bayes factor\n\nexp(M1$ln.ML - M2$ln.ML)\n\n[1] 832.1584\n\n\nAs you see, the value is very different to the value we calculated before.\n\n5.3.2.1 Fractional BF\nThe fact that Bayesian model weights are strongly dependent on the width of the prior choice has sparked discussion of the appropriateness of this approach in situations with uninformative priors. For example, in situations where multiple nested models are compared, the width of the uniformative prior may completely determine the complexity of models that are being selected. One suggestion that has been made is not to perform multi-model inference with uninformative priors at all, or that at least additional analysis is necessary to find parameter priors that are sensible for the multi-model setup at hand. Another solution is to calibrate the model on a part of the data first, use the result as new priors and then perform the analysis described above (intrinsic Bayes factor (Berger-IntrinsicBayesFactor-1996), fractional Bayes factor (OHagan-FractionalBayesFactors-1995)). If sufficient data is available so that the likelihood falls off sufficiently strong during the calibration step, this approach should nearly eliminate any ambiguity resulting from the prior choice.\n\n# likelihoods with half the data \n# note: half of the data is ensured here by summing only seq(1, 30, 2) of the\n# observations in the likelihood\n\nlikelihood1 &lt;- function(param){\n  pred = param[1] + param[2]*x + param[3] * x^2\n  singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[4]^2), log = TRUE)\n  return(sum(singlelikelihoods[seq(1, 30, 2)]))  \n}\nlikelihood2 &lt;- function(param){\n  pred = param[1] + param[2]*x \n  singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[3]^2), log = TRUE)\n  return(sum(singlelikelihoods[seq(1, 30, 2)]))  \n}\n\nsetUp1 &lt;- createBayesianSetup(likelihood1, \n                              lower = c(-500,-500,-500,0.01), \n                              upper = c(500,500,500,3000))\nsetUp2 &lt;- createBayesianSetup(likelihood2, \n                              lower = c(-500,-500,0.01), \n                              upper = c(500,500,3000))\n\nout1 &lt;- runMCMC(bayesianSetup = setUp1)\nout2 &lt;- runMCMC(bayesianSetup = setUp2)\n\nnewPrior1 = createPriorDensity(out1, start = 200, \n                               lower = c(-500,-500,-500,0.01), \n                               upper = c(500,500,500,3000))\nnewPrior2 = createPriorDensity(out2, start = 200, \n                               lower = c(-500,-500,0.01), \n                               upper = c(500,500,3000))\n\n# now rerun this with likelihoods for the other half of the data and new prior\n\nxTest= x[16:30]\nyTest = y[16:30]\n\nlikelihood1 &lt;- function(param){\n  pred = param[1] + param[2]*x + param[3] * x^2\n  singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[4]^2), log = TRUE)\n  return(sum(singlelikelihoods[seq(2, 30, 2)]))  \n}\nlikelihood2 &lt;- function(param){\n  pred = param[1] + param[2]*x \n  singlelikelihoods = dnorm(y, mean = pred, sd = 1/(param[3]^2), log = TRUE)\n  return(sum(singlelikelihoods[seq(2, 30, 2)]))  \n}\n\nsetUp1 &lt;- createBayesianSetup(likelihood1, prior = newPrior1)\nsetUp2 &lt;- createBayesianSetup(likelihood2, prior = newPrior2)\n\nout1 &lt;- runMCMC(bayesianSetup = setUp1)\nM1 = marginalLikelihood(out1, start = 1000)\n\nout2 &lt;- runMCMC(bayesianSetup = setUp2)\nM2 = marginalLikelihood(out2, start = 1000)\n\nCalculating the fractional Bayes factor\n\nexp(M1$ln.ML - M2$ln.ML)\n\n[1] 1667203\n\n\n\n\n\n\nBurnham, Kenneth P, and David R Anderson. 2002. Model Selection and Multi-Model Inference: A Practical Information-Theoretic Approach. Springer Verlag.\n\n\nDormann Carsten, F., M. Calabrese Justin, Guillera-Arroita Gurutzeta, Matechou Eleni, Bahn Volker, BartoÅ,, Kamil, M. Beale Colin, et al. 2018. “Model Averaging in Ecology: A Review of Bayesian, Information-Theoretic, and Tactical Approaches for Predictive Inference.” Ecol Monogr 0 (0). https://doi.org/10.1002/ecm.1309.\n\n\nKass, R. E. And Raftery, A. E. 1995. “Bayes Factors.” Journal of The American Statistical Association 90 (430): 773–95.\n\n\nWeinberg, Martin D. 2012. “Computing the Bayes Factor from a Markov Chain Monte Carlo Simulation of the Posterior Distribution.” Bayesian Analysis 7 (3): 737–70."
  },
  {
    "objectID": "3A-ModelSelection.html#information-criteria",
    "href": "3A-ModelSelection.html#information-criteria",
    "title": "5  Bayesian model selection",
    "section": "5.2 Information Criteria",
    "text": "5.2 Information Criteria\nIn Bayesian inference, there are two main IC metrics that are used DIC and the more modern metric WAIC.\nBoth are essentially extension of the AIC with the\nDIC is intended as a generalisation of Akaike’s Information Criterion (AIC). For non-hierarchical models with little prior information, pD should be approximately the true number of parameters. AIC requires counting parameters and hence any intermediate level (‘random-effects’) parameters need to be integrated out. A recent ‘conditional AIC’ by Vaida and Blanchard (2005) focuses on the random effects in normal hierarchical models and uses tr(H) as the effective number of parameters, and so again matches DIC.\n\n5.2.1 DIC\nDIC can be calculated in Jags using\n\ndic = dic.samples(jagsModel, n.iter = 5000)\n\nIn the result of DIC, you will find\n\nThe mean deviance (Likelihood), to be interpreted the average fit of the model\nThe estimated complexity of the model\nThe penalized deviance, which is deviance penalized for model complexity\n\nWe can calculate DICs for all the previous 4 models:\n\ndic0 = dic.samples(jagsModel0, n.iter = 5000)\ndic1 = dic.samples(jagsModel1, n.iter = 5000)\ndic2 = dic.samples(jagsModel2, n.iter = 5000)\ndic3 = dic.samples(jagsModel3, n.iter = 5000)\n\n\ndic0\n\nMean deviance:  530.2 \npenalty 104.2 \nPenalized deviance: 634.4 \n\ndic1\n\nMean deviance:  522 \npenalty 99.42 \nPenalized deviance: 621.4 \n\ndic2\n\nMean deviance:  519.3 \npenalty 61.82 \nPenalized deviance: 581.2 \n\ndic3\n\nMean deviance:  524.4 \npenalty 18.28 \nPenalized deviance: 542.7 \n\n\nWe see that the effective number of parameters estimated by the DIC is very different between the different models. The shrinkage priors reduce the effective number of parameters. The spike and slap model has formally (by counting in the code) the most parameters of all models, more than 200, but the DIC estimates that it has effectively only 18 parameters.\nFor more R. B. O’Hara. M. J. Sillanpää. “A review of Bayesian variable selection methods: what, how and which.” Bayesian Anal. 4 (1) 85 - 117, March 2009. https://doi.org/10.1214/09-BA403\n\n\n5.2.2 WAIC\nIt was suggested that WAIC has a bunch of favorable advantages over DIC, but it is also harder to calculate. In Jags, WAIC is a bit complicated. It is better supported in STAN and brms.\n\nlibrary(loo)\n\nlibrary(brms)\n?waic\n\n# model with population-level effects only\nfit1 &lt;- brm(rating ~ treat + period + carry,\n            data = inhaler)\nwaic1 &lt;- waic(fit1)\n\nWarning: \n2 (0.3%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n# model with an additional varying intercept for subjects\nfit2 &lt;- brm(rating ~ treat + period + carry + (1|subject),\n            data = inhaler)\nwaic2 &lt;- waic(fit2)\n\nWarning: \n26 (4.5%) p_waic estimates greater than 0.4. We recommend trying loo instead.\n\n\ncompare both models\n\nloo_compare(waic1, waic2)\n\n     elpd_diff se_diff\nfit2  0.0       0.0   \nfit1 -9.6       4.6   \n\n\n\n\n5.2.3 Careful with step-wise model selection\nNote that step-wise selection based on information criteria shares many of the same problems that appear in frequentist IC selection. Those are in particular\n\nAffects uncertainties, in particular p-values\nDestroys causal relationships\n\nThe issue is known under the term “post-selection inference” and applies to Bayesian and frequentist models alike, see e.g. Kuchibhotla, A. K., Kolassa, J. E., & Kuffner, T. A. (2022). Post-selection inference. Annual Review of Statistics and Its Application, 9, 505-527.\nThis example shows how AIC selection, followed by a conventional regression analysis of the selected model, massively inflates false positives.\n\nset.seed(1)\nlibrary(MASS)\n\ndat = data.frame(matrix(runif(20000), ncol = 100))\ndat$y = rnorm(200)\nfullModel = lm(y ~ . , data = dat)\nsummary(fullModel)\n# 2 predictors out of 100 significant (on average, we expect 5 of 100 to be significant)\n\nselection = stepAIC(fullModel, trace = F)\nsummary(lm(y ~ X1 + X2 + X3 + X5 + X7 + X13 + X20 + X23 + X30 + \n             X37 + X42 + X45 + X46 + X47 + X48 + X64 + X65 + X66 + X71 + \n             X75 + X80 + X81 + X87 + X88 + X89 + X90 + X94 + X100, data = dat))\n\n# voila, 15 out of 28 (before 100) predictors significant - looks like we could have good fun to discuss / publish these results!\n\n\n#######################################################\n# Same thing, but now we put in 10 significant predictors (the first 10 x)\n\nset.seed(1)\nlibrary(MASS)\n\ndat = data.frame(matrix(runif(20000), ncol = 100))\ndat$y = rnorm(200)\ndat$y = dat$y + rowSums(dat[,1:10]) \nfullModel = lm(y ~ . , data = dat)\nsummary(fullModel)\n# 2 predictors out of 100 significant (on average, we expect 5 of 100 to be significant)\n\nselection = stepAIC(fullModel, trace = F)\nsummary(lm(y ~ X1 + X2 + X3 + X4 + X5 + X6 + X7 + X8 + X9 + X10 + X13 + \n             X14 + X20 + X23 + X24 + X26 + X30 + X37 + X42 + X46 + X47 + \n             X48 + X49 + X64 + X65 + X66 + X68 + X71 + X73 + X75 + X80 + \n             X81 + X87 + X88 + X89 + X90 + X94, data = dat))\n\n# true positives are good, but false positives remain"
  },
  {
    "objectID": "3B-Workflow.html#picking-an-initial-model",
    "href": "3B-Workflow.html#picking-an-initial-model",
    "title": "6  Bayesian Workflow",
    "section": "6.1 Picking an initial model",
    "text": "6.1 Picking an initial model\nAn initial model will consist of a model structure and priors. For priors, see previous section on prior choice. For the initial model structure, you need to know what structures are available and what their advantage / disadvantage are. This is the same as for all statistical analysis. Thus, all the the consideration in https://theoreticalecology.github.io/AdvancedRegressionModels/, sections on model choice apply.\nFor the following, we will stay with our simple airquality example\n\nmodelCode = \"\nmodel{\n\n  # Likelihood\n  for(i in 1:i.max){\n    mu[i] &lt;- Temp*x[i]+ intercept\n    y[i] ~ dnorm(mu[i],tau)\n  }\n\n  # Prior distributions\n  \n  # For location parameters, typical choice is wide normal\n  intercept ~ dnorm(0,0.0001)\n  Temp ~ dnorm(0,0.0001)\n\n  # For scale parameters, typical choice is decaying\n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau) # this line is optional, just in case you want to observe sigma or set sigma (e.g. for inits)\n\n}\n\""
  },
  {
    "objectID": "3B-Workflow.html#prior-predictive-checks",
    "href": "3B-Workflow.html#prior-predictive-checks",
    "title": "6  Bayesian Workflow",
    "section": "6.2 Prior predictive checks",
    "text": "6.2 Prior predictive checks\nPrior predictive checks mean that we check what predictions would be possible or preferred by the model a prior. We can do this either by removing the data in the code above and observing the parameters or predictions, or by adding a prior predictive block directly in the model, such as the following one:\n\nmodelCode = \"\nmodel{\n\n  intercept2 ~ dnorm(0,0.0001)\n  Temp2 ~ dnorm(0,0.0001)\n  \n  for(i in 1:i.max){\n    muPrior[i] &lt;- Temp2*x[i]+ intercept2\n  }\n  \n\n}\n\"\n\nWhen you now observe muPrior, you get an idea about what results are possible with your priors"
  },
  {
    "objectID": "3B-Workflow.html#validate-computation",
    "href": "3B-Workflow.html#validate-computation",
    "title": "6  Bayesian Workflow",
    "section": "6.3 Validate computation",
    "text": "6.3 Validate computation\n\n6.3.1 Convergence diagnostics\nAs discussed in section 1.2\n\n\n6.3.2 Fake data simulation\nThis means that you simulate data from your model, and check if you can retrieve the same parameters.\n\n\n6.3.3 Simulation-based calibration\nSee help of\n\n?BayesianTools::calibrationTest"
  },
  {
    "objectID": "3B-Workflow.html#posterior-model-checks",
    "href": "3B-Workflow.html#posterior-model-checks",
    "title": "6  Bayesian Workflow",
    "section": "6.4 Posterior model checks",
    "text": "6.4 Posterior model checks\n\n6.4.1 Prior sensitivity\nPrior sensitivity means that you will check to what extend results are driven by the prior. To do this, change the prior and look at the results\n\n\n6.4.2 Posterior predictive checks / residuals\nIn posterior predictive checks, you look at the posterior model predictions. There are two things you can do:\n\nPosterior predictions = credible interval\nPosterior simulations = prediction interval\n\nUsually one does both things together. Technically, in a posterior simulation, we will add a block to the model where we simulate new data as assumed in the likelihood.\n\nmodelCode = \"\nmodel{\n  for(i in 1:i.max){\n    yPosterior[i] ~ dnorm(mu[i],tau)\n  }\n}\n\"\n\nAs you see, in the case of a linear regression, this is a very simple expression, but in more complicated models, this can be a longer block. Moreover, note that in hierarchical models, often the question arises on which parameters you want to condition on and on which not. See also comments in\n\n?DHARMa::simulateResiduals\n\nLet’s look at a full example with prior and posterior checks:\n\nlibrary(rjags)\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.0\n\n\nLoaded modules: basemod,bugs\n\nlibrary(BayesianTools)\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\ndat = airquality[complete.cases(airquality),] \ndat = dat[order(dat$Temp),] # order so that we can later make more convenient line plots\n\nData = list(y = dat$Ozone, \n            x = dat$Temp, \n            i.max = nrow(dat))\n\nmodelCode = \"\nmodel{\n  # Likelihood\n  \n  for(i in 1:i.max){\n    mu[i] &lt;- Temp*x[i]+ intercept\n    y[i] ~ dnorm(mu[i],tau)\n  }\n  sigma &lt;- 1/sqrt(tau)\n\n  # Priors\n  intercept ~ dnorm(0,0.0001)\n  Temp ~ dnorm(0,0.0001)\n  tau ~ dgamma(0.001, 0.001)\n  \n  # Prior predictive - sample new parameters from prior and make predictions\n  \n  intercept2 ~ dnorm(0,0.0001)\n  Temp2 ~ dnorm(0,0.0001)\n  \n  for(i in 1:i.max){\n    muPrior[i] &lt;- Temp2*x[i]+ intercept2\n  }\n  \n  # Posterior predictive - same as likelihood, but remove data\n  \n  for(i in 1:i.max){\n    yPosterior[i] ~ dnorm(mu[i],tau)\n  }\n}\n\"\n\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 111\n   Unobserved stochastic nodes: 116\n   Total graph size: 501\n\nInitializing model\n\nupdate(jagsModel, n.iter = 1000)\nSamples &lt;- coda.samples(jagsModel, variable.names = c(\"intercept\",\"Temp\",\"sigma\"), n.iter = 5000)\n\n# Prior predictive analysis\n\nSamples &lt;- coda.samples(jagsModel, variable.names = c(\"muPrior\"), n.iter = 5000)\n\npred &lt;- getSample(Samples, start = 300)\n\n# plotting the distributions of predictions \nplot(Ozone ~ Temp, data = dat, ylim = c(-200, 300))\nfor(i in 1:nrow(pred)) lines(dat$Temp, pred[i,])\n\n\n\n# what we see here: a priori many regression lines are possible. You can change your prior to be more narrow and see how this would push prior space in a certain area\n\n# Posterior predictive analysis - Here we can choose to observe the posterior mean predictions or the observations. We will do both in this case because both are inputs to the DHARMa plots\n\nSamples &lt;- coda.samples(jagsModel, variable.names = c(\"mu\", \"yPosterior\"), n.iter = 5000)\n\nlibrary(BayesianTools)\nlibrary(DHARMa)\n\nx &lt;- getSample(Samples, start = 300)\n\ndim(x)\n\n[1] 1051  222\n\n# note - yesterday, we calculated the predictions from the parameters\n# here we observe them direct - this is the normal way to calcualte the \n# posterior predictive distribution\nposteriorPredDistr = x[,1:111]\nposteriorPredSim = x[,112:222]\n\nsim = createDHARMa(simulatedResponse = t(posteriorPredSim), observedResponse = dat$Ozone, fittedPredictedResponse = apply(posteriorPredDistr, 2, median), integerResponse = F)\nplot(sim)\n\n\n\n# all additional plots in the DHARMa package are possible\n\nessentially, we see here the same issue as in the standard residual plots\n\nfit &lt;- lm(Ozone ~ Temp, data = dat)\nsummary(fit)\n\n\nCall:\nlm(formula = Ozone ~ Temp, data = dat)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-40.922 -17.459  -0.874  10.444 118.078 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -147.6461    18.7553  -7.872 2.76e-12 ***\nTemp           2.4391     0.2393  10.192  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 23.92 on 109 degrees of freedom\nMultiple R-squared:  0.488, Adjusted R-squared:  0.4833 \nF-statistic: 103.9 on 1 and 109 DF,  p-value: &lt; 2.2e-16\n\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nplot(allEffects(fit, partial.residuals = T))\n\n\n\npar(mfrow = c(2,2))\nplot(fit) # residuals\n\n\n\n\n\n\n6.4.3 Validation or cross-validation\nValidation or cross-validation means that you test the performance of the model on data to which it was not fit. Purpose is to get an idea of overfitting."
  },
  {
    "objectID": "3B-Workflow.html#final-inference",
    "href": "3B-Workflow.html#final-inference",
    "title": "6  Bayesian Workflow",
    "section": "6.5 Final inference",
    "text": "6.5 Final inference\n\n6.5.1 Parameters\nhttps://stats.stackexchange.com/questions/86472/posterior-very-different-to-prior-and-likelihood\n\n\n6.5.2 Predictions"
  },
  {
    "objectID": "3C-GLMM.html#poisson-regression",
    "href": "3C-GLMM.html#poisson-regression",
    "title": "7  Generalized linear mixed models",
    "section": "7.1 Poisson regression",
    "text": "7.1 Poisson regression\n\nDat &lt;- read.table('https://raw.githubusercontent.com/florianhartig/LearningBayes/master/data/LizardData.txt')\nplot(Dat$Veg,Dat$Count)\n\n\n\n\nStandard frequentist GLM\n\nfit &lt;- glm(Count ~ Veg, data = Dat, family = \"poisson\")\nsummary(fit)\n\n\nCall:\nglm(formula = Count ~ Veg, family = \"poisson\", data = Dat)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-6.8004  -4.2794  -0.9161   2.4673   9.6535  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  2.62494    0.01923  136.50   &lt;2e-16 ***\nVeg          0.26236    0.01736   15.11   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 3015.9  on 199  degrees of freedom\nResidual deviance: 2785.7  on 198  degrees of freedom\nAIC: 3433.1\n\nNumber of Fisher Scoring iterations: 6\n\n\n\nlibrary(rjags)\nlibrary(DHARMa)\nlibrary(BayesianTools)\n\n# Model specification\nmodel = \"\n  model{\n  # Likelihood\n  for(i in 1:n.dat){\n    # poisson model p(y|lambda)\n    y[i] ~ dpois(lambda[i])\n    # logit link function\n    log(lambda[i]) &lt;- mu[i]\n    # linear predictor on the log scale\n    mu[i] &lt;- alpha + beta.Veg*Veg[i] + beta.Veg2*Veg2[i]\n    }\n  # Priors\n  alpha ~ dnorm(0,0.001)\n  beta.Veg  ~ dnorm(0,0.001)\n  beta.Veg2 ~ dnorm(0,0.001)\n  }\n \"\n\n###########################################################\n# Setting up the JAGS run:\n\n# Set up a list that contains all the necessary data\nModel.Data &lt;- list(y = Dat$Count, n.dat = nrow(Dat),\n                   Veg = Dat$Veg, Veg2 = Dat$Veg^2)\n\n# Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1,0,1),\n                            beta.Veg = rnorm(1,0,1),\n                            beta.Veg2 = rnorm(1,0,1))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file= textConnection(model), data=Model.Data, \n                        inits = inits.fn, n.chains = 3, n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 3\n   Total graph size: 1406\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c('alpha','beta.Veg','beta.Veg2')\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel , variable.names = para.names, n.iter = 5000)\n\n# Statistical summaries of the posterior distributions\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n             Mean      SD  Naive SE Time-series SE\nalpha      3.2913 0.02356 0.0001924      0.0003401\nbeta.Veg   0.4527 0.02935 0.0002396      0.0003436\nbeta.Veg2 -0.8894 0.03006 0.0002454      0.0004643\n\n2. Quantiles for each variable:\n\n             2.5%     25%     50%     75%   97.5%\nalpha      3.2456  3.2752  3.2913  3.3074  3.3369\nbeta.Veg   0.3960  0.4329  0.4525  0.4723  0.5108\nbeta.Veg2 -0.9488 -0.9095 -0.8892 -0.8690 -0.8316\n\n# Plot MCMC samples\nplot(Samples)\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n          Point est. Upper C.I.\nalpha              1          1\nbeta.Veg           1          1\nbeta.Veg2          1          1\n\nMultivariate psrf\n\n1\n\n# Correlation plot\ncorrelationPlot(Samples)"
  },
  {
    "objectID": "3C-GLMM.html#adding-posterior-predictions-and-residual-checks",
    "href": "3C-GLMM.html#adding-posterior-predictions-and-residual-checks",
    "title": "7  Generalized linear mixed models",
    "section": "7.2 Adding posterior predictions and residual checks",
    "text": "7.2 Adding posterior predictions and residual checks\n\nmodel =\"\n  model{\n  # Likelihood\n  for(i in 1:n.dat){\n    # poisson model p(y|lambda)\n    y[i] ~ dpois(lambda[i])\n    # logit link function\n    log(lambda[i]) &lt;- mu[i]\n    # linear predictor on the log scale\n    mu[i] &lt;- alpha + beta.Veg*Veg[i] + beta.Veg2*Veg2[i]\n    }\n  # Priors\n  alpha ~ dnorm(0,0.001)\n  beta.Veg  ~ dnorm(0,0.001)\n  beta.Veg2 ~ dnorm(0,0.001)\n\n  # Model predictions\n  for(i in 1:n.pred){\n    y.pred[i] ~ dpois(lambda.pred[i])\n    log(lambda.pred[i]) &lt;- mu.pred[i]\n    mu.pred[i] &lt;- alpha + beta.Veg*Veg.pred[i] + beta.Veg2*Veg2.pred[i]\n    }\n  }\n \"\n\n###########################################################\n# Setting up the JAGS run:\n\n# Set up a list that contains all the necessary data\n# Note that for prediction (later used for model checking) \n# we here use the original predictor variables.\nModel.Data &lt;- list(y = Dat$Count, n.dat = nrow(Dat),\n                   Veg = Dat$Veg, Veg2 = Dat$Veg^2,\n                   Veg.pred = Dat$Veg, Veg2.pred = Dat$Veg^2,\n                   n.pred = nrow(Dat))\n\n# Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1,0,1),\n                            beta.Veg = rnorm(1,0,1),\n                            beta.Veg2 = rnorm(1,0,1))\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file= textConnection(model), data=Model.Data, \n                        inits = inits.fn, n.chains = 3, n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 203\n   Total graph size: 2007\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c('alpha','beta.Veg','beta.Veg2')\n\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel , variable.names = para.names, n.iter = 5000)\n\n# Statistical summaries of the posterior distributions\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n             Mean      SD  Naive SE Time-series SE\nalpha      3.2907 0.02355 0.0001923      0.0003479\nbeta.Veg   0.4523 0.02929 0.0002392      0.0003554\nbeta.Veg2 -0.8886 0.02959 0.0002416      0.0004464\n\n2. Quantiles for each variable:\n\n             2.5%     25%    50%     75%   97.5%\nalpha      3.2440  3.2750  3.291  3.3066  3.3370\nbeta.Veg   0.3949  0.4324  0.452  0.4720  0.5110\nbeta.Veg2 -0.9469 -0.9085 -0.888 -0.8683 -0.8316\n\n# Plot MCMC samples\nplot(Samples)\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n          Point est. Upper C.I.\nalpha              1          1\nbeta.Veg           1          1\nbeta.Veg2          1          1\n\nMultivariate psrf\n\n1\n\n# Correlation plot\ncorrelationPlot(Samples)\n\n\n\n#############################################################\n# Sample simulated posterior for lizard counts (y.pred)\nPred.Samples &lt;- coda.samples(jagsModel, \n                             variable.names = \"y.pred\", \n                             n.iter = 5000)\n\n# Transform mcmc.list object to a matrix\nPred.Mat &lt;- as.matrix(Pred.Samples)\n\n# Plot Model predictions against data\nPred.Q &lt;- apply(Pred.Mat,2,quantile,prob=c(0.05,0.5,0.95))\nplot(Dat$Veg, Dat$Count)\nord &lt;- order(Dat$Veg)\nlines(Dat$Veg[ord], Pred.Q['50%',ord],col='blue',lwd=2)\nlines(Dat$Veg[ord], Pred.Q['5%',ord],col='blue')\nlines(Dat$Veg[ord], Pred.Q['95%',ord],col='blue')\n\n\n\n###########################################################\n# Model checking with DHARMa\n\n# Create model checking plots\nres = createDHARMa(simulatedResponse = t(Pred.Mat),\n                   observedResponse = Dat$Count, \n                   fittedPredictedResponse = apply(Pred.Mat, 2, median),\n                   integer = T)\nplot(res)\n\n\n\n###########################################################"
  },
  {
    "objectID": "3C-GLMM.html#adding-an-overdispersion-term",
    "href": "3C-GLMM.html#adding-an-overdispersion-term",
    "title": "7  Generalized linear mixed models",
    "section": "7.3 Adding an overdispersion term",
    "text": "7.3 Adding an overdispersion term\n\nmodel = \"\n  model{\n  # Likelihood\n  for(i in 1:n.dat){\n    # poisson model p(y|lambda)\n    y[i] ~ dpois(lambda[i])\n    # logit link function\n    log(lambda[i]) &lt;- mu[i] + eps[i]\n    # linear predictor on the log scale\n    mu[i] &lt;- alpha + beta.Veg*Veg[i] + beta.Veg2*Veg2[i]\n    # overdispersion error terms\n    eps[i] ~ dnorm(0,tau.eps) \n    }\n  # Priors\n  alpha ~ dnorm(0,0.001)\n  beta.Veg  ~ dnorm(0,0.001)\n  beta.Veg2 ~ dnorm(0,0.001)\n  tau.eps ~ dgamma(0.001,0.001)\n\n  # Model predictions\n  for(i in 1:n.pred){\n    y.pred[i] ~ dpois(lambda.pred[i])\n    log(lambda.pred[i]) &lt;- mu.pred[i] + eps.pred[i]\n    mu.pred[i] &lt;- alpha + beta.Veg*Veg.pred[i] + beta.Veg2*Veg2.pred[i]\n    eps.pred[i] ~ dnorm(0, tau.eps)\n  }\n  }\n\n\"\n###########################################################\n# Setting up the JAGS run:\n\n# Set up a list that contains all the necessary data\n# Note that for prediction (later used for model checking) \n# we here use the original predictor variables.\nModel.Data &lt;- list(y = Dat$Count, n.dat = nrow(Dat),\n                   Veg = Dat$Veg, Veg2 = Dat$Veg^2,\n                   Veg.pred = Dat$Veg, Veg2.pred = Dat$Veg^2,\n                   n.pred = nrow(Dat))\n\n# Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1,0,1),\n                            beta.Veg = rnorm(1,0,1),\n                            beta.Veg2 = rnorm(1,0,1),\n                            tau.eps = 1\n                            )\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file= textConnection(model), data=Model.Data, \n                        inits = inits.fn, n.chains = 3, n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 604\n   Total graph size: 3008\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c('alpha','beta.Veg','beta.Veg2',\n                'tau.eps')\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel , variable.names = para.names, n.iter = 5000)\n\n# Statistical summaries of the posterior distributions\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n             Mean      SD  Naive SE Time-series SE\nalpha      2.9906 0.11383 0.0009294       0.009623\nbeta.Veg   0.6682 0.09514 0.0007769       0.004503\nbeta.Veg2 -0.9983 0.08197 0.0006693       0.004320\ntau.eps    0.8520 0.15253 0.0012454       0.004001\n\n2. Quantiles for each variable:\n\n             2.5%     25%     50%     75%   97.5%\nalpha      2.7663  2.9132  2.9940  3.0689  3.2019\nbeta.Veg   0.4889  0.6062  0.6658  0.7264  0.8720\nbeta.Veg2 -1.1656 -1.0529 -0.9968 -0.9424 -0.8437\ntau.eps    0.5894  0.7424  0.8397  0.9474  1.1854\n\n# Plot MCMC samples\nplot(Samples)\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n          Point est. Upper C.I.\nalpha           1.04       1.14\nbeta.Veg        1.02       1.06\nbeta.Veg2       1.02       1.07\ntau.eps         1.00       1.00\n\nMultivariate psrf\n\n1.05\n\n# Correlation plot\ncorrelationPlot(Samples)\n\n\n\n#############################################################\n# Sample simulated posterior for lizard counts (y.pred)\nPred.Samples &lt;- coda.samples(jagsModel, \n                             variable.names = \"y.pred\", \n                             n.iter = 5000)\n\n# Transform mcmc.list object to a matrix\nPred.Mat &lt;- as.matrix(Pred.Samples)\n\n# Plot Model predictions against data\nPred.Q &lt;- apply(Pred.Mat,2,quantile,prob=c(0.05,0.5,0.95))\nplot(Dat$Veg, Dat$Count)\nord &lt;- order(Dat$Veg)\nlines(Dat$Veg[ord], Pred.Q['50%',ord],col='blue',lwd=2)\nlines(Dat$Veg[ord], Pred.Q['5%',ord],col='blue')\nlines(Dat$Veg[ord], Pred.Q['95%',ord],col='blue')\n\n\n\n###########################################################\n# Model checking with DHARMa\n\n# Create model checking plots\nres = createDHARMa(simulatedResponse = t(Pred.Mat),\n                   observedResponse = Dat$Count, \n                   fittedPredictedResponse = apply(Pred.Mat, 2, median),\n                   integer = T)\nplot(res)\n\n\n\n###########################################################"
  },
  {
    "objectID": "3C-GLMM.html#adding-zero-inflation",
    "href": "3C-GLMM.html#adding-zero-inflation",
    "title": "7  Generalized linear mixed models",
    "section": "7.4 Adding zero-inflation",
    "text": "7.4 Adding zero-inflation\n\nmodel = \"\n  model{\n  # Likelihood\n  for(i in 1:n.dat){\n    # poisson model p(y|lambda)\n    y[i] ~ dpois(lambda.eff[i])\n    # effective mean abundance\n    lambda.eff[i] &lt;- lambda[i] * Inc[i]\n    # binary variable to indicate occupancy\n    Inc[i] ~ dbin(p.Inc,1)\n    # logit link function\n    log(lambda[i]) &lt;- mu[i] + eps[i]\n    # linear predictor on the log scale\n    mu[i] &lt;- alpha + beta.Veg*Veg[i] + beta.Veg2*Veg2[i]\n    # overdispersion error terms\n    eps[i] ~ dnorm(0,tau.eps) \n    }\n  # Priors\n  alpha ~ dnorm(0,0.001)\n  beta.Veg  ~ dnorm(0,0.001)\n  beta.Veg2 ~ dnorm(0,0.001)\n  tau.eps ~ dgamma(0.001,0.001)\n  p.Inc ~ dbeta(1,1)\n\n  # Model predictions\n  for(i in 1:n.pred){\n    y.pred[i] ~ dpois(lambda.eff.pred[i])\n    lambda.eff.pred[i] &lt;- lambda.pred[i] * Inc.pred[i]\n    Inc.pred[i] ~ dbin(p.Inc, 1)\n    log(lambda.pred[i]) &lt;- mu.pred[i] + eps.pred[i]\n    mu.pred[i] &lt;- alpha + beta.Veg*Veg.pred[i] + beta.Veg2*Veg2.pred[i]\n    eps.pred[i] ~ dnorm(0, tau.eps)\n  }\n  }\n\"\n\n###########################################################\n# Setting up the JAGS run:\n\n# Set up a list that contains all the necessary data\n# Note that for prediction (later used for model checking) \n# we here use the original predictor variables.\nModel.Data &lt;- list(y = Dat$Count, n.dat = nrow(Dat),\n                   Veg = Dat$Veg, Veg2 = Dat$Veg^2,\n                   Veg.pred = Dat$Veg, Veg2.pred = Dat$Veg^2,\n                   n.pred = nrow(Dat))\n\n# Specify a function to generate inital values for the parameters\ninits.fn &lt;- function() list(alpha = rnorm(1,0,1),\n                            beta.Veg = rnorm(1,0,1),\n                            beta.Veg2 = rnorm(1,0,1),\n                            tau.eps = 1,\n                            p.Inc = rbeta(1,1,1),\n                            Inc = rep(1,nrow(Dat))\n                            )\n\n# Compile the model and run the MCMC for an adaptation (burn-in) phase\njagsModel &lt;- jags.model(file= textConnection(model), data=Model.Data, \n                        inits = inits.fn, n.chains = 3, n.adapt= 5000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 200\n   Unobserved stochastic nodes: 1005\n   Total graph size: 3810\n\nInitializing model\n\n# Specify parameters for which posterior samples are saved\npara.names &lt;- c('alpha','beta.Veg','beta.Veg2',\n                'tau.eps','p.Inc')\n# Continue the MCMC runs with sampling\nSamples &lt;- coda.samples(jagsModel , variable.names = para.names, n.iter = 5000)\n\n# Statistical summaries of the posterior distributions\nsummary(Samples)\n\n\nIterations = 5001:10000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n             Mean      SD  Naive SE Time-series SE\nalpha      3.3355 0.04714 0.0003849       0.001581\nbeta.Veg   0.2093 0.05037 0.0004113       0.001275\nbeta.Veg2 -0.6882 0.04761 0.0003887       0.001402\np.Inc      0.7313 0.03353 0.0002738       0.000445\ntau.eps    8.8435 1.96433 0.0160387       0.057017\n\n2. Quantiles for each variable:\n\n             2.5%     25%     50%     75%   97.5%\nalpha      3.2399  3.3044  3.3362  3.3675  3.4253\nbeta.Veg   0.1130  0.1754  0.2086  0.2432  0.3099\nbeta.Veg2 -0.7839 -0.7199 -0.6879 -0.6558 -0.5958\np.Inc      0.6629  0.7091  0.7320  0.7542  0.7949\ntau.eps    5.6401  7.4571  8.6237  9.9795 13.2890\n\n# Plot MCMC samples\nplot(Samples)\n\n\n\n\n\n\n# Check convergence\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n          Point est. Upper C.I.\nalpha              1       1.00\nbeta.Veg           1       1.01\nbeta.Veg2          1       1.01\np.Inc              1       1.00\ntau.eps            1       1.01\n\nMultivariate psrf\n\n1.01\n\n# Correlation plot\ncorrelationPlot(Samples)\n\n\n\n#############################################################\n# Sample simulated posterior for lizard counts (y.pred)\nPred.Samples &lt;- coda.samples(jagsModel, \n                             variable.names = \"y.pred\", \n                             n.iter = 5000)\n\n# Transform mcmc.list object to a matrix\nPred.Mat &lt;- as.matrix(Pred.Samples)\n\n# Plot Model predictions against data\nPred.Q &lt;- apply(Pred.Mat,2,quantile,prob=c(0.05,0.5,0.95))\nplot(Dat$Veg, Dat$Count)\nord &lt;- order(Dat$Veg)\nlines(Dat$Veg[ord], Pred.Q['50%',ord],col='blue',lwd=2)\nlines(Dat$Veg[ord], Pred.Q['5%',ord],col='blue')\nlines(Dat$Veg[ord], Pred.Q['95%',ord],col='blue')\n\n\n\n###########################################################\n# Model checking with DHARMa\n\n# Create model checking plots\nres = createDHARMa(simulatedResponse = t(Pred.Mat),\n                   observedResponse = Dat$Count, \n                   fittedPredictedResponse = apply(Pred.Mat, 2, median),\n                   integer = T)\nplot(res)"
  },
  {
    "objectID": "4A-ErrorInVariable.html#regression-dillution-in-distribution-estimates",
    "href": "4A-ErrorInVariable.html#regression-dillution-in-distribution-estimates",
    "title": "8  Error in variable models",
    "section": "8.1 Regression dillution in distribution estimates",
    "text": "8.1 Regression dillution in distribution estimates\n\n8.1.1 Creation of the data\nAssume we observe data from an ecological system that creates an exponential size distribution (e.g. tree sizes, see Taubert, F.; Hartig, F.; Dobner, H.-J. & Huth, A. (2013) On the Challenge of Fitting Tree Size Distributions in Ecology. PLoS ONE, 8, e58036-), but our measurments are performed with a substantial lognormal observation error\n\nmeanSize &lt;- 10\ntrueLogSd &lt;- 1\nsampleSize &lt;- 500\ntruevalues = rexp(rate = 1/meanSize, n = sampleSize)\nobservations = rlnorm(n = length(truevalues), mean = log(truevalues), sd = trueLogSd)\n\nPlotting true and observed data\n\nmaxV &lt;- ceiling(max(observations,truevalues))\ncounts &lt;- rbind(\n  obs = hist(observations, breaks = 0:maxV, plot = F)$counts,\n  true = hist(truevalues, breaks = 0:maxV, plot = F)$counts\n)\nbarplot(log(t(counts)+1), beside=T)\n\n\n\n\n\n\n8.1.2 Fitting a non-hierarchical model leads to bias\n\nnormalModel = textConnection('\n                             model {\n                             # Priors\n                             meanSize ~ dunif(1,100)\n                             \n                             # Likelihood\n                             for(i in 1:nObs){\n                             true[i] ~ dexp(1/meanSize)\n                             }\n                             }\n                             ')\n\n# Bundle data\npositiveObservations &lt;- observations[observations&gt;0]\ndata = list(true = positiveObservations, nObs=length(positiveObservations))\n\n# Parameters to be monitored (= to estimate)\nparams = c(\"meanSize\")\n\njagsModel = jags.model( file= normalModel , data=data, n.chains = 3, n.adapt= 500)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 500\n   Unobserved stochastic nodes: 1\n   Total graph size: 505\n\nInitializing model\n\nresults = coda.samples( jagsModel , variable.names=params,n.iter=5000)\nplot(results)\n\n\n\n\nThe main thing to note about this is that parameter estimates are heavily biased.\n\n\n8.1.3 Fitting a hierarchical model removes the bias\nModel specification if hierarchical model that accounts for the observation error in Jags\n\nhierarchicalModel = textConnection('\n                                   model {\n                                   # Priors\n                                   meanSize ~ dunif(1,100)\n                                   sigma ~ dunif(0,20) # Precision 1/variance JAGS and BUGS use prec instead of sd\n                                   precision &lt;- pow(sigma, -2)\n                                   \n                                   # Likelihood\n                                   for(i in 1:nObs){\n                                   true[i] ~ dexp(1/meanSize)\n                                   observed[i] ~ dlnorm(log(true[i]), precision)\n                                   }\n                                   }\n                                   ')\n# Bundle data\ndata = list(observed = observations, nObs=length(observations))\n# Parameters to be monitored (= to estimate)\nparams = c(\"meanSize\", \"sigma\")\n\njagsModel = jags.model( file= hierarchicalModel , data=data, n.chains = 3, n.adapt= 500)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 500\n   Unobserved stochastic nodes: 502\n   Total graph size: 1511\n\nInitializing model\n\n#update(jagsModel, 2500) # updating without sampling\nresults = coda.samples( jagsModel , variable.names=params,n.iter=5000)\nplot(results)\n\n\n\n\nIt’s always good to check the correlation structure in the posterior"
  },
  {
    "objectID": "4A-ErrorInVariable.html#regression-dilution-in-slope-estimates",
    "href": "4A-ErrorInVariable.html#regression-dilution-in-slope-estimates",
    "title": "8  Error in variable models",
    "section": "8.2 Regression dilution in slope estimates",
    "text": "8.2 Regression dilution in slope estimates\n\nlibrary(EcoData)\nlibrary(rjags)\n\nnobs = nrow(volcanoisland)\n\n# imagine we had a very bad measurement devide for the altitude\nvolcanoisland$sAltitudeR = volcanoisland$sAltitude + rnorm(nobs)\n\nplot(log(windObs) ~ sAltitude, data = volcanoisland)\nfit = lm(log(windObs) ~ sAltitude, data = volcanoisland)\nsummary(fit)\n\n\nCall:\nlm(formula = log(windObs) ~ sAltitude, data = volcanoisland)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.5295 -0.5738  0.0083  0.5897  2.3810 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.74308    0.02687   64.87   &lt;2e-16 ***\nsAltitude    0.41156    0.02688   15.31   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8498 on 998 degrees of freedom\nMultiple R-squared:  0.1902,    Adjusted R-squared:  0.1894 \nF-statistic: 234.3 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\nabline(fit, col = \"red\")\n\nfit = lm(log(windObs) ~ sAltitudeR, data = volcanoisland)\nsummary(fit)\n\n\nCall:\nlm(formula = log(windObs) ~ sAltitudeR, data = volcanoisland)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-3.3499 -0.5476  0.0075  0.6033  2.5089 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  1.75802    0.02821   62.31   &lt;2e-16 ***\nsAltitudeR   0.21449    0.01938   11.07   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8911 on 998 degrees of freedom\nMultiple R-squared:  0.1094,    Adjusted R-squared:  0.1085 \nF-statistic: 122.5 on 1 and 998 DF,  p-value: &lt; 2.2e-16\n\nabline(fit, col = \"blue\")\n\n\n\n\nlet’s see if we can correct the error\n\ndata = list(WindObs = log(volcanoisland$windObs),\n            Altitude = volcanoisland$sAltitudeR,\n            plot = as.numeric(volcanoisland$plot),\n            nobs = nobs,\n            nplots = length(unique(volcanoisland$plot)))\n\n\nmodelCode = \"model{\n\n  # Likelihood\n  for(i in 1:nobs){\n\n    # error on y\n    WindObs[i] ~ dnorm(mu[i],tau)\n\n    # error on x\n    Altitude[i] ~ dnorm(TrueAltitude[plot[i]],tauMeasure)\n\n    mu[i] &lt;- AltitudeEffect*TrueAltitude[plot[i]]+ Intercept\n  }\n  \n  # Prior distributions\n  \n  # For location parameters, normal choice is wide normal\n  AltitudeEffect ~ dnorm(0,0.0001)\n  Intercept ~ dnorm(0,0.0001)\n\n  for(i in 1:nplots){\n     TrueAltitude[i] ~ dnorm(0,0.0001)\n  }\n\n  # For scale parameters, normal choice is decaying\n  tau ~ dgamma(0.001, 0.001)\n  sigma &lt;- 1/sqrt(tau)\n\n  tauMeasure ~ dgamma(0.001, 0.001)\n  sdMeasure &lt;- 1/sqrt(tauMeasure)\n\n}\n\"\n\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 2000\n   Unobserved stochastic nodes: 104\n   Total graph size: 3314\n\nInitializing model\n\npara.names &lt;- c(\"AltitudeEffect\",\"Intercept\",\"sigma\", \"sdMeasure\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\nplot(Samples)\n\n\n\nsummary(Samples)\n\n\nIterations = 1:5000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                 Mean      SD  Naive SE Time-series SE\nAltitudeEffect 0.4789 0.03043 0.0002485      0.0004178\nIntercept      1.7767 0.02995 0.0002445      0.0003376\nsdMeasure      1.0588 0.02678 0.0002186      0.0002798\nsigma          0.7837 0.01972 0.0001610      0.0002141\n\n2. Quantiles for each variable:\n\n                 2.5%    25%    50%    75%  97.5%\nAltitudeEffect 0.4215 0.4585 0.4782 0.4986 0.5408\nIntercept      1.7191 1.7565 1.7767 1.7970 1.8352\nsdMeasure      1.0085 1.0405 1.0580 1.0763 1.1137\nsigma          0.7463 0.7701 0.7833 0.7968 0.8233"
  },
  {
    "objectID": "4B-Occupancy.html",
    "href": "4B-Occupancy.html",
    "title": "9  Occupancy models",
    "section": "",
    "text": "library(EcoData)\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nplot(lizardsObs ~ earth , data = volcanoisland)\n\n\n\nfit&lt;- glm(lizardsObs ~ earth + windObs , data = volcanoisland, family = binomial)\n\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n\nsummary(fit)\n\n\nCall:\nglm(formula = lizardsObs ~ earth + windObs, family = binomial, \n    data = volcanoisland)\n\nDeviance Residuals: \n     Min        1Q    Median        3Q       Max  \n-2.20221  -0.50521  -0.15234  -0.00544   3.03979  \n\nCoefficients:\n            Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)  1.16640    0.21592   5.402 6.59e-08 ***\nearth       -0.21692    0.02982  -7.273 3.51e-13 ***\nwindObs     -0.61135    0.05763 -10.608  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for binomial family taken to be 1)\n\n    Null deviance: 889.22  on 999  degrees of freedom\nResidual deviance: 562.65  on 997  degrees of freedom\nAIC: 568.65\n\nNumber of Fisher Scoring iterations: 8\n\nplot(allEffects(fit))\n\n\n\n\nHowever, there is something wrong with the model\n\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nres &lt;- simulateResiduals(fit)\nplot(res)\n\n\n\n\nSuspicion - the lizards atually depend on the altitude, but they don’t like wind and therefore hide when there is a lot of wind, and wind also correlates with altitude.\n\nplot(windObs ~ sAltitude, data = volcanoisland)\n\n\n\n\nCould we find out what’s the true effect of the environmental predictors? let’s build an occupancy model where we model the true presence of the lizzard as a latent variable.\n\nlibrary(rjags)\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.0\n\n\nLoaded modules: basemod,bugs\n\nData = list(WindObs = log(volcanoisland$windObs),\n            Altitude = volcanoisland$sAltitude[seq(1, 999, by = 10)],\n            SoilPlot = unique(volcanoisland$earth),\n            LizzardObs =volcanoisland$lizardsObs,\n            plot = as.numeric(volcanoisland$plot),\n            nobs = nrow(volcanoisland),\n            nplots = length(unique(volcanoisland$plot)))\n\n\nmodelCode = \"\nmodel{\n\n  # Likelihood\n  for(i in 1:nobs){\n\n    LizzardObs[i] ~ dbern(ObservationProb[i] *  LizzardTrue[plot[i]])\n    logit(ObservationProb[i]) &lt;- intO + windO * WindObs[i]\n\n  }\n\n  for(i in 1:nplots){\n    LizzardTrue[i] ~ dbern(LizzardSuitability[i])\n    logit(LizzardSuitability[i]) &lt;- intL+ SoilL*SoilPlot[i] + altL * Altitude[i]\n  }\n\n  # Prior distributions\n  intO ~ dnorm(0,0.001)\n  windO ~ dnorm(0,0.001)\n  intL ~ dnorm(0,0.001)\n  SoilL ~ dnorm(0,0.001)\n  altL ~ dnorm(0,0.001)\n\n  # posterior predictive simulations\n\n  # Likelihood\n  for(i in 1:nobs){\n    LizzardObsSim[i] ~ dbern(ObservationProb[i] *  LizzardTrueSim[plot[i]])\n  }\n  for(i in 1:nplots){\n    LizzardTrueSim[i] ~ dbern(LizzardSuitability[i])\n  }\n\n}\n\"\n\ninits.fn &lt;- function() list(LizzardTrue = rep(1,100))\njagsModel &lt;- jags.model(file= textConnection(modelCode), inits = inits.fn, data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1000\n   Unobserved stochastic nodes: 1205\n   Total graph size: 9771\n\nInitializing model\n\npara.names &lt;- c(\"intO\",\"windO\",\"intL\", \"SoilL\", \"altL\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\nplot(Samples)\n\n\n\n\n\n\nsummary(Samples)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n          Mean      SD  Naive SE Time-series SE\nSoilL -0.32036 0.08307 0.0006783       0.001047\naltL   0.04013 0.25479 0.0020803       0.003181\nintL   0.23622 0.26860 0.0021931       0.003652\nintO   5.66535 0.65717 0.0053657       0.038296\nwindO -4.14582 0.42957 0.0035074       0.025122\n\n2. Quantiles for each variable:\n\n         2.5%      25%      50%     75%   97.5%\nSoilL -0.4928 -0.37404 -0.31692 -0.2632 -0.1663\naltL  -0.4540 -0.13044  0.03736  0.2078  0.5453\nintL  -0.2759  0.05005  0.23318  0.4088  0.7863\nintO   4.4430  5.20658  5.65008  6.1011  6.9961\nwindO -5.0138 -4.43041 -4.13413 -3.8423 -3.3544\n\ndic = dic = dic.samples(jagsModel, n.iter = 5000, type = \"pD\")\ndic\n\nMean deviance:  261 \npenalty NaN \nPenalized deviance: NaN \n\n\nInspecting the occupancy results\n\npara.names &lt;- c(\"LizzardTrue\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n\nlibrary(BayesianTools)\nx = getSample(Samples)\n\n# there was no Lizard observed on plot 3 on all 10 replicates\nvolcanoisland$lizardsObs[21:30]\n\n [1] 0 0 0 0 0 0 0 0 0 0\n\n# Still, occupancy probability that the species is there is 22 percent\nbarplot(table(x[,3]))\n\n\n\npara.names &lt;- c(\"LizzardSuitability\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\nx = getSample(Samples)\nS=x\nSuitability = apply(S,2,mean)\nuncertaintyS = apply(S,2,sd)\n\nx = volcanoisland$x[seq(1, 999, by = 10)]\ny = volcanoisland$y[seq(1, 999, by = 10)]\n\npar(mfrow = c(1,2))\n\nplot(x,y, cex = Suitability)\nplot(x,y, cex = uncertaintyS)\n\n\n\n\nChecking residuals of this model\n\npara.names &lt;- c(\"LizzardObsSim\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n\nlibrary(DHARMa)\nx = getSample(Samples)\n\n\nsim = createDHARMa(simulatedResponse = t(x), observedResponse = volcanoisland$lizardsObs, fittedPredictedResponse = apply(x, 2, mean), integerResponse = T)\nplot(sim)\n\n\n\n# fix for a bug in DHARMa, will be corrected\nsim$simulatedResponse = t(x)\nsim$refit = F\nsim$integerResponse = T\n\nres2 = recalculateResiduals(sim, group = as.factor(volcanoisland$plot))\n\nplot(res2)\n\n\n\ntestDispersion(res2)\n\n\n\n\n\n    DHARMa nonparametric dispersion test via sd of residuals fitted vs.\n    simulated\n\ndata:  simulationOutput\ndispersion = 0.93804, p-value = 0.834\nalternative hypothesis: two.sided\n\nx = volcanoisland$x[seq(1, 999, by = 10)]\ny = volcanoisland$y[seq(1, 999, by = 10)]\n\ntestSpatialAutocorrelation(res2, x = x, y = y)\n\n\n\n\n\n    DHARMa Moran's I test for distance-based autocorrelation\n\ndata:  res2\nobserved = 0.115750, expected = -0.010101, sd = 0.017233, p-value =\n2.818e-13\nalternative hypothesis: Distance-based autocorrelation"
  },
  {
    "objectID": "4C-StateSpaceModels.html",
    "href": "4C-StateSpaceModels.html",
    "title": "10  State-space models",
    "section": "",
    "text": "10.0.1 House marten example from Kery & Schaub\nBased on the book “Bayesian population analysis using WinBUGS - a hierarchical perspective” by Marc K�ry & Michael Schaub (2012, Academic Press)\n\nrm(list=ls())\nlibrary(R2jags)\n\n\nmodel = \"\nmodel {\n# Priors and constraints\nlogN[1] ~ dnorm(5.6, 0.01)       # Prior for initial population size\nmean.r ~ dnorm(0, 0.001)             # Prior for mean growth rate\nsigma.proc ~ dunif(0, 1)             # Prior for sd of state process\nsigma2.proc &lt;- pow(sigma.proc, 2)\ntau.proc &lt;- pow(sigma.proc, -2)\nsigma.obs ~ dunif(0, 1)              # Prior for sd of observation process\nsigma2.obs &lt;- pow(sigma.obs, 2)\ntau.obs &lt;- pow(sigma.obs, -2)\n\n# Likelihood\n# State process\nfor (t in 1:(T-1)){\n   r[t] ~ dnorm(mean.r, tau.proc)\n   logN[t+1] &lt;- logN[t] + r[t]\n   }\n# Observation process\nfor (t in 1:T) {\n   y[t] ~ dnorm(logN[t], tau.obs)\n   }\n\n# Population sizes on real scale\nfor (t in 1:T) {\n   N[t] &lt;- exp(logN[t])\n   }\n}\n\"\n\n\n# House martin population data from Magden\npyears &lt;- 6 # Number of future years with predictions\nhm.counts &lt;- c(271, 261, 309, 318, 231, 216, 208, 226, 195, 226, 233, 209, 226, 192, 191, 225, 245, 205, 191, 174, rep(NA, pyears))\nyear &lt;- 1990:(2009 + pyears)\n\n# Bundle data\njags.data &lt;- list(y = log(hm.counts), T = length(year))\n\n# Initial values\ninits &lt;- function(){list(sigma.proc = runif(1, 0, 1), mean.r = rnorm(1), sigma.obs = runif(1, 0, 1), logN.est = c(rnorm(1, 5.6, 0.1), rep(NA, (length(year)-1))))}\n\n# Parameters monitored\nparameters &lt;- c(\"r\", \"mean.r\", \"sigma2.obs\", \"sigma2.proc\", \"N\")\n\n# MCMC settings\nni &lt;- 200000\nnt &lt;- 6\nnb &lt;- 100000\nnc &lt;- 3\n\n# Call JAGS from R (BRT 3 min)\nhm.ssm &lt;- jags(jags.data, inits, parameters, textConnection(model), n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())\n\nmodule glm loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 20\n   Unobserved stochastic nodes: 35\n   Total graph size: 118\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused initial value for \"logN.est\" in chain 1\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused initial value for \"logN.est\" in chain 2\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused initial value for \"logN.est\" in chain 3\n\n\nInitializing model\n\n\nSummarize posteriors\n\nprint(hm.ssm, digits = 3)\nplot(hm.ssm)\n\n\n\n\nDraw figure\n\nfitted &lt;- lower &lt;- upper &lt;- numeric()\nyear &lt;- 1990:2015\nn.years &lt;- length(hm.counts)\nfor (i in 1:n.years){\n   fitted[i] &lt;- mean(hm.ssm$BUGSoutput$sims.list$N[,i])\n   lower[i] &lt;- quantile(hm.ssm$BUGSoutput$sims.list$N[,i], 0.025)\n   upper[i] &lt;- quantile(hm.ssm$BUGSoutput$sims.list$N[,i], 0.975)}\nm1 &lt;- min(c(fitted, hm.counts, lower), na.rm = TRUE)\nm2 &lt;- max(c(fitted, hm.counts, upper), na.rm = TRUE)\npar(mar = c(4.5, 4, 1, 1))\nplot(0, 0, ylim = c(m1, m2), xlim = c(1, n.years), ylab = \"Population size\", xlab = \"Year\", col = \"black\", type = \"l\", lwd = 2, axes = FALSE, frame = FALSE)\naxis(2, las = 1)\naxis(1, at = 1:n.years, labels = year)\npolygon(x = c(1:n.years, n.years:1), y = c(lower, upper[n.years:1]), col = \"gray90\", border = \"gray90\")\npoints(hm.counts, type = \"l\", col = \"black\", lwd = 2)\npoints(fitted, type = \"l\", col = \"blue\", lwd = 2)\nlegend(x = 1, y = 150, legend = c(\"Counts\", \"Estimates\"), lty = c(1, 1), lwd = c(2, 2), col = c(\"black\", \"blue\"), bty = \"n\", cex = 1)\n\n\n\n# Probability of N(2015) &lt; N(2009)\nmean(hm.ssm$BUGSoutput$sims.list$N[,26] &lt; hm.ssm$BUGSoutput$mean$N[20])\n\n[1] 0.6867875"
  },
  {
    "objectID": "4D-AutoregressiveModels.html#example-with-jags",
    "href": "4D-AutoregressiveModels.html#example-with-jags",
    "title": "11  Autoregressive models",
    "section": "11.1 Example with Jags",
    "text": "11.1 Example with Jags\nThis is based on code posted originally by Petr Keil http://www.petrkeil.com/?p=1910, to illustrate some comments I made in response to this blog post\n\n11.1.1 Creating the data\nThis helper function that makes distance matrix for a side*side 2D array\n\ndist.matrix &lt;- function(side)\n{\n  row.coords &lt;- rep(1:side, times=side)\n  col.coords &lt;- rep(1:side, each=side)\n  row.col &lt;- data.frame(row.coords, col.coords)\n  D &lt;- dist(row.col, method=\"euclidean\", diag=TRUE, upper=TRUE)\n  D &lt;- as.matrix(D)\n  return(D)\n}\n\nHere is the function that simulates the autocorrelated 2D array with a given side, and with exponential decay given by lambda (the mean mu is constant over the array, it equals to global.mu)\n\ncor.surface &lt;- function(side, global.mu, lambda)\n{\n  D &lt;- dist.matrix(side)\n  # scaling the distance matrix by the exponential decay\n  SIGMA &lt;- exp(-lambda*D)\n  mu &lt;- rep(global.mu, times=side*side)\n  # sampling from the multivariate normal distribution\n  M &lt;- matrix(nrow=side, ncol=side)\n  M[] &lt;- rmvnorm(1, mu, SIGMA)\n  return(M)\n}\n\nOK, finally simulating the data\n\n# parameters (the truth) that I will want to recover by JAGS\nside = 10\nglobal.mu = 0\nlambda = 0.3  # let's try something new\n\n# simulating the main raster that I will analyze as data\nM &lt;- cor.surface(side = side, lambda = lambda, global.mu = global.mu)\nimage(M)\n\n\n\nmean(M)\n\n[1] 0.4496296\n\n# simulating the inherent uncertainty of the mean of M: \n#test = replicate(1000, mean(cor.surface(side = side, lambda = lambda, global.mu = global.mu)))\n#hist(test, breaks = 40)\n\n\n\n11.1.2 Fitting the model in JAGS\npreparing the data\n\ny &lt;- as.vector(as.matrix(M))\nmy.data &lt;- list(N = side * side, D = dist.matrix(side), y = y)\n\ndefining the model\n\nmodelCode = textConnection(\"\n    model\n{\n    # priors\n    lambda ~ dgamma(1, 0.1) \n    global.mu ~ dnorm(0, 0.01)\n    for(i in 1:N)\n{\n    # vector of mvnorm means mu\n    mu[i] &lt;- global.mu\n}\n    \n    # derived quantities\n    for(i in 1:N)\n{\n    for(j in 1:N)\n{\n    # turning the distance matrix to covariance matrix\n    D.covar[i,j] &lt;- exp(-lambda*D[i,j])\n}\n}\n    # turning covariances into precisions (that's how I understand it)\n    D.tau[1:N,1:N] &lt;- inverse(D.covar[1:N,1:N])\n    \n    # likelihood\n    y[1:N] ~ dmnorm(mu[], D.tau[,])\n}\n\")\n\nRunning the model\n\nfit &lt;-  jags(data=my.data, \n             parameters.to.save=c(\"lambda\", \"global.mu\"),\n             model.file=modelCode,\n             n.iter=10000,\n             n.chains=3,\n             n.burnin=5000,\n             n.thin=5,\n             DIC=FALSE)\n\nmodule glm loaded\n\n\nmodule dic loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1\n   Unobserved stochastic nodes: 2\n   Total graph size: 10114\n\nInitializing model\n\nplot(as.mcmc(fit))\n\n\n\npairs(as.matrix(as.mcmc(fit)))"
  },
  {
    "objectID": "4E-IntegratedModels.html",
    "href": "4E-IntegratedModels.html",
    "title": "12  Integrated Models",
    "section": "",
    "text": "The following example example of a simple IPM (counts, capture-recapture, reproduction) is from Kéry, M., & Schaub, M. (2011). Bayesian population analysis using WinBUGS: a hierarchical perspective. Academic Press., chapter 11.3\n\n# Population counts (from years 1 to 10)\ny &lt;- c(45, 48, 44, 59, 62, 62, 55, 51, 46, 42)\n\n# Capture-recapture data (in m-array format, from years 1 to 10)\nm &lt;- matrix(c(11,  0,  0,  0,  0,  0,  0,  0,  0,  70,\n               0, 12,  0,  1,  0,  0,  0,  0,  0,  52,\n               0,  0, 15,  5,  1,  0,  0,  0,  0,  42,\n               0,  0,  0,  8,  3,  0,  0,  0,  0,  51,\n               0,  0,  0,  0,  4,  3,  0,  0,  0,  61,\n               0,  0,  0,  0,  0, 12,  2,  3,  0,  66,\n               0,  0,  0,  0,  0,  0, 16,  5,  0,  44,\n               0,  0,  0,  0,  0,  0,  0, 12,  0,  46,\n               0,  0,  0,  0,  0,  0,  0,  0, 11,  71,\n              10,  2,  0,  0,  0,  0,  0,  0,  0,  13,\n               0,  7,  0,  1,  0,  0,  0,  0,  0,  27,\n               0,  0, 13,  2,  1,  1,  0,  0,  0,  14,\n               0,  0,  0, 12,  2,  0,  0,  0,  0,  20,\n               0,  0,  0,  0, 10,  2,  0,  0,  0,  21,\n               0,  0,  0,  0,  0, 11,  2,  1,  1,  14,\n               0,  0,  0,  0,  0,  0, 12,  0,  0,  18,\n               0,  0,  0,  0,  0,  0,  0, 11,  1,  21,\n               0,  0,  0,  0,  0,  0,  0,  0, 10,  26), ncol = 10, byrow = TRUE)\n\n# Productivity data (from years 1 to 9)\nJ &lt;- c(64, 132,  86, 154, 156, 134, 116, 106, 110)\nR &lt;- c(21, 28, 26, 38, 35, 33, 31, 30, 33) \n\nAnalysis of the model\n\n#library(rjags)\nlibrary(R2jags)\n\nLoading required package: rjags\n\n\nLoading required package: coda\n\n\nLinked to JAGS 4.3.0\n\n\nLoaded modules: basemod,bugs\n\n\n\nAttaching package: 'R2jags'\n\n\nThe following object is masked from 'package:coda':\n\n    traceplot\n\nmodel = \"\nmodel {\n#-------------------------------------------------\n#  Integrated population model\n#  - Age structured model with 2 age classes: \n#       1-year old and adult (at least 2 years old)\n#  - Age at first breeding = 1 year\n#  - Prebreeding census, female-based\n#  - All vital rates assumed to be constant\n#-------------------------------------------------\n\n#-------------------------------------------------\n# 1. Define the priors for the parameters\n#-------------------------------------------------\n# Observation error\ntauy &lt;- pow(sigma.y, -2)\nsigma.y ~ dunif(0, 50)\nsigma2.y &lt;- pow(sigma.y, 2)\n\n# Initial population sizes\n# Note that this part is different than in BUGS:\n# 1. JAGS seems to be very sensitive to the choice of the prior distribution for the initial population sizes and of the choice of the observation model (priors)\n# 2. Since the initial population sizes are used in binomial distributions, the numbers must be integers, otherwise JAGS does not run.\n# The following specification seems to work very well and produces similar results as BUGS:\n\nn1 ~ dnorm(25, tauy)T(0,)     # 1-year\nnad ~ dnorm(25, tauy)T(0,)    # Adults\nN1[1] &lt;- round(n1)\nNad[1] &lt;- round(nad)\n\n# Survival and recapture probabilities, as well as productivity\nfor (t in 1:(nyears-1)){\n   sjuv[t] &lt;- mean.sjuv\n   sad[t] &lt;- mean.sad\n   p[t] &lt;- mean.p\n   f[t] &lt;- mean.fec\n   }\n\nmean.sjuv ~ dunif(0, 1)\nmean.sad ~ dunif(0, 1)\nmean.p ~ dunif(0, 1)\nmean.fec ~ dunif(0, 20)\n\n#-------------------------------------------------\n# 2. Derived parameters\n#-------------------------------------------------\n# Population growth rate\nfor (t in 1:(nyears-1)){\n   lambda[t] &lt;- Ntot[t+1] / Ntot[t]\n   }\n\n#-------------------------------------------------\n# 3. The likelihoods of the single data sets\n#-------------------------------------------------\n# 3.1. Likelihood for population population count data (state-space model)\n   # 3.1.1 System process\n   for (t in 2:nyears){\n      mean1[t] &lt;- f[t-1] / 2 * sjuv[t-1] * Ntot[t-1]\n      N1[t] ~ dpois(mean1[t])\n      Nad[t] ~ dbin(sad[t-1], Ntot[t-1])\n      }\n   for (t in 1:nyears){\n      Ntot[t] &lt;- Nad[t] + N1[t]\n      }\n   \n   # 3.1.2 Observation process\n   for (t in 1:nyears){\n      y[t] ~ dnorm(Ntot[t], tauy)\n      }\n\n# 3.2 Likelihood for capture-recapture data: CJS model (2 age classes)\n# Multinomial likelihood\nfor (t in 1:2*(nyears-1)){\n   m[t,1:nyears] ~ dmulti(pr[t,], r[t])\n   }\n\n# m-array cell probabilities for juveniles\nfor (t in 1:(nyears-1)){\n   # Main diagonal\n   q[t] &lt;- 1-p[t]\n   pr[t,t] &lt;- sjuv[t] * p[t]\n   # Above main diagonal\n   for (j in (t+1):(nyears-1)){\n      pr[t,j] &lt;- sjuv[t]*prod(sad[(t+1):j])*prod(q[t:(j-1)])*p[j]\n      } #j  \n   # Below main diagonal\n   for (j in 1:(t-1)){\n      pr[t,j] &lt;- 0\n      } #j\n   # Last column: probability of non-recapture\n   pr[t,nyears] &lt;- 1-sum(pr[t,1:(nyears-1)])\n   } #t\n\n# m-array cell probabilities for adults\nfor (t in 1:(nyears-1)){\n   # Main diagonal\n   pr[t+nyears-1,t] &lt;- sad[t] * p[t]\n   # Above main diagonal\n   for (j in (t+1):(nyears-1)){\n      pr[t+nyears-1,j] &lt;- prod(sad[t:j])*prod(q[t:(j-1)])*p[j]\n      } #j\n   # Below main diagonal\n   for (j in 1:(t-1)){\n      pr[t+nyears-1,j] &lt;- 0\n      } #j\n   # Last column\n   pr[t+nyears-1,nyears] &lt;- 1 - sum(pr[t+nyears-1,1:(nyears-1)])\n   } #t\n\n# 3.3. Likelihood for productivity data: Poisson regression\nfor (t in 1:(nyears-1)){\n   J[t] ~ dpois(rho[t])\n   rho[t] &lt;- R[t]*f[t]\n   }\n}\n\"\n\n# Bundle data\njags.data &lt;- list(m = m, y = y, J = J, R = R, nyears = dim(m)[2], r = rowSums(m))\n\n# Initial values\ninitial&lt;- function(){list(mean.sjuv = runif(1, 0, 1), mean.sad = runif(1, 0, 1), mean.p = runif(1, 0, 1), mean.fec = runif(1, 0, 10), sigma.y = runif(1, 0, 1), n1 = rpois(1, 30), nad = rpois(1, 30))}  \ninits&lt;-list(initial(),initial(),initial())\n# Parameters monitored\nparameters &lt;- c(\"mean.sjuv\", \"mean.sad\", \"mean.p\", \"mean.fec\", \"N1\", \"Nad\", \"Ntot\", \"sigma2.y\", \"lambda\")\n\n# MCMC settings\nni &lt;- 50000\nnt &lt;- 6\nnb &lt;- 25000\nnc &lt;- 3\n\n# Call JAGS from R (BRT 2 min)\nipm &lt;- jags(jags.data, inits, parameters, textConnection(model), n.chains = nc, n.thin = nt, n.iter = ni, n.burnin = nb, working.directory = getwd())\n\nmodule glm loaded\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 21\n   Unobserved stochastic nodes: 25\n   Total graph size: 536\n\nInitializing model\n\n# Produce Fig. 11-4\npar(cex = 1.2)\nlower &lt;- upper &lt;- numeric()\nfor (i in 1:10){\n   lower[i] &lt;- quantile(ipm$BUGSoutput$sims.list$Ntot[,i], 0.025)\n   upper[i] &lt;- quantile(ipm$BUGSoutput$sims.list$Ntot[,i], 0.975)\n   }\nplot(ipm$BUGSoutput$mean$Ntot, type = \"b\", ylim = c(35, 65), ylab = \"Population size\", xlab = \"Year\", las = 1, pch = 16, col = \"blue\", frame = F, cex = 1.5)\nsegments(1:10, lower, 1:10, upper, col = \"blue\")\npoints(y, type = \"b\", col = \"black\", pch = 16, lty = 2, cex = 1.5)\nlegend(x = 1, y = 65, legend = c(\"Counts\", \"Estimates\"), pch = c(16, 16), col = c(\"black\", \"blue\"), lty = c(2, 1), bty = \"n\")\n\n\n\n\nNote that sometimes the model does not run, because of the estimation of the population growth rate (lambda). Presumably, awkward numbers such as 0 are produced during the initialising phase and then the ratio cannot be computed. There are two options: 1) just run the model again until it works, or 2) delete the calculation of lambda within the JAGS code and compute it afterwards based on the posterior samples of Ntot."
  },
  {
    "objectID": "4F-BayesianSEMs.html#lavaan-and-blavaan",
    "href": "4F-BayesianSEMs.html#lavaan-and-blavaan",
    "title": "13  Bayesian causal models",
    "section": "13.1 Lavaan and blavaan",
    "text": "13.1 Lavaan and blavaan\nLaavan is the most popular package to fit multivariate normal SEMs. The advantage of lavaan (over piecewiseSEM which we will use later) is that it can easily include latent factors in the analysis (which we will not use in this example). The downside of lavaan is that it can’t handle non-Gaussian responses.\nHere the standard lavaan model:\n\nlibrary(lavaan)\n\nThis is lavaan 0.6-17\nlavaan is FREE software! Please report any bugs.\n\nlibrary(lavaanPlot)\n\nk_mod &lt;- \"\n  rich ~ firesev + cover\n  cover ~ firesev\"\n\nk_fit_lavaan &lt;- sem(model = k_mod, data = keeley)\n\nWarning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan\nWARNING: some observed variances are (at least) a factor 1000 times larger than\nothers; use varTable(fit) to investigate\n\nlavaanPlot(model=k_fit_lavaan, coefs = TRUE, sig = .05)\n\n\n\n\n\nSame as Bayesian fit using blavaan, which is a laavan style interface to STAN\n\nlibrary(blavaan)\n\nLoading required package: Rcpp\n\n\nThis is blavaan 0.5-2\n\n\nOn multicore systems, we suggest use of future::plan(\"multicore\") or\n  future::plan(\"multisession\") for faster post-MCMC computations.\n\nk_fit_blavaan = blavaan(model = k_mod, data = keeley,\n                auto.var=TRUE, auto.fix.first=TRUE,\n                auto.cov.lv.x=TRUE)\n\nWarning in lav_data_full(data = data, group = group, cluster = cluster, : lavaan\nWARNING: some observed variances are (at least) a factor 1000 times larger than\nothers; use varTable(fit) to investigate\n\n\nWarning in lav_partable_check(lavpartable, categorical = lavoptions$.categorical, : lavaan WARNING: automatically added intercepts are set to zero:\n    [rich cover]\n\n\n\nSAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.002524 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 25.24 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 1500 [  0%]  (Warmup)\nChain 1: Iteration:  150 / 1500 [ 10%]  (Warmup)\nChain 1: Iteration:  300 / 1500 [ 20%]  (Warmup)\nChain 1: Iteration:  450 / 1500 [ 30%]  (Warmup)\nChain 1: Iteration:  501 / 1500 [ 33%]  (Sampling)\nChain 1: Iteration:  650 / 1500 [ 43%]  (Sampling)\nChain 1: Iteration:  800 / 1500 [ 53%]  (Sampling)\nChain 1: Iteration:  950 / 1500 [ 63%]  (Sampling)\nChain 1: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nChain 1: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nChain 1: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nChain 1: Iteration: 1500 / 1500 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 1.682 seconds (Warm-up)\nChain 1:                0.865 seconds (Sampling)\nChain 1:                2.547 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000143 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.43 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 1500 [  0%]  (Warmup)\nChain 2: Iteration:  150 / 1500 [ 10%]  (Warmup)\nChain 2: Iteration:  300 / 1500 [ 20%]  (Warmup)\nChain 2: Iteration:  450 / 1500 [ 30%]  (Warmup)\nChain 2: Iteration:  501 / 1500 [ 33%]  (Sampling)\nChain 2: Iteration:  650 / 1500 [ 43%]  (Sampling)\nChain 2: Iteration:  800 / 1500 [ 53%]  (Sampling)\nChain 2: Iteration:  950 / 1500 [ 63%]  (Sampling)\nChain 2: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nChain 2: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nChain 2: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nChain 2: Iteration: 1500 / 1500 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.659 seconds (Warm-up)\nChain 2:                0.88 seconds (Sampling)\nChain 2:                2.539 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'stanmarg' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000163 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.63 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 1500 [  0%]  (Warmup)\nChain 3: Iteration:  150 / 1500 [ 10%]  (Warmup)\nChain 3: Iteration:  300 / 1500 [ 20%]  (Warmup)\nChain 3: Iteration:  450 / 1500 [ 30%]  (Warmup)\nChain 3: Iteration:  501 / 1500 [ 33%]  (Sampling)\nChain 3: Iteration:  650 / 1500 [ 43%]  (Sampling)\nChain 3: Iteration:  800 / 1500 [ 53%]  (Sampling)\nChain 3: Iteration:  950 / 1500 [ 63%]  (Sampling)\nChain 3: Iteration: 1100 / 1500 [ 73%]  (Sampling)\nChain 3: Iteration: 1250 / 1500 [ 83%]  (Sampling)\nChain 3: Iteration: 1400 / 1500 [ 93%]  (Sampling)\nChain 3: Iteration: 1500 / 1500 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.93 seconds (Warm-up)\nChain 3:                0.873 seconds (Sampling)\nChain 3:                2.803 seconds (Total)\nChain 3: \nComputing post-estimation metrics (including lvs if requested)...\n\nsummary(k_fit_blavaan)\n\nblavaan 0.5.2 ended normally after 1000 iterations\n\n  Estimator                                      BAYES\n  Optimization method                             MCMC\n  Number of model parameters                         5\n\n  Number of observations                            90\n\n  Statistic                                 MargLogLik         PPP\n  Value                                             NA       0.509\n\nParameter Estimates:\n\n\nRegressions:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n  rich ~                                                                       \n    firesev          -2.658    0.981   -4.572   -0.725    1.000    normal(0,10)\n    cover             8.115    4.637   -1.135   17.104    0.999    normal(0,10)\n  cover ~                                                                      \n    firesev          -0.084    0.019   -0.121   -0.046    1.000    normal(0,10)\n\nVariances:\n                   Estimate  Post.SD pi.lower pi.upper     Rhat    Prior       \n   .rich            183.777   26.783  138.601  241.632    1.000 gamma(1,.5)[sd]\n   .cover             0.084    0.012    0.063    0.111    1.000 gamma(1,.5)[sd]\n\nlavaanPlot(model=k_fit_blavaan, coefs = TRUE, sig = .05)"
  },
  {
    "objectID": "4F-BayesianSEMs.html#piecewise-sems",
    "href": "4F-BayesianSEMs.html#piecewise-sems",
    "title": "13  Bayesian causal models",
    "section": "13.2 Piecewise SEMs",
    "text": "13.2 Piecewise SEMs\nThe piecewiseSEM allows you to create a causal model from a set of standard GLMMs. Advantage is that it can handle standard GLMMs. Disadvantage is that it can’t handle latent variables\n\nk_fit_psem &lt;- psem(\n  lm(rich ~ firesev + cover, data=keeley),\n  lm(cover ~ firesev, data=keeley),\n  data = keeley\n)\nsummary(k_fit_psem)\n\n\nStructural Equation Model of k_fit_psem \n\nCall:\n  rich ~ firesev + cover\n  cover ~ firesev\n\n    AIC\n 768.968\n\n---\nTests of directed separation:\n\n No independence claims present. Tests of directed separation not possible.\n\n--\nGlobal goodness-of-fit:\n\nChi-Squared = 0 with P-value = 1 and on 0 degrees of freedom\nFisher's C = NA with P-value = NA and on 0 degrees of freedom\n\n---\nCoefficients:\n\n  Response Predictor Estimate Std.Error DF Crit.Value P.Value Std.Estimate    \n      rich   firesev  -2.5308    0.9926 87    -2.5496  0.0125      -0.2768   *\n      rich     cover   9.9105    5.1701 87     1.9169  0.0585       0.2081    \n     cover   firesev  -0.0839    0.0184 88    -4.5594  0.0000      -0.4371 ***\n\n  Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05\n\n---\nIndividual R-squared:\n\n  Response method R.squared\n      rich   none      0.17\n     cover   none      0.19\n\n\nPiecewise SEMs can pretty much in the same way be code in brm:\n\nlibrary(brms)\n\nLoading 'brms' package (version 2.17.0). Useful instructions\ncan be found by typing help('brms'). A more detailed introduction\nto the package is available through vignette('brms_overview').\n\n\n\nAttaching package: 'brms'\n\n\nThe following object is masked from 'package:stats':\n\n    ar\n\nrich_mod &lt;- bf(rich ~ firesev + cover)\ncover_mod &lt;- bf(cover ~ firesev)\n\nk_fit_brms &lt;- brm(rich_mod +\n                  cover_mod + \n                  set_rescor(FALSE), \n                data=keeley,\n                cores=4, chains = 2)\n\nCompiling Stan program...\n\n\nTrying to compile a simple C file\n\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\n\nStart sampling\n\nplot(k_fit_brms)"
  },
  {
    "objectID": "4F-BayesianSEMs.html#freestyle-jags-model",
    "href": "4F-BayesianSEMs.html#freestyle-jags-model",
    "title": "13  Bayesian causal models",
    "section": "13.3 Freestyle JAGS model",
    "text": "13.3 Freestyle JAGS model\nIf you want latent variables and non-Gaussian responses, you will have to code by hand. Here an example, which is effectively a piecewise SEM bout could be extended to include latent variables.\nTODO\n\nlibrary(EcoData)\nlibrary(effects)\n\n\nislandPsem &lt;- psem(\n  lm(windObs ~ sAltitude, data = volcanoisland),\n  glm(lizardsObs ~ sAltitude , family = binomial, data = volcanoisland),\n  glm(beetles ~ windObs + lizardsObs,  family = poisson, data = volcanoisland)\n)\nsummary(islandPsem)\n\n\n\nlibrary(rjags)\nlibrary(R2jags)\n\n# 1) Model definition exactly how we created our data \nmodelCode = \"\nmodel{\n\n  # Likelihood\n  for(i in 1:i.max){\n\n    Wind[i] ~ dnorm(windPred[i], windPrec)\n    windPred[i] &lt;- intW + altW*Alt[i]\n\n    Ducks[i] ~ dbern(lambdaD[i])\n    logit(lambdaD[i]) &lt;- intD + altD*Alt[i] + habitatD * Habitat[i] + soilD * SoilTexture[i]\n\n    Beetles[i] ~ dpois(lambda[i])\n    lambda[i] &lt;- exp(mu[i] )  \n    mu[i] &lt;- intB + altB*Alt[i] + alt2B*Alt[i]*Alt[i] + windB * Wind[i] + OLREB[i]\n  }\n\n  # Prior distributions\n  intB ~ dnorm(0,0.001)\n  altB ~ dnorm(0,0.001)\n  alt2B ~ dnorm(0,0.001)\n  windB ~ dnorm(0,0.001)\n  for(i in 1:i.max){\n    OLREB[i] ~ dnorm(0,precOLREB)\n  }\n  precOLREB ~ dgamma(0.001,0.001)\n\n  intW ~ dnorm(0,0.001)\n  altW ~ dnorm(0,0.001)\n  windPrec &lt;- 1/(windSD * windSD)\n  windSD ~ dunif(0,100)\n\n  intD ~ dnorm(0,0.001)\n  altD ~ dnorm(0,0.001)\n  habitatD ~ dnorm(0,0.001)\n  soilD ~ dnorm(0,0.001)\n\n  # posterior predictive simulations\n  for(i in 1:i.max){\n    yPred[i] ~ dpois(lambda[i])\n  }\n}\n\"\n\nwindPartiallObs &lt;- islandData$windObs\nsel = sample.int(1000, 500)\nwindPartiallObs[sel] = NA\n\n# 2) Set up a list that contains all the necessary data (here, including parameters of the prior distribution)\nData = list(Beetles = islandData$beetles, Alt = islandData$sAltitude, i.max = length(islandData$sAltitude), Wind = windPartiallObs, Ducks = islandData$ducks, Habitat = islandData$habitatQuality, SoilTexture = islandData$earth)\n\n# 3) Specify a function to generate inital values for the parameters\n\n# Out of laziness, we don't provide inits for the other parameters. For a real study, provide overdispersed sampling functions for all parameters\ninits.fn &lt;- function() list(intB = rnorm(1), altB = rnorm(1), alt2B = rnorm(1))\n\n\nlibrary(R2jags)\n\nR2JagsResults &lt;- jags(data=Data, inits=inits.fn, parameters.to.save=c(\"intB\",\"altB\",\"alt2B\", \"intW\", \"altW\", \"windB\", \"intD\", \"altD\", \"habitatD\", \"soilD\"), n.chains=3, n.iter=10000, model.file=textConnection(modelCode), DIC = F)\n\nplot(R2JagsResults)\nprint(R2JagsResults)\n\n\n\nlibrary(DHARMa)\nsimulations = R2JagsResults$BUGSoutput$sims.list$yPred\npred = apply(simulations, 2, median)\ndim(simulations)\nsim = createDHARMa(simulatedResponse = t(simulations), observedResponse = islandData$beetles, fittedPredictedResponse = pred, integerResponse = T)\nplot(sim)\n\n\nplotResiduals(islandData$year, sim$scaledResiduals, asFactor = T)\n\ntestSpatialAutocorrelation(sim)"
  },
  {
    "objectID": "4G-ProcessBased.html",
    "href": "4G-ProcessBased.html",
    "title": "14  Process-based models",
    "section": "",
    "text": "library(BayesianTools)\n\n# Create input data for the model\nPAR &lt;- VSEMcreatePAR(1:1000)\nplot(PAR, main = \"PAR (driving the model)\", xlab = \"Day\")\n\n\n\n# load reference parameter definition (upper, lower prior)\nrefPars &lt;- VSEMgetDefaults()\n# this adds one additional parameter for the likelihood standard deviation (see below)\nrefPars[12,] &lt;- c(2, 0.1, 4) \nrownames(refPars)[12] &lt;- \"error-sd\"\nhead(refPars)\n\n           best lower upper\nKEXT      0.500 2e-01 1e+00\nLAR       1.500 2e-01 3e+00\nLUE       0.002 5e-04 4e-03\nGAMMA     0.400 2e-01 6e-01\ntauV   1440.000 5e+02 3e+03\ntauS  27370.000 4e+03 5e+04\n\n# create some simulated test data \n# generally recommended to start with simulated data before moving to real data\nreferenceData &lt;- VSEM(refPars$best[1:11], PAR) # model predictions with reference parameters  \nreferenceData[,1] = 1000 * referenceData[,1] \n# this adds the error - needs to conform to the error definition in the likelihood\nobs &lt;- referenceData + rnorm(length(referenceData), sd = refPars$best[12])\noldpar &lt;- par(mfrow = c(2,2))\nfor (i in 1:4) plotTimeSeries(observed = obs[,i], \n                              predicted = referenceData[,i], main = colnames(referenceData)[i])\n\n\n\n# Best to program in a way that we can choose easily which parameters to calibrate\nparSel = c(1:6, 12)\n\n# here is the likelihood \nlikelihood &lt;- function(par, sum = TRUE){\n  # set parameters that are not calibrated on default values \n  x = refPars$best\n  x[parSel] = par\n  predicted &lt;- VSEM(x[1:11], PAR) # replace here VSEM with your model \n  predicted[,1] = 1000 * predicted[,1] # this is just rescaling\n  diff &lt;- c(predicted[,1:4] - obs[,1:4]) # difference betweeno observed and predicted\n  # univariate normal likelihood. Note that there is a parameter involved here that is fit\n  llValues &lt;- dnorm(diff, sd = x[12], log = TRUE)  \n  if (sum == FALSE) return(llValues)\n  else return(sum(llValues))\n}\n\n# optional, you can also directly provide lower, upper in the createBayesianSetup, see help\nprior &lt;- createUniformPrior(lower = refPars$lower[parSel], \n                            upper = refPars$upper[parSel], best = refPars$best[parSel])\n\nbayesianSetup &lt;- createBayesianSetup(likelihood, prior, names = rownames(refPars)[parSel])\n\n# settings for the sampler, iterations should be increased for real applicatoin\nsettings &lt;- list(iterations = 2000, nrChains = 2)\n\nout &lt;- runMCMC(bayesianSetup = bayesianSetup, sampler = \"DEzs\", settings = settings)\n\n\n Running DEzs-MCMC, chain  1 iteration 300 of 2001 . Current logp  -8566.156 -8499.593 -8507.241 . Please wait! \n\n Running DEzs-MCMC, chain  1 iteration 600 of 2001 . Current logp  -8489.945 -8489.103 -8485.902 . Please wait! \n\n Running DEzs-MCMC, chain  1 iteration 900 of 2001 . Current logp  -8486.072 -8489.803 -8489.643 . Please wait! \n\n Running DEzs-MCMC, chain  1 iteration 1200 of 2001 . Current logp  -8486.468 -8485.746 -8485.014 . Please wait! \n\n Running DEzs-MCMC, chain  1 iteration 1500 of 2001 . Current logp  -8485.923 -8491.022 -8486.336 . Please wait! \n\n Running DEzs-MCMC, chain  1 iteration 1800 of 2001 . Current logp  -8485.298 -8487.846 -8484.556 . Please wait! \n\n Running DEzs-MCMC, chain  1 iteration 2001 of 2001 . Current logp  -8485.874 -8485.425 -8487.278 . Please wait! \n\n\nrunMCMC terminated after 1.579seconds\n\n\n\n Running DEzs-MCMC, chain  2 iteration 300 of 2001 . Current logp  -8561.552 -8563.564 -8569.686 . Please wait! \n\n Running DEzs-MCMC, chain  2 iteration 600 of 2001 . Current logp  -8538.887 -8538.977 -8546.54 . Please wait! \n\n Running DEzs-MCMC, chain  2 iteration 900 of 2001 . Current logp  -8502.444 -8491.463 -8522.535 . Please wait! \n\n Running DEzs-MCMC, chain  2 iteration 1200 of 2001 . Current logp  -8486.893 -8487.119 -8487.553 . Please wait! \n\n Running DEzs-MCMC, chain  2 iteration 1500 of 2001 . Current logp  -8487.805 -8486.57 -8484.996 . Please wait! \n\n Running DEzs-MCMC, chain  2 iteration 1800 of 2001 . Current logp  -8484.447 -8487.071 -8488.836 . Please wait! \n\n Running DEzs-MCMC, chain  2 iteration 2001 of 2001 . Current logp  -8485.458 -8488.215 -8485.654 . Please wait! \n\n\nrunMCMC terminated after 1.828seconds\n\n\n\nplot(out)\n\n\n\n\n\n\nsummary(out)\n\n# # # # # # # # # # # # # # # # # # # # # # # # # \n## MCMC chain summary ## \n# # # # # # # # # # # # # # # # # # # # # # # # # \n \n# MCMC sampler:  DEzs \n# Nr. Chains:  6 \n# Iterations per chain:  667 \n# Rejection rate:  0.909 \n# Effective sample size:  72 \n# Runtime:  3.407  sec. \n \n# Parameters\n           psf       MAP      2.5%    median     97.5%\nKEXT     1.210     0.438     0.364     0.549     0.807\nLAR      2.261     2.708     1.358     2.433     2.965\nLUE      5.705     0.002     0.002     0.002     0.002\nGAMMA    6.758     0.515     0.298     0.450     0.527\ntauV     1.189  1475.404  1190.045  1471.547  2867.823\ntauS     1.425 31318.063 12105.862 25200.275 37808.067\nerror-sd 1.072     2.016     1.873     2.012     2.202\n\n## DIC:  17263.1 \n## Convergence \n Gelman Rubin multivariate psrf:  8.746 \n \n## Correlations \n           KEXT    LAR    LUE  GAMMA   tauV   tauS error-sd\nKEXT      1.000 -0.228 -0.345 -0.286  0.333  0.050   -0.102\nLAR      -0.228  1.000  0.612  0.592 -0.129 -0.052    0.040\nLUE      -0.345  0.612  1.000  0.890  0.029  0.067    0.108\nGAMMA    -0.286  0.592  0.890  1.000  0.080  0.265   -0.017\ntauV      0.333 -0.129  0.029  0.080  1.000  0.170   -0.057\ntauS      0.050 -0.052  0.067  0.265  0.170  1.000   -0.338\nerror-sd -0.102  0.040  0.108 -0.017 -0.057 -0.338    1.000\n\nmarginalPlot(out)\n\n\n\ngelmanDiagnostics(out) # should be below 1.05 for all parameters to demonstrate convergence \n\nPotential scale reduction factors:\n\n         Point est. Upper C.I.\nKEXT           1.21       1.51\nLAR            2.26       3.84\nLUE            5.70       9.63\nGAMMA          6.76      11.61\ntauV           1.19       1.50\ntauS           1.42       2.02\nerror-sd       1.07       1.17\n\nMultivariate psrf\n\n8.75\n\n\n\n# Posterior predictive simulations\n\n# Create a prediction function\ncreatePredictions &lt;- function(par){\n  # set the parameters that are not calibrated on default values \n  x = refPars$best\n  x[parSel] = par\n  predicted &lt;- VSEM(x[1:11], PAR) # replace here VSEM with your model \n  return(predicted[,1] * 1000)\n}\n\n# Create an error function\ncreateError &lt;- function(mean, par){\n  return(rnorm(length(mean), mean = mean, sd = par[7]))\n}\n\n# plot prior predictive distribution and prior predictive simulations\nplotTimeSeriesResults(sampler = out, model = createPredictions, observed = obs[,1],\n                      error = createError, prior = TRUE, main = \"Prior predictive\")\n\nDHARMa::plotTimeSeriesResults called with posterior predictive (residual) diagnostics. Type vignette(\"DHARMa\", package=\"DHARMa\") for a guide on how to interpret these plots\n\n\n\n\n# plot posterior predictive distribution and posterior predictive simulations\nplotTimeSeriesResults(sampler = out, model = createPredictions, observed = obs[,1],\n                      error = createError, main = \"Posterior predictive\")\n\nDHARMa::plotTimeSeriesResults called with posterior predictive (residual) diagnostics. Type vignette(\"DHARMa\", package=\"DHARMa\") for a guide on how to interpret these plots"
  },
  {
    "objectID": "4H-ApproximateBayesian.html#motivation",
    "href": "4H-ApproximateBayesian.html#motivation",
    "title": "15  Approximate Bayesian Inference",
    "section": "15.1 Motivation",
    "text": "15.1 Motivation\nIn Hartig, F.; Calabrese, J. M.; Reineking, B.; Wiegand, T. & Huth, A. (2011) Statistical inference for stochastic simulation models - theory and application. Ecol. Lett., 14, 816-827, we classify two main competing methods for models via simulation-based likelihood approximation\n\nLikelihood-approximations based on local, non-parametric approximations of the variance of the simulation outputs, particularly Approximate Bayesian Computation (ABC, Beaumont, M. A. (2010) Approximate Bayesian computation in evolution and ecology. Annu. Rev. Ecol. Evol. Syst., 41, 379-406.)\nLikelihood-approximations based on parametric, typically global approximation of the simulation output such as Synthetic Likelihood, see Wood, S. N. (2010) Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466, 1102-1104. An example for fitting a stochastic forest gap model do data via this method is Hartig, F.; Dislich, C.; Wiegand, T. & Huth, A. (2014) Technical Note: Approximate Bayesian parameterization of a process-based tropical forest model. Biogeosciences, 11, 1261-1272."
  },
  {
    "objectID": "4H-ApproximateBayesian.html#example",
    "href": "4H-ApproximateBayesian.html#example",
    "title": "15  Approximate Bayesian Inference",
    "section": "15.2 Example",
    "text": "15.2 Example\nAssume we have a stochastic model that we want to fit. It takes one parameter, and has an output of 10 values which happen to be around the mean of the parameter that we put in\n\nstochasticModel &lt;- function(par){\n  if (par[2] + par[1] &lt;= 0) return(rep(-9999,20))\n  else return(rnorm(20, mean = (2.7*par[1] * par[2]), sd = par[2] + par[1] ))\n}\n\nLets’s create some data with known parameters\n\ndata &lt;- stochasticModel(c(3,-2))\n\n\n15.2.1 Summary statistics\nWe want to use ABC / synthetic likelihood to infer the parameters that were used. Both ABC and synthetic likelihoods require summary statistics, we use mean and sd of the data.\n\nmeandata &lt;- mean(data)\nstandarddeviationdata &lt;- sd(data)\n\n\n\n15.2.2 ABC-MCMC solution\nFollowing Marjoram, P.; Molitor, J.; Plagnol, V. & Tavare, S. (2003) Markov chain Monte Carlo without likelihoods. Proc. Natl. Acad. Sci. USA, 100, 15324-15328, we plug the ABC acceptance into a standard metropolis hastings MCMC.\n\nlibrary(coda)\nrun_MCMC_ABC &lt;- function(startvalue, iterations){\n \n    chain = array(dim = c(iterations+1,2))\n    chain[1,] = startvalue\n \n    for (i in 1:iterations){\n         \n        # proposalfunction\n        proposal = rnorm(2,mean = chain[i,], sd= c(0.2,0.2))\n        \n        simulation &lt;- stochasticModel(proposal)\n \n        # comparison with the observed summary statistics\n        diffmean &lt;- abs(mean(simulation) - meandata)\n        diffsd &lt;- abs(sd(simulation) - standarddeviationdata)\n          \n        if((diffmean &lt; 0.3) & (diffsd &lt; 0.3)){\n            chain[i+1,] = proposal\n        }else{\n            chain[i+1,] = chain[i,]\n        }\n    }\n    return(mcmc(chain))\n}\n \nposterior &lt;- run_MCMC_ABC(c(3,-2),50000)\nplot(posterior)\n\n\n\n\n\n\n15.2.3 Synthetic likelihood\nFollowing Wood, S. N. (2010) Statistical inference for noisy nonlinear ecological dynamic systems. Nature, 466, 1102-1104 and Hartig, F.; Dislich, C.; Wiegand, T. & Huth, A. (2014) Technical Note: Approximate Bayesian parameterization of a process-based tropical forest model. Biogeosciences, 11, 1261-1272, the synthetic likelihood approach is based on sampling a few times from the model, and approximating the likelihood by fitting a Gaussian distribution to the simulation outputs:\n\nrun_MCMC_Synthetic &lt;- function(startvalue, iterations){\n \n    chain = array(dim = c(iterations+1,2))\n    chain[1,] = startvalue\n \n    for (i in 1:iterations){\n         \n        # proposalfunction\n        proposal = rnorm(2,mean = chain[i,], sd= c(0.2,0.2))\n        \n        # simulate several model runs\n        simualatedData &lt;- matrix(NA, nrow = 100, ncol = 2)\n        for (i in 1:100){\n          simulation &lt;- stochasticModel(c(3,-2))\n          simualatedData[i,] &lt;- c(mean(simulation) , sd(simulation))          \n        }\n        syntheticLikelihood1 &lt;- fitdistr(simualatedData[1,], \"normal\")\n        syntheticLikelihood2 &lt;- fitdistr(simualatedData[2,], \"normal\")\n        \n        prob1 &lt;- dnorm(meandata-syntheticLikelihood1$estimate[1], sd = syntheticLikelihood1$estimate[2], log = T)\n\n        prob2 &lt;- dnorm(standarddeviationdata-syntheticLikelihood2$estimate[1], sd = syntheticLikelihood2$estimate[2], log = T)\n        \n        if(prob &lt; runif(1)){\n            chain[i+1,] = proposal\n        }else{\n            chain[i+1,] = chain[i,]\n        }\n    }\n    return(mcmc(chain))\n}\n \nposterior &lt;- run_MCMC_ABC(c(3,-2),20000)\nplot(posterior)"
  },
  {
    "objectID": "4H-ApproximateBayesian.html#a-movement-model-fit-with-abc-rejection-and-abc-mcmc",
    "href": "4H-ApproximateBayesian.html#a-movement-model-fit-with-abc-rejection-and-abc-mcmc",
    "title": "15  Approximate Bayesian Inference",
    "section": "15.3 A movement model fit with ABC-Rejection and ABC-MCMC",
    "text": "15.3 A movement model fit with ABC-Rejection and ABC-MCMC\nThe ABC rejection was originally proposed by Tavare, 1997. The ABC-MCMC was suggested by Marjoram, P.; Molitor, J.; Plagnol, V. & Tavare, S. (2003) Markov chain Monte Carlo without likelihoods. Proc. Natl. Acad. Sci. USA, 100, 15324-15328.\nCode implemented by Florian Hartig, following the pseudocode from Hartig, F.; Calabrese, J. M.; Reineking, B.; Wiegand, T. & Huth, A. (2011) Statistical inference for stochastic simulation models - theory and application. Ecol. Lett., 14, 816-827., supporting information.\n\n15.3.1 The model and data\n\n15.3.1.1 Process-model\n\nlibrary(compiler)\nmodel &lt;- function(params=2, startvalues = c(0,0,0), steps = 200){\n  x = startvalues[1]\n  y = startvalues[2]\n  direction = startvalues[3]\n  \n  movementLength = params[1]\n  turningWidth = 1\n  \n  output = data.frame(x = rep(NA, steps+1), y = rep(NA, steps+1))\n  output[1, ] = c(x,y)\n  for (i in 1:steps){\n    direction = direction + rnorm(1,0,turningWidth)\n    length = rexp(1, 1/movementLength)\n    x = x + sin(direction) * length\n    y = y + cos(direction) * length\n    output[i+1, ] = c(x,y)\n  }\nreturn(output)\n}\nmodel &lt;- cmpfun(model)\n\nLet’s see what the model does\n\ndata &lt;- model()\nplot(data, type = \"l\")\n\n\n\n\n\n\n15.3.1.2 Observation model\nAssume we have recorded the test data. In fact, let’s do it a bit harder. Assume we observe with error, and our measurement device has a problem - if the x-values have a digit larger than 0.7, we get an NA\n\nobservationModel &lt;- function(realData, sd=1){\n  realData$xobs = rnorm(length(realData$x), mean = realData$x, sd = sd)\n  realData$yobs = rnorm(length(realData$x), mean = realData$y, sd = sd)\n  realData$xobs[realData$xobs - floor(realData$xobs) &gt; 0.7 ] = NA\n  return(realData)\n}\n\nobsdata &lt;- observationModel(data)\nplot(data, type = \"l\")\npoints(obsdata$xobs, obsdata$yobs, col = \"red\", pch = 4)\n\n\n\n\n\n\n\n15.3.2 Summary statistics\n\nsummaryStatistics &lt;- function(dat){\n  meandisplacement = mean(sqrt(diff(dat$xobs)^2 + diff(dat$yobs)^2), na.rm = T)\n\n  meandisplacement10 = mean(sqrt(diff(dat$xobs, lag = 2)^2 + diff(dat$yobs, lag = 2)^2), na.rm = T)/3\n  \n  #meanturning = mean(abs(diff(atan2(diff(dat$yobs),diff(dat$xobs)))), na.rm = T) \n  \n  return(c(meandisplacement, meandisplacement10))\n} \n\ndataSummary &lt;- summaryStatistics(obsdata)\ndataSummary\n\n[1] 2.803799 1.433168\n\n\n\n\n15.3.3 ABC rejection algorithm\n\nn = 10000\nfit = data.frame(movementLength = runif(n, 0, 5), error = runif(n, 0,5), distance = rep(NA, n))\n\nfor (i in 1:n){\n  simulatedPrediction &lt;- model(fit[i,1])\n  simulatedObservation&lt;- observationModel(simulatedPrediction, fit[i,2])\n  simulatedSummary &lt;- summaryStatistics(simulatedObservation)\n  simulatedSummary\n  #deviation = max( simulatedSummary - dataSummary)\n  deviation = sqrt(sum((simulatedSummary - dataSummary)^2))\n  fit[i,3] = deviation\n}\n\nI had already calculated the euclidian distance between observed and simulated summaries. We now plot parameters for different acceptance intervals\n\nplot(fit[fit[,3] &lt; 1, 1:2], xlim = c(0,6), ylim = c(0,5), col = \"lightgrey\", main = \"Accepted parameters for \\n different values of epsilon\")\npoints(fit[fit[,3] &lt; 0.2, 1:2],  pch = 18, col = \"gray\")\npoints(fit[fit[,3] &lt; 0.1, 1:2],  pch = 8, col = \"red\")\n\nlegend(\"topright\", c(\"&lt; 1\", \"&lt; 0.2\", \"&lt; 0.1\"), pch = c(1,18,8), col = c(\"lightgrey\", \"gray\", \"red\"))\n\nabline(v = 2)\nabline(h = 1) \n\n\n\n\n\n\n15.3.4 ABC-MCMC Algorithm\n\nn = 10000\nfit = data.frame(movementLength = rep(NA, n), error = rep(NA, n), distance = rep(NA, n))\n\n\ncurrentPar = c(2,0.9)\nfor (i in 1:n){\n  newPar = rnorm(2,currentPar, sd = c(0.2,0.2))\n  if (min(newPar) &lt; 0 ) fit[i,] = c(currentPar, deviation)\n  \n  else{\n    simulatedPrediction &lt;- model(newPar[1])\n    simulatedObservation&lt;- observationModel(simulatedPrediction, newPar[2])\n    simulatedSummary &lt;- summaryStatistics(simulatedObservation)\n    deviation = sqrt(sum( simulatedSummary - dataSummary)^2)\n    \n    if (deviation &lt; 0.2){\n      fit[i,] = c(newPar, deviation)\n      currentPar = newPar\n    } \n    else fit[i,] = c(currentPar, deviation)\n  }\n}\n\n\nplot(fit[, 1:2], xlim = c(0,6), ylim = c(0,5), col = \"#00000022\", main = \"Accepted parameters\")\n\nabline(v = 2)\nabline(h = 1)"
  },
  {
    "objectID": "5-Summary.html#further-readings",
    "href": "5-Summary.html#further-readings",
    "title": "16  Summary and Conclusions",
    "section": "16.1 Further readings",
    "text": "16.1 Further readings\n\n16.1.1 Textbooks\n\n16.1.1.1 Basic for Ecologists\n\nKéry, M. (2010) Introduction to WinBUGS for Ecologists. Academic Press.\nKruschke, J. F. (2010) Doing Bayesian Data Analysis: A Tutorial with R and BUGS. Academic Press.\nKorner-Nievergelt, F.; Roth, T.; von Felten, S.; Almasi, B. & Korner-Nievergelt, P. (2015) Bayesian Data Analysis in Ecology Using Linear Models with R, BUGS, and Stan.\n\n\n\n16.1.1.2 Comprehensive\n\nLunn D. et al. (2012) The BUGS Book: A Practical Introduction to Bayesian Analysis. Chapman and Hall/CRC.\nGelman, A.; Carlin, J. B.; Stern, H. S. & Rubin, D. B. (2003) Bayesian Data Analysis. Chapman & Hall, London.\n\n\n\n16.1.1.3 Hierarchical\n\nKéry, M. and Schaub, M. (2011) Bayesian population analysis using WinBUGS. Academic Press.\nKéry, M. & Royle, J. A.(2016) Applied Hierarchical Modeling in Ecology: Analysis of distribution, abundance and species richness in R and BUGS: Volume 1: Prelude and Static Models\nBanerjee, S. et al. (2009) Hierarchical Modeling and Anallysis for Spatial Data. Chapman and Hall/CRC.\nClark, J. S. and Gelfand, A. E. (2006) Hierarchical Modelling for the Environmental Sciences. Oxford University Press.\n\n\n\n16.1.1.4 Classics\n\nJaynes, E. T. (2003) Probability theory: the logic of science. Cambridge university press.\n\n\n\n\n16.1.2 Important articles\n\n16.1.2.1 Reviews / opinion papers on Bayesian methods in ecology\n\nHobbs, N. T. & Hilborn, R. (2006) Alternatives to statistical hypothesis testing in ecology: A guide to self teaching Ecol. Appl., 16, 5-19\nEllison, A. M. (2004) Bayesian inference in ecology Ecol. Lett., 7, 509-520\n\n\n\n16.1.2.2 Prior choice\n\nKass, R. E. & Wasserman, L. (1996) The selection of prior distributions by formal rules. J. Am. Stat. Assoc., 91, 1343-1370\n\n\n\n16.1.2.3 Foundations of Bayesian statistics, Bayes vs. Frequentists\n\nEfron, B. (2013) A 250-year argument: Belief, behavior, and the bootstrap Bulletin Of The American Mathematical Society, 50, 129-146\nGelman, A. & Robert, C. P. (2010) “Not only defended but also applied”: The perceived absurdity of Bayesian inference ArXiv e-prints\nFisher, R. A. (1922) On the mathematical foundations of theoretical statistics Philos. T. Roy. Soc. A., 222, 309-368\nKass, R. (2011) Statistical inference: The big picture Stat. Sci., 26, 1-9\nJaynes, E. (1976) Confidence intervals vs. Bayesian intervals Foundations of probability theory, statistical inference, and statistical theories of science, 2, 175-257.\n\n\n\n16.1.2.4 MCMC sampling\n\nAndrieu, C.; de Freitas, N.; Doucet, A. & Jordan, M. I. (2003) An introduction to MCMC for machine learning Mach. Learning, 50, 5-43\nAndrieu, C. & Thoms, J. (2008) A tutorial on adaptive MCMC Stat. Comput., 18, 343-373.\n\n\n\n16.1.2.5 Summaries of the posterior\nhttp://www.sumsar.net/blog/2014/10/probable-points-and-credible-intervals-part-one/\n\n\n\n16.1.3 Hierarchical Models\n\nWikle, C. K. (2003) Hierarchical Bayesian models for predicting the spread of ecological processes Ecology, 84, 1382-1394\nClark, J. S. (2003) Uncertainty and variability in demography and population growth: A hierarchical approach Ecology, 84, 1370-1381\nClark, J. S. & Gelfand, A. E. (2006) A future for models and data in environmental science. Trends in Ecology & Evolution, 21, 375-380\nCressie, N.; Calder, C. A.; Clark, J. S.; Hoef, J. M. V. & Wikle, C. K. (2009) Accounting for uncertainty in ecological analysis: the strengths and limitations of hierarchical statistical modeling Ecol. Appl., 19, 553-570\nMarion, G.; McInerny, G. J.; Pagel, J.; Catterall, S.; Cook, A. R.; Hartig, F. & O’Hara, R. B. (2012) Parameter and uncertainty estimation for process-oriented population and distribution models: data, statistics and the niche J. Biogeogr., 39, 2225–2239\nCook, A.; Marion, G.; Butler, A. & Gibson, G. (2007) Bayesian Inference for the Spatio-Temporal Invasion of Alien Species Bull. Math. Biol., 69, 2005-2025\nPagel, J. & Schurr, F. M. (2011) Forecasting species ranges by statistical estimation of ecological niches and spatial population dynamics Global Ecol. Biogeogr.\n\n\n\n16.1.4 Bayesian Model selection\n\nKass, R. E. & Raftery, A. E. (1995) Bayes Factors J. Am. Stat. Assoc., 90, 773-795\n\nhttps://radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-monte-carlo-method-ever/\nhttp://www.biomedcentral.com/1471-2105/14/85\nhttp://hedibert.org/bayes-factor-computing-marginal-likelihoods-savage-dickey-ratio-reversible-jump-mcmc-bayesian-model-averaging-and-deviance-information-criterion/\n\n\n16.1.5 Fitting (stochastic) process-based models\n\nVan Oijen, M.; Rougier, J. & Smith, R. (2005) Bayesian calibration of process-based forest models: bridging the gap between models and data Tree Physiol., 25, 915-927\nBeaumont, M. A. (2010) Approximate Bayesian computation in evolution and ecology Annu. Rev. Ecol. Evol. Syst., 41, 379-406\nCsilléry, K.; Blum, M. G. B.; Gaggiotti, O. E. & François, O. (2010) Approximate Bayesian Computation (ABC) in practice Trends in Ecology & Evolution, 25, 410-418\nHartig, F.; Calabrese, J. M.; Reineking, B.; Wiegand, T. & Huth, A. (2011) Statistical inference for stochastic simulation models – theory and application Ecol. Lett., 14, 816-827\nJabot, F. & Chave, J. (2009) Inferring the parameters of the neutral theory of biodiversity using phylogenetic information and implications for tropical forests Ecol. Lett., 12, 239-248\nHartig, F.; Dyke, J.; Hickler, T.; Higgins, S. I.; O’Hara, R. B.; Scheiter, S. & Huth, A. (2012) Connecting dynamic vegetation models to data – an inverse perspective J. Biogeogr., 39, 2240-2252."
  },
  {
    "objectID": "6B-BayesianNumerics.html#rejection-sampling",
    "href": "6B-BayesianNumerics.html#rejection-sampling",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.1 Rejection Sampling",
    "text": "B.1 Rejection Sampling\n\nB.1.1 Overview\nRejection sampling is the most basic Monte Carlo sampler around. In terms of computing time it is typically less efficient than MCMCs and SMCs, but it does have some advantages that make it interesting. For example, it is trivially parallelizable. Also, rejection sampling sometimes comes in handy when you want to merge an existing sample (e.g. from an MCMC) with another distribution.\n\n\nB.1.2 How it works\nThe idea of rejection sampling is that you create a sample from a distribution by drawing random values, and accept them proportional to their value of the distribution.\nIn a Bayesian setting, this typically means that you create draws from the posterior by drawing randomly from the prior, and then accepting proportional to the likelihood. You could of course also draw randomly from the whole space, and accept from the posterior, but this would likely be less efficient.\n\n\nB.1.3 An example in R\nAssume we want to draw from a beta distribution with shape parameters 6,3, which looks like this\n\ncurve(dbeta(x, 3,6),0,1)\n\n\n\n\nTo do this, we first create a data.frame with 100000 random values between 0 and 1, and calculate their beta density values\n\nsampled &lt;- data.frame(proposal = runif(100000,0,1))\nsampled$targetDensity &lt;- dbeta(sampled$proposal, 3,6)\n\nNow, accept proportional to the targetDensity. It’s easiest if we calculate the highest density value, and then accept the others in relation to that\n\nmaxDens = max(sampled$targetDensity, na.rm = T)\nsampled$accepted = ifelse(runif(100000,0,1) &lt; sampled$targetDensity / maxDens, TRUE, FALSE)\n\nPlot the result\n\nhist(sampled$proposal[sampled$accepted], freq = F, col = \"grey\", breaks = 100)\ncurve(dbeta(x, 3,6),0,1, add =T, col = \"red\")\n\n\n\n\n\n\nB.1.4 When would you use this sampler type\n\nIf you have many cores available and prefer an easy parallel implementation over computation efficiency\nWhen working with ABC, rejection has some interesting advantages additional to the parallelization option. See section ABC for explanation of the ABC-rejection algorithm\nIf you have an existing posterior sample, and you want to apply another function on"
  },
  {
    "objectID": "6B-BayesianNumerics.html#mcmc",
    "href": "6B-BayesianNumerics.html#mcmc",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.2 MCMC",
    "text": "B.2 MCMC\n\nB.2.1 Metropolis-Hastings\nSummary: the idea of this tutorial is to understand how MCMCs are used to sample the posterior distribution in a Bayesian analysis. We will\n\nSet up the posterior function for a linear regression\nUse a simple hand-programmed MCMC to sample from the posterior\nDiscuss the basics of tuning, convergence checks and interpretation of the posterior sample."
  },
  {
    "objectID": "6B-BayesianNumerics.html#creating-some-test-data",
    "href": "6B-BayesianNumerics.html#creating-some-test-data",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.3 Creating some test data",
    "text": "B.3 Creating some test data\nAs a first step, we create some test data that will be used to fit our model. Let’s assume a linear relationship between the predictor and the response variable, so we take a linear model and add some noise.\n\ntrueA &lt;- 5\ntrueB &lt;- 0\ntrueSd &lt;- 10\nsampleSize &lt;- 31\n \n# Create independent x-values\nx &lt;- (-(sampleSize-1)/2):((sampleSize-1)/2)\n# Create dependent values according to ax + b + N(0,sd)\ny &lt;-  trueA * x + trueB + rnorm(n=sampleSize,mean=0,sd=trueSd)\n\n# Plot data\nplot(x,y, main=\"Test Data\")"
  },
  {
    "objectID": "6B-BayesianNumerics.html#defining-the-statistical-model-and-the-likelihood-function",
    "href": "6B-BayesianNumerics.html#defining-the-statistical-model-and-the-likelihood-function",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.4 Defining the statistical model and the likelihood function",
    "text": "B.4 Defining the statistical model and the likelihood function\nThe next step is to specify the statistical model. We already know that the data was created with a linear relationship y = a*x + b between x and y and a normal error model N(0,sd) with standard deviation sd, so let’s use the same model for the fit and see if we can retrieve our original parameter values.\nFor estimating parameters in a Bayesian analysis, we need to derive the likelihood function for the model that we want to fit. The likelihood is the probability (density) with which we would expect the observed data to occur conditional on the parameters of the model that we look at. So, given that our linear model y = b + ax + N(0,sd) takes the parameters (a, b, sd) as an input, we have to return the probability of obtaining the test data above under this model\nThis sounds more complicated as it is, as you see in the code below, we simply calculate the difference between predictions y = b + ax and the observed y, and then we have to look up the probability densities (using dnorm) for such deviations to occur. As an illustration, the last lines of the code plot the Likelihood for a range of parameter values of the slope parameter a. The result should look something like the below plot.\n\n# Likelihood function\nlikelihood &lt;- function(param){\n    a = param[1]\n    b = param[2]\n    sd = param[3]\n     \n    pred = a*x + b\n    \n    singlelikelihoods = dnorm(y, mean = pred, sd = sd, log = T)\n    sumll = sum(singlelikelihoods)\n    return(sumll)  \n}\n\n\n \n# Example: plot the likelihood profile of the slope a\nslopevalues &lt;- function(x){return(likelihood(c(x, trueB, trueSd)))}\nslopelikelihoods &lt;- lapply(seq(3, 7, by=.05), slopevalues )\nplot (seq(3, 7, by=.05), slopelikelihoods , type=\"l\", xlab = \"values of slope parameter a\", ylab = \"Log likelihood\")"
  },
  {
    "objectID": "6B-BayesianNumerics.html#why-we-work-with-logarithms",
    "href": "6B-BayesianNumerics.html#why-we-work-with-logarithms",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.5 Why we work with logarithms?",
    "text": "B.5 Why we work with logarithms?\nYou might have noticed that I return the logarithm of the probabilities in the likelihood function, which is also the reason why I sum the probabilities of all our datapoints (the logarithm of a product equals the sum of the logarithms). Why do we do this? You don’t have to, but it’s strongly advisable because likelihoods, where a lot of small probabilities are multiplied, can get ridiculously small pretty fast (something like \\(10^-34\\)). At some stage, computer programs are getting into numerical rounding or underflow problems then. So, bottom-line: when you program something with likelihoods, always use logarithms!!!"
  },
  {
    "objectID": "6B-BayesianNumerics.html#defining-the-prior",
    "href": "6B-BayesianNumerics.html#defining-the-prior",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.6 Defining the prior",
    "text": "B.6 Defining the prior\nAs a second step, as always in Bayesian statistics, we have to specify a prior distribution for each parameter.\n\nI use wide normal distributions for slope and intercept, a standard choice. If I would have many predictors and therefore the danger of overfitting, one could think about making them more narrow, deliberately biasing them towards 0\nI am using a flat prior on 1/sd^2 (the latter expression is often called the precision), which is a standard non-informative choice for the variance. This then corresponds to a decay of the standard deviation with 1/sqrt(sd).\n\nOne would think that a flat prior on the variance would be a better idea, but one can show that this would typically lead to too much probability mass for large variances, effectively introducing a bias in the analysis. NOTE: good uniformative choices for parameters are not neccessarily flat!!!\n\n# Prior distribution\nprior &lt;- function(param){\n    if (param[3] &lt;= 0) return(-Inf)\n    \n    aprior = dnorm(param[1], sd = 50, log = T)\n    bprior = dnorm(param[2], sd = 50, log = T)\n    \n    sdprior = 1/sqrt(param[3]) + dunif(param[3], min=0, max=30, log = T)\n    return(aprior+bprior+sdprior)\n}"
  },
  {
    "objectID": "6B-BayesianNumerics.html#the-posterior",
    "href": "6B-BayesianNumerics.html#the-posterior",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.7 The posterior",
    "text": "B.7 The posterior\nThe product of prior and likelihood is the actual quantity the MCMC will be working on. This function is called the posterior (or to be exact, it’s called the posterior after it’s normalized, which the MCMC will do for us, but let’s not be picky for the moment). Again, here we work with the sum because we work with logarithms.\n\n# Posterior function\nposterior &lt;- function(param){\n   return (likelihood(param) + prior(param))\n}"
  },
  {
    "objectID": "6B-BayesianNumerics.html#the-mcmc",
    "href": "6B-BayesianNumerics.html#the-mcmc",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.8 The MCMC",
    "text": "B.8 The MCMC\nNow, here comes the actual Metropolis-Hastings algorithm. One of the most frequent applications of this algorithm (as in this example) is sampling from the posterior density in Bayesian statistics. In principle, however, the algorithm may be used to sample from any integrable function. So, the aim of this algorithm is to jump around in parameter space, but in a way that the probability to be at a point is proportional to the function we sample from (this is usually called the target function). In our case this is the posterior defined above.\nThis is achieved by:\n\nStarting at a random parameter value\nChoosing a new parameter value close to the old value based on some probability density that is called the proposal function\n\n*Jumping to this new point with a probability p(new)/p(old), where p is the target function, and p&gt;1 means jumping as well.\nIt requires a bit of mathematics to prove that this works, but for the moment I can assure you it does – when we run this algorithm, distribution of the parameters it visits converges to the target distribution p.\nSo, let’s get this in R:\n\nrun_metropolis_MCMC &lt;- function(startvalue, iterations){\n    chain = array(dim = c(iterations+1,3))\n    chain[1,] = startvalue\n    for (i in 1:iterations){\n        proposal = rnorm(3,mean = chain[i,], sd= c(0.1,0.5,0.3))\n          \n        probab = exp(posterior(proposal) - posterior(chain[i,]))\n        if (runif(1) &lt; probab){\n            chain[i+1,] = proposal\n        }else{\n            chain[i+1,] = chain[i,]\n        }\n    }\n    return(chain)\n}\n\nAgain, working with the logarithms of the posterior might be a bit confusing at first, in particular when you look at the line where the acceptance probability is calculated as:\n\\[(probab = exp(posterior(proposal) – posterior(chain[i,])))\\]\nTo understand why we do this: note that \\(p1/p2 = exp[log(p1)-log(p2)]\\)\nOK, let’s run the algorithm\n\nstartvalue = c(4,0,10)\nchain = run_metropolis_MCMC(startvalue, 10000)\n\nThe first steps of the algorithm may be biased by the initial value, and are therefore usually discarded for the further analysis (burn-in time). To discard the first 5000 steps, and transform the chain to an mcmc object, run this\n\nlibrary(coda)\nburnin = 5000\nresult &lt;- mcmc(chain[burnin:nrow(chain),], start = burnin)\n\nThe mcmc function is part of the coda R package that provides a number of standard functions for plotting and analysis of the posterior samples. For those functions to work, you need to have your output as an object of class “mcmc”, or “mcmc.list”, which we will discuss later. Coda is the standard package for this type of analysis, and most Bayesian packages in R use this class to return MCMC outputs, so you will likely come across this syntax whatever Bayesian code you are running.\nObjects of class “mcmc” hold and array with the mcmc samples, and a number of additional information. You can look at the structure with str(chain), and you can transform a “mcmc” object back to a normal data-frame by data.frame(chain).\nThe advantage of having a coda object is that a lot of things that we typically want to do with the chain are already implemented, so for example we can simply summary() and plot() the outputs which gives some useful information on the console and a plot that should look roughly like this:\n\nplot(result)\n\n\n\nsummary(result) \n\n\nIterations = 5000:10001\nThinning interval = 1 \nNumber of chains = 1 \nSample size per chain = 5002 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n       Mean     SD Naive SE Time-series SE\n[1,]  5.180 0.1839  0.00260        0.01171\n[2,] -2.294 1.5781  0.02231        0.16545\n[3,]  8.981 1.1590  0.01639        0.14718\n\n2. Quantiles for each variable:\n\n       2.5%    25%    50%    75%   97.5%\nvar1  4.813  5.056  5.181  5.307  5.5304\nvar2 -5.649 -3.295 -2.182 -1.219  0.6498\nvar3  7.023  8.127  8.913  9.707 11.4433\n\nrejectionRate(result)\n\n     var1      var2      var3 \n0.2335533 0.2335533 0.2335533 \n\n\nAn interesting statistics provided by summary is the acceptance rate: how often was a proposal rejected by the metropolis-hastings acceptance criterion? The acceptance rate can be influenced by the proposal function: generally, the closer the proposals are, the larger the acceptance rate. Very high acceptance rates, however, are usually not beneficial: this means that the algorithms is “staying” at the same point, which results in a suboptimal probing of the parameter space (mixing). It can be shown that acceptance rates between 20% and 30% are optimal for typical applications (more on that later).\nIn the plot() function, each row corresponds to one parameter, so there a are two plots for each parameter. The left plot is called a trace plot – it shows the values the parameter took during the runtime of the chain. The right plot is usually called a marginal density plot. Basically, it is the (smoothened) histogram of the values in the trace-plot, i.e. the distribution of the values of the parameter in the chain.\nComparison with the normal lm:\n\nsummary(lm(y~x))\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-19.8977  -4.0954   0.5968   4.3722  19.5387 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -2.3579     1.5510   -1.52    0.139    \nx             5.1888     0.1734   29.92   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 8.635 on 29 degrees of freedom\nMultiple R-squared:  0.9686,    Adjusted R-squared:  0.9675 \nF-statistic: 895.4 on 1 and 29 DF,  p-value: &lt; 2.2e-16\n\n\nYou see that we retrieve more or less the original parameters that were used to create our data, and you also see that we get a certain area around the highest posterior values that also have some support by the data, which is the Bayesian equivalent of confidence intervals."
  },
  {
    "objectID": "6B-BayesianNumerics.html#improving-convergence-mixing",
    "href": "6B-BayesianNumerics.html#improving-convergence-mixing",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.9 Improving convergence / mixing",
    "text": "B.9 Improving convergence / mixing\nSo, what to do if there is no convergence yet? Of course, you can always run the MCMC longer, but the other option is to make it converge faster … the word that is used here is “mixing”, which basically means how well the algorithm jumps around in the parameter space … the mixing is affected by the choice of your proposal function. Two things can happen:\n\nYour proposal function is narrow compared to the distribution we sample from – high acceptance rate, but we don’t get anywhere, bad mixing\nYour proposal function is too wide compared to the distribution we sample from – low acceptance rate, most of the time we stay where we are\n\nLet’s create both situations so that you get the picture. I’m turning back to the old data. Proposalfunction too narrow:\n\nx &lt;- (-(sampleSize-1)/2):((sampleSize-1)/2) + 20\ny &lt;-  trueA * x + trueB + rnorm(n=sampleSize,mean=0,sd=trueSd)\n\nproposalfunction &lt;- function(param){\n  return(rnorm(3,mean = param, sd= c(0.001,0.5,0.3)))\n}\n\nstartvalue = c(4,0,10)\n\nchain1 = mcmc(run_metropolis_MCMC(startvalue, 10000)[5000:ncol(chain),], start = 5000)\nchain2 = mcmc(run_metropolis_MCMC(startvalue, 10000)[5000:ncol(chain),], start = 5000)\ncombinedchains = mcmc.list(chain1, chain2) \nplot(combinedchains)\n\n\n\n#gelman.plot(combinedchains)\n\nProposalfunction too wide\n\nproposalfunction &lt;- function(param){\n  return(rnorm(3,mean = param, sd= c(1000,0.5,0.3)))\n}\nchain1 = mcmc(run_metropolis_MCMC(startvalue, 10000)[5000:ncol(chain),], start = 5000)\nchain2 = mcmc(run_metropolis_MCMC(startvalue, 10000)[5000:ncol(chain),], start = 5000)\ncombinedchains = mcmc.list(chain1, chain2) \nplot(combinedchains)\n\n\n\n#gelman.plot(combinedchains)\n\nAs displayed in the figure, these problems can be seen in the trace plots. Theoretical considerations show that an acceptance rate of 20-30% is optimal for typical target distributions, but this is in practice not so helpful because you can still have very bad mixing although being at this level by having the proposal of one parameter too narrow and the proposal of another parameter too wide. Better to look at the trace-plots of the individual parameters. Again, correlations are a problem, so if you have strong correlations in parameter space, you can get bad mixing. Using multivariate proposals that are adjusted to the correlation structure can help, but in general it is better to avoid correlations if possible. The good news at last: most of the more “professional” MCMC sampling software such as Jags or WinBugs will do these things automatically for you.\n\n\n\n\n\n\nThings to try out\n\n\n\n\nChange the prior definition\nHow could you change the likelihood to account for outliers?\nChange the proposal function\nChange from log likelihoods to normal probabilities –&gt; You should see that you run into numerical problems if you increase the sample size\nDecorrelate slope and intercept –&gt; you should see that convergence goes down"
  },
  {
    "objectID": "6B-BayesianNumerics.html#references-for-further-reading",
    "href": "6B-BayesianNumerics.html#references-for-further-reading",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.10 References for further reading",
    "text": "B.10 References for further reading\n\nGelman, A.; Carlin, J. B.; Stern, H. S. & Rubin, D. B. (2003) Bayesian Data Analysis\nAndrieu, C.; de Freitas, N.; Doucet, A. & Jordan, M. I. (2003) An introduction to MCMC for machine learning Mach. Learning, Springer, 50, 5-43\nHartig, F.; Calabrese, J. M.; Reineking, B.; Wiegand, T. & Huth, A. (2011) Statistical inference for stochastic simulation models – theory and application Ecol. Lett., 14, 816–827."
  },
  {
    "objectID": "6B-BayesianNumerics.html#adaptive-metropolis",
    "href": "6B-BayesianNumerics.html#adaptive-metropolis",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.11 Adaptive metropolis",
    "text": "B.11 Adaptive metropolis\nIn this previous version, the proposal function was simlply creating independent normal draws \\(proposalfunction &lt;- function(param){return(rnorm(3,mean = param, sd= c(0.1,0.5,0.3))) }\\) .We change now to a multivariate normal version that will be adapted later.\n\nlibrary(MASS)\nsig = diag(x = c(0.1,0.5,0.3), nrow=3, ncol=3)\n\nproposalfunction &lt;- function(param){\n  return(mvrnorm(1,mu = param, Sigma= sig))\n}\n\nHere is the mcmc. I adapted the code slightly to allow for running the MCMC for a while, stopping, and continuing with the MCMC, which will be useful for the adaptation.\n\nrun_metropolis_MCMC &lt;- function(iterations){\n  startindex = nrow(chain)\n  chain = rbind(chain, array(dim = c(iterations,3)))\n  for (i in startindex:(startindex+iterations-1)){\n    proposal = proposalfunction(chain[i,])\n    \n    probab = exp(likelihood(proposal)+ prior(proposal) - likelihood(chain[i,])- prior(chain[i,]))\n    if (runif(1) &lt; probab){\n      chain[i+1,] = proposal\n    }else{\n      chain[i+1,] = chain[i,]\n    }\n  }\n  return(chain)\n}\n\nBut first of all, we will just run the MCMC like before and check the convergence.\n\nchain = array( c(4,2,8), dim = c(1,3))\n\n# Running a non-adapted analysis with deliberately bad proposal function \n\n# Settings for the proposal covariance matrix\nsig = diag(x = c(1,1,1), nrow=3, ncol=3)\n\nchain1 = run_metropolis_MCMC(10000)\nchain2 = run_metropolis_MCMC(10000)\ncombinedchains = mcmc.list(mcmc(chain1), mcmc(chain2))\nplot(combinedchains)\n\n\n\ngelman.diag(combinedchains)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\n[1,]       1.05       1.11\n[2,]       1.07       1.15\n[3,]       1.00       1.01\n\nMultivariate psrf\n\n1.02\n\ngelman.plot(combinedchains)\n\n\n\n\nWe use the samples obtained already to adjust the proposal for explanations why the scaling factor of \\(2.38^2/d\\) is optional (see references)\n\nchain = array( c(4,2,8), dim = c(1,3))\nchain = run_metropolis_MCMC(2000)\nsig = 2.38^2 / 3 * cov(chain) \nchain = run_metropolis_MCMC(2000)\nsig = 2.38^2 / 3 * cov(chain) \nchain = run_metropolis_MCMC(2000)\nsig = 2.38^2 / 3 * cov(chain) \nchain = run_metropolis_MCMC(2000)\nsig = 2.38^2 / 3 * cov(chain) \nchain = run_metropolis_MCMC(2000)\nsig = 2.38^2 / 3 * cov(chain) \n\n \nchain = array( c(4,2,8), dim = c(1,3))\nchain1 = run_metropolis_MCMC(10000)\nchain2 = run_metropolis_MCMC(10000)\ncombinedchains = mcmc.list(mcmc(chain1), mcmc(chain2))\nplot(combinedchains)\n\n\n\ngelman.diag(combinedchains)\n\nPotential scale reduction factors:\n\n     Point est. Upper C.I.\n[1,]          1          1\n[2,]          1          1\n[3,]          1          1\n\nMultivariate psrf\n\n1\n\ngelman.plot(combinedchains)\n\n\n\n\nRosenthal, J. S. (2011) Optimal proposal distributions and adaptive MCMC. Handbook of Markov Chain Monte Carlo, CRC Press."
  },
  {
    "objectID": "6B-BayesianNumerics.html#smc",
    "href": "6B-BayesianNumerics.html#smc",
    "title": "Appendix B — Bayesian Numerics",
    "section": "B.12 SMC",
    "text": "B.12 SMC\n\nB.12.1 Overview\nSMC sampling is an extension of rejection sampling (see script about rejection sampling) that deals with a particular problem of rejection samplign, which is that a simple rejection sampler discards a large number of model evaluations when the area of non-vanising posterior density is very small compared to the prior volume\n\n\nB.12.2 How it works\nSimilar to rejection sampling, we start with an initial sample of parameters, calculate the likelihood, and then remove parameters according to their likelihood / posterior.\n\n\nB.12.3 An example in R\nAssume we want to sample from a narrow, 2-dim normal distribution with mean 0.5,0.5\n\nlikelihood &lt;- function(x) sum(dnorm(x,0.5, 0.01, log = T))\n\nAssume our prior is unifor from 0 to 1.\n\nprior = function(x)sum(dunif(x,0,1))\n\nStart with a random sample from the prior\n\ninitialParticles &lt;- matrix(runif(1000,0,1), ncol = 2)\n\nNow comes the SMC. The idea is to\n\nParticles drawn from the prior, if we do nothing or likelihood flat it just stays prior (could generalize this to other starting distributions)\nFor N steps do\nCalculate Likelihood L for the particles\nSelect particles for next step with probability L^(1/N) (to compensate for the several steps)\nAdd a Metropolis-Hastings resampling step to avoid particle depletion / starvation (ending up with very few identical particles)\n\n\nsmc_sampler &lt;- function(likelihood, prior, initialParticles, iterations =1, resampling = T){\n \n  particles &lt;- initialParticles\n \n  numPar &lt;- ncol(initialParticles)\n \n  for (i in 1:iterations){\n    \n    likelihoodValues &lt;- apply(particles, 1, likelihood)\n    \n    relativeL = exp(likelihoodValues - max(likelihoodValues, na.rm = T))^(1/iterations) # dividing max to avoid numerical problems, no effect on the relative probabilities\n    \n    sel = sample.int(n=length(likelihoodValues), size = length(likelihoodValues), replace = T, prob = relativeL)\n    \n    particles = particles[sel,]\n   \n    if (resampling == T){\n       \n      proposal &lt;- function(x) rnorm(length(x), mean = x, sd = 0.005)\n   \n      particlesProposals = t(apply(particles, 1, proposal))\n   \n      particlesProposalsLikelihood &lt;- apply(particlesProposals, 1, likelihood)\n \n      jumpProb &lt;- exp(particlesProposalsLikelihood - likelihoodValues[sel])^(i/iterations) * exp(prior(particlesProposals) - prior(particles))\n      accepted &lt;- jumpProb &gt; runif(length(particlesProposalsLikelihood), 0 ,1)\n     \n      particles[accepted, ] = particlesProposals[accepted, ]\n    }\n   \n  }\n  return(particles )\n}\n\nPlot the result\n\npar(mfrow = c(2,2))\n\nfinalParticles &lt;- smc_sampler(likelihood, prior, initialParticles, iterations = 1, resampling = F)\nhist(finalParticles[,1], freq = F, col = \"grey\", breaks = 50, xlim = c(0,1), main = \"Rejection Sampler\")\ncurve(dnorm(x,0.5, 0.01),0,1, n = 5000, add =T, col = \"red\")\nhist(finalParticles[,2], freq = F, col = \"grey\", breaks = 50, xlim = c(0,1), main = \"Rejection Sampler\")\ncurve(dnorm(x,0.5, 0.01),0,1, n = 5000, add =T, col = \"red\")\n\n\n\nfinalParticles &lt;- smc_sampler(likelihood, prior, initialParticles, iterations = 50)\nhist(finalParticles[,1], freq = F, col = \"grey\", breaks = 50, xlim = c(0,1), main = \"SMC Sampler\")\ncurve(dnorm(x,0.5, 0.01),0,1, n = 5000, add =T, col = \"red\")\nhist(finalParticles[,2], freq = F, col = \"grey\", breaks = 50, xlim = c(0,1), main = \"SMC Sampler\")\ncurve(dnorm(x,0.5, 0.01),0,1, n = 5000, add =T, col = \"red\")\n\n\n\n\nMore advanced SMC methods\n\nBayesianTools\nSpeich, M., Dormann, C. F., & Hartig, F. (2021). Sequential Monte-Carlo algorithms for Bayesian model calibration–A review and method comparison✰. Ecological Modelling, 455, 109608.\nhttps://github.com/biips/rbiips"
  },
  {
    "objectID": "6C-CaseStudies.html#owls-poisson-glm",
    "href": "6C-CaseStudies.html#owls-poisson-glm",
    "title": "Appendix C — Case Studies",
    "section": "C.1 Owls (Poisson GLM)",
    "text": "C.1 Owls (Poisson GLM)\nFor this case study, we will use the fairly well known Owl dataset which is provided in glmmTMB (see ?Owls for more info about the data). A frequentist base model would be:\n\nlibrary(glmmTMB)\nlibrary(effects)\n\nLoading required package: carData\n\n\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nm1 &lt;- glm(SiblingNegotiation ~ SexParent, data=Owls , family = poisson)\nsummary(m1)\n\n\nCall:\nglm(formula = SiblingNegotiation ~ SexParent, family = poisson, \n    data = Owls)\n\nDeviance Residuals: \n    Min       1Q   Median       3Q      Max  \n-3.7971  -3.4676  -0.4639   1.6272   6.7678  \n\nCoefficients:\n              Estimate Std. Error z value Pr(&gt;|z|)    \n(Intercept)    1.79380    0.02605  68.847  &lt; 2e-16 ***\nSexParentMale  0.18154    0.03272   5.548 2.89e-08 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n(Dispersion parameter for poisson family taken to be 1)\n\n    Null deviance: 4290.9  on 598  degrees of freedom\nResidual deviance: 4259.7  on 597  degrees of freedom\nAIC: 5944.5\n\nNumber of Fisher Scoring iterations: 5\n\nplot(allEffects(m1))\n\n\n\n\nExercise 1: fit this GLM using a Bayesian approach, e.g. Jags, STAN or brms\n\n\n\n\n\n\nTip\n\n\n\nFor STAN and JAGS, you will have to transform categorical variables to dummy coding, i.e.\n\nsex = as.numeric(Owls$SexParent) - 1 \n\nThen you can code\n\nsexEffect * sex[i]\n\n\n\nExercise 2: include a log offset to the model too account for BroodSize\n\nm2 &lt;- glm(SiblingNegotiation ~ FoodTreatment*SexParent + offset(log(BroodSize)), data=Owls , family = poisson)\n\nExercise 3: check residuals and / or add obvious additional components to the model inspired by the frequentist example here.\n\n\n\n\n\n\nSolution using jags\n\n\n\n\n\nExercise 1:\n\nlibrary(glmmTMB)\nlibrary(rjags)\n\nData = list(SiblingNegotiation = Owls$SiblingNegotiation, \n            SexParent = as.numeric(Owls$SexParent)-1, # dummy coding!\n            FoodTreatment = as.numeric(Owls$FoodTreatment)-1,\n            LogBroodSize = log(Owls$BroodSize),\n            nobs = nrow(Owls))\n\n\nmodelCode = \"model{\n\n  for(i in 1:nobs){\n    SiblingNegotiation[i] ~ dpois(lambda[i])  # poisson error distribution\n    lambda[i] &lt;- exp(eta[i]) # inverse link function\n    eta[i] &lt;- intercept + EffectSexParent*SexParent[i] + EffektFoodTreatment*FoodTreatment[i] + EffektInterSexFood*SexParent[i]*FoodTreatment[i] + LogBroodSize[i]       # linear predictor\n  }\n  \n  intercept ~ dnorm(0,0.0001)\n  EffectSexParent ~ dnorm(0,0.0001)\n  EffektFoodTreatment ~ dnorm(0,0.0001)\n  EffektInterSexFood ~ dnorm(0,0.0001)\n\n}\"\n\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 599\n   Unobserved stochastic nodes: 4\n   Total graph size: 2465\n\nInitializing model\n\npara.names &lt;- c(\"intercept\",\"EffectSexParent\", \"EffektFoodTreatment\", \"EffektInterSexFood\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n\nplot(Samples)\n\n\n\nsummary(Samples)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                        Mean      SD  Naive SE Time-series SE\nEffectSexParent      0.06477 0.04044 0.0003302       0.001258\nEffektFoodTreatment -0.54843 0.05264 0.0004298       0.001621\nEffektInterSexFood   0.07474 0.06677 0.0005452       0.002044\nintercept            0.59271 0.03290 0.0002686       0.001022\n\n2. Quantiles for each variable:\n\n                        2.5%      25%      50%      75%   97.5%\nEffectSexParent     -0.01426  0.03821  0.06487  0.09166  0.1445\nEffektFoodTreatment -0.65267 -0.58382 -0.54851 -0.51282 -0.4453\nEffektInterSexFood  -0.05817  0.03028  0.07491  0.11859  0.2081\nintercept            0.52785  0.57096  0.59306  0.61455  0.6563\n\n\nIncluding the offset\n\nlibrary(rjags)\n\nData = list(SiblingNegotiation = Owls$SiblingNegotiation, \n            SexParent = as.numeric(Owls$SexParent)-1, # dummy coding!\n            FoodTreatment = as.numeric(Owls$FoodTreatment)-1,\n            LogBroodSize = log(Owls$BroodSize),\n            nobs = nrow(Owls))\n\n\nmodelCode = \"model{\n\nfor(i in 1:nobs){\nSiblingNegotiation[i] ~ dpois(lambda[i])  # poisson error distribution\nlambda[i] &lt;- exp(eta[i]) # inverse link function\neta[i] &lt;- intercept + EffectSexParent*SexParent[i] + EffektFoodTreatment*FoodTreatment[i] + EffektInterSexFood*SexParent[i]*FoodTreatment[i] + LogBroodSize[i]       # linear predictor\n}\n\nintercept ~ dnorm(0,0.0001)\nEffectSexParent ~ dnorm(0,0.0001)\nEffektFoodTreatment ~ dnorm(0,0.0001)\nEffektInterSexFood ~ dnorm(0,0.0001)\n\n}\"\n\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 599\n   Unobserved stochastic nodes: 4\n   Total graph size: 2465\n\nInitializing model\n\npara.names &lt;- c(\"intercept\",\"EffectSexParent\", \"EffektFoodTreatment\", \"EffektInterSexFood\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n\nplot(Samples)\n\n\n\nsummary(Samples)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n                        Mean      SD  Naive SE Time-series SE\nEffectSexParent      0.06586 0.04134 0.0003376       0.001288\nEffektFoodTreatment -0.54818 0.05393 0.0004403       0.001584\nEffektInterSexFood   0.07293 0.06833 0.0005579       0.002015\nintercept            0.59241 0.03348 0.0002734       0.001055\n\n2. Quantiles for each variable:\n\n                        2.5%      25%      50%      75%   97.5%\nEffectSexParent     -0.01427  0.03737  0.06524  0.09458  0.1469\nEffektFoodTreatment -0.65330 -0.58465 -0.54833 -0.51157 -0.4424\nEffektInterSexFood  -0.06025  0.02658  0.07308  0.11989  0.2055\nintercept            0.52634  0.56933  0.59317  0.61581  0.6562\n\n\nChecking residuals\n\nlibrary(rjags)\n\nData = list(SiblingNegotiation = Owls$SiblingNegotiation, \n            SexParent = as.numeric(Owls$SexParent)-1, # dummy coding!\n            FoodTreatment = as.numeric(Owls$FoodTreatment)-1,\n            LogBroodSize = log(Owls$BroodSize),\n            nobs = nrow(Owls))\n\n\nmodelCode = \"model{\n\n  for(i in 1:nobs){\n    SiblingNegotiation[i] ~ dpois(lambda[i])  # poisson error distribution\n    lambda[i] &lt;- exp(eta[i]) # inverse link function\n    eta[i] &lt;- intercept + EffectSexParent*SexParent[i] + EffektFoodTreatment*FoodTreatment[i] + EffektInterSexFood*SexParent[i]*FoodTreatment[i] + LogBroodSize[i]       # linear predictor\n  }\n  \n  intercept ~ dnorm(0,0.0001)\n  EffectSexParent ~ dnorm(0,0.0001)\n  EffektFoodTreatment ~ dnorm(0,0.0001)\n  EffektInterSexFood ~ dnorm(0,0.0001)\n\n  # Posterior predictive simulations \n  for (i in 1:nobs) {\n    SiblingNegotiationPred[i]~dpois(lambda[i])\n  }\n\n}\"\n\njagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, n.chains = 3)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 599\n   Unobserved stochastic nodes: 603\n   Total graph size: 3064\n\nInitializing model\n\npara.names &lt;- c(\"intercept\",\"EffectSexParent\", \"EffektFoodTreatment\", \"EffektInterSexFood\",\"lambda\", \"SiblingNegotiationPred\")\nSamples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n\nlibrary(BayesianTools)\nlibrary(DHARMa)\n\nThis is DHARMa 0.4.6. For overview type '?DHARMa'. For recent changes, type news(package = 'DHARMa')\n\nx = getSample(Samples)\n# note - yesterday, we calcualted the predictions from the parameters\n# here we observe them direct - this is the normal way to calcualte the \n# posterior predictive distribution\nposteriorPredDistr = x[,5:(4+599)]\nposteriorPredSim = x[,(5+599):(4+2*599)]\n\n\nsim = createDHARMa(simulatedResponse = t(posteriorPredSim), observedResponse = Owls$SiblingNegotiation, fittedPredictedResponse = apply(posteriorPredDistr, 2, median), integerResponse = T)\nplot(sim)\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSolution using brms\n\n\n\n\n\nHere a base model with random effect\n\nlibrary(brms)\nm2 = brms::brm(SiblingNegotiation ~ FoodTreatment * SexParent\n  + (1|Nest) + offset(log(BroodSize)), \n  data = Owls , \n  family = negbinomial)\n\nRunning /Library/Frameworks/R.framework/Resources/bin/R CMD SHLIB foo.c\nclang -mmacosx-version-min=10.13 -I\"/Library/Frameworks/R.framework/Resources/include\" -DNDEBUG   -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/Rcpp/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/unsupported\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/BH/include\" -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/src/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppParallel/include/\"  -I\"/Library/Frameworks/R.framework/Versions/4.2/Resources/library/rstan/include\" -DEIGEN_NO_DEBUG  -DBOOST_DISABLE_ASSERTS  -DBOOST_PENDING_INTEGER_LOG2_HPP  -DSTAN_THREADS  -DUSE_STANC3 -DSTRICT_R_HEADERS  -DBOOST_PHOENIX_NO_VARIADIC_EXPRESSION  -D_HAS_AUTO_PTR_ETC=0  -include '/Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp'  -D_REENTRANT -DRCPP_PARALLEL_USE_TBB=1   -I/usr/local/include   -fPIC  -Wall -g -O2  -c foo.c -o foo.o\nIn file included from &lt;built-in&gt;:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/StanHeaders/include/stan/math/prim/fun/Eigen.hpp:22:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Dense:1:\nIn file included from /Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/Core:19:\n/Library/Frameworks/R.framework/Versions/4.2/Resources/library/RcppEigen/include/Eigen/src/Core/util/Macros.h:679:10: fatal error: 'cmath' file not found\n#include &lt;cmath&gt;\n         ^~~~~~~\n1 error generated.\nmake: *** [foo.o] Error 1\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000232 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.32 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 2.019 seconds (Warm-up)\nChain 1:                1.583 seconds (Sampling)\nChain 1:                3.602 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.00012 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 1.2 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 1.917 seconds (Warm-up)\nChain 2:                1.496 seconds (Sampling)\nChain 2:                3.413 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000128 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.28 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 1.962 seconds (Warm-up)\nChain 3:                1.63 seconds (Sampling)\nChain 3:                3.592 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.00013 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.3 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 1.981 seconds (Warm-up)\nChain 4:                1.58 seconds (Sampling)\nChain 4:                3.561 seconds (Total)\nChain 4: \n\nsummary(m2)\n\n Family: negbinomial \n  Links: mu = log; shape = identity \nFormula: SiblingNegotiation ~ FoodTreatment * SexParent + (1 | Nest) + offset(log(BroodSize)) \n   Data: Owls (Number of observations: 599) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nGroup-Level Effects: \n~Nest (Number of levels: 27) \n              Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsd(Intercept)     0.39      0.10     0.22     0.61 1.00     1034     1605\n\nPopulation-Level Effects: \n                                    Estimate Est.Error l-95% CI u-95% CI Rhat\nIntercept                               0.72      0.14     0.44     1.02 1.00\nFoodTreatmentSatiated                  -0.78      0.17    -1.11    -0.43 1.00\nSexParentMale                          -0.03      0.15    -0.33     0.25 1.00\nFoodTreatmentSatiated:SexParentMale     0.16      0.21    -0.25     0.57 1.00\n                                    Bulk_ESS Tail_ESS\nIntercept                               2621     2779\nFoodTreatmentSatiated                   3128     3210\nSexParentMale                           3373     2454\nFoodTreatmentSatiated:SexParentMale     3079     3016\n\nFamily Specific Parameters: \n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nshape     0.84      0.07     0.71     0.97 1.00     5786     2933\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n\nplot(m2, ask = FALSE)"
  },
  {
    "objectID": "6C-CaseStudies.html#beetles",
    "href": "6C-CaseStudies.html#beetles",
    "title": "Appendix C — Case Studies",
    "section": "C.2 Beetles",
    "text": "C.2 Beetles\n\n# This first part creates a dataset with beetles counts across an altitudinal gradient (several plots each observed several years), with a random intercept on year and zero-inflation. \n\naltitude = rep(seq(0,1,len = 50), each = 20)\ndataID = 1:1000\nspatialCoordinate = rep(seq(0,30, len = 50), each = 20)\n\n# random effects + zeroinflation\nplot = rep(1:50, each = 20)\nyear = rep(1:20, times = 50)\n\nyearRandom = rnorm(20, 0, 1)\nplotRandom = rnorm(50, 0, 1)\noverdispersion = rnorm(1000, sd = 0.5)\nzeroinflation = rbinom(1000,1,0.6)\n\nbeetles &lt;- rpois(1000, exp( 0  + 12*altitude - 12*altitude^2 \n                            #  + overdispersion   + plotRandom[plot]\n                            + yearRandom[year]) * zeroinflation )\n\ndata = data.frame(dataID, beetles, altitude, plot, year, spatialCoordinate)\n\nplot(year, altitude, cex = beetles/50, pch =2, main = \"Beetle counts across altitudinal gradient\\n triangle is proportional to counts\")\n\n\n\n\n\nlibrary(R2jags)\n\n\nAttaching package: 'R2jags'\n\n\nThe following object is masked from 'package:coda':\n\n    traceplot\n\nmodelData=as.list(data)\nmodelData = append(data, list(nobs=1000, nplots = 50, nyears = 20))\nhead(data)\n\n  dataID beetles altitude plot year spatialCoordinate\n1      1       2        0    1    1                 0\n2      2       0        0    1    2                 0\n3      3       0        0    1    3                 0\n4      4       2        0    1    4                 0\n5      5       0        0    1    5                 0\n6      6       0        0    1    6                 0\n\n# 1) Fit GLM only \n\nmodelstring=\"\nmodel {\n  \n  # Likelihood\n  for (i in 1:nobs) {\n    lambda[i] &lt;- exp(intercept + alt * altitude[i] + alt2 * altitude[i] * altitude[i]) \n    beetles[i]~dpois(lambda[i]) \n  }\n  \n  # Fixed effect priors \n  intercept ~ dnorm(0,0.0001)\n  alt ~ dnorm(0,0.0001)\n  alt2 ~ dnorm(0,0.0001)\n\n  # Posterior predictive simulations \n  \n  for (i in 1:nobs) {\n    beetlesPred[i]~dpois(lambda[i])\n  }\n  Prediction &lt;- sum(beetlesPred)\n}\n\"\n\nmodel=jags(model.file = textConnection(modelstring), data=modelData, n.iter=10000,  parameters.to.save = c(\"intercept\", \"alt\", \"alt2\", \"beetlesPred\", \"lambda\"), DIC = F)\n\nmodule glm loaded\n\n\nmodule dic loaded\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"dataID\" in data\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"plot\" in data\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"year\" in data\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"spatialCoordinate\" in data\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"nplots\" in data\n\n\nWarning in jags.model(model.file, data = data, inits = init.values, n.chains =\nn.chains, : Unused variable \"nyears\" in data\n\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 1000\n   Unobserved stochastic nodes: 1003\n   Total graph size: 3208\n\nInitializing model\n\nlibrary(DHARMa)\nsimulations = model$BUGSoutput$sims.list$beetlesPred\npred = apply(model$BUGSoutput$sims.list$lambda, 2, median)\ndim(simulations)\n\n[1] 3000 1000\n\nsim = createDHARMa(simulatedResponse = t(simulations), observedResponse = data$beetles, fittedPredictedResponse = pred, integerResponse = T)\nplotSimulatedResiduals(sim)\n\nplotSimulatedResiduals is deprecated, please switch your code to simply using the plot() function\n\n\nDHARMa:testOutliers with type = binomial may have inflated Type I error rates for integer-valued distributions. To get a more exact result, it is recommended to re-run testOutliers with type = 'bootstrap'. See ?testOutliers for details\n\n\nqu = 0.25, log(sigma) = -4.086977 : outer Newton did not converge fully."
  },
  {
    "objectID": "6C-CaseStudies.html#cndd-estimated-comita-et-al.-2010",
    "href": "6C-CaseStudies.html#cndd-estimated-comita-et-al.-2010",
    "title": "Appendix C — Case Studies",
    "section": "C.3 CNDD estimated, Comita et al., 2010",
    "text": "C.3 CNDD estimated, Comita et al., 2010\nThis is the model from Comita, L. S., Muller-Landau, H. C., Aguilar, S., & Hubbell, S. P. (2010). Asymmetric density dependence shapes species abundances in a tropical tree community. Science, 329(5989), 330-332.\nThe model was originally written in WinBugs. The version here was here slightly modified to be run with JAGS. Rerunning these models were part of the tests we did to settle on the methodology in Hülsmann, L., Chisholm, R. A., Comita, L., Visser, M. D., de Souza Leite, M., Aguilar, S., … & Hartig, F. (2024). Latitudinal patterns in stabilizing density dependence of forest communities. Nature, 1-8.\nOur tests indicated that this model is excellent in recovering CNDD estimates from simulations. The main reason we didn’t use a similar model in Hülsmann et al., 2024 were computational limitations and the difficulty to include splines on the species-specific density responses in such a hierarchical setting.\nTask: go through the paper and the code and try to understand what the structure of the model!\n\nmodel{\n  for (i in 1:N) {\n    SD[i] ~ dbern(p[i])\n    SD_sim[i] ~ dbern(p[i])\n    logit(p[i]) &lt;- B[SPP[i], ] %*% PREDS[i, ] + u[PLOT[i]]\n  }\n  \n  # Standard Random intercept on plot\n  for (m in 1:Nplots) {\n    u[m] ~ dnorm(0, a.tau)\n  }\n  a.sigma ~ dunif(0, 100)\n  a.tau &lt;- 1 / (a.sigma * a.sigma)\n  \n  #redundant parameterization speeds convergence in WinBugs, see Gelman & Hill (2007)\n  for (k in 1:K) {\n    for (j in 1:Nspp) {\n      B[j, k] &lt;- xi[k] * B.raw[j, k]\n    }\n    xi[k] ~ dunif(0, 100)\n  }\n  \n  #multivariate normal distribution for B values of each species\n  for (j in 1:Nspp) {\n    B.raw[j, 1:K] ~ dmnorm(B.raw.hat[j, ], Tau.B.raw[, ])\n    \n    #G.raw is matrix of regression coefficients for species-level model\n    #ABUND is  species-level predictors (abundance and shade tolerance)\n    for (k in 1:K) {\n      B.raw.hat[j, k] &lt;-\n        G.raw[k, ] %*% ABUND[j, ] # ABUND needs to be matrix w/ 1st column all 1's\n    }\n  }\n  \n  #priors for G and redundant parameterization\n  for (k in 1:K) {\n    for (l in 1:3) {\n      G[k, l] &lt;- xi[k] * G.raw[k, l]\n      G.raw[k, l] ~ dnorm(0, 0.1)\n    }\n  }\n  \n  #covariance matrix modeled using scaled inverse wishart model\n  Tau.B.raw[1:K, 1:K] ~ dwish(W[, ], df)\n  df &lt;- K + 1\n  Sigma.B.raw[1:K, 1:K] &lt;- inverse(Tau.B.raw[, ])\n  \n  # correlations\n  for (k in 1:K) {\n    for (k.prime in 1:K) {\n      rho.B[k, k.prime] &lt;-\n        Sigma.B.raw[k, k.prime]  /   sqrt(Sigma.B.raw[k, k] * Sigma.B.raw[k.prime, k.prime])\n    }\n    #\n    sigma.B[k] &lt;- abs(xi[k]) * sqrt(Sigma.B.raw[k, k])\n  }\n  \n  ################ Predictions ############\n  # Addition to original model (Nov 2019)\n  # to estimate the effect of mortality (response) when changing 1 unit on x-axis\n  # data (old, read with the name 'txt') is centered but not scaled, therefore 'zero' is here the value of '-6.81'\n  \n  for (i in 1:N) {\n    baseMort[i] = ilogit(B[SPP[i], 1] * PREDS[i, 1] + B[SPP[i], 2] * (-6.81) + B[SPP[i], 3:5] %*% PREDS[i, 3:5])\n    conMort[i] = ilogit(B[SPP[i], 1] * PREDS[i, 1] + B[SPP[i], 2] * (-5.81) + B[SPP[i], 3:5] %*% PREDS[i, 3:5])\n    # relConEffekt[i] &lt;- (conMort[i] - baseMort[i]) / baseMort[i]\n    # hetEffekt[i] &lt;- ilogit(B[SPP[i],1] * PREDS[i,1] + B[SPP[i],2] * PREDS[i,3]^CC[SPP[i]] + B[SPP[i],3] * 1 + B[SPP[i],4:5] %*% PREDS[i,4:5]) - ilogit(B[SPP[i],1] * PREDS[i,1] + B[SPP[i],2] * PREDS[i,3]^CC[SPP[i]] + B[SPP[i],3] * 0 + B[SPP[i],4:5] %*% PREDS[i,4:5])\n    \n  }\n  \n}"
  },
  {
    "objectID": "6C-CaseStudies.html#support-for-mixed-model",
    "href": "6C-CaseStudies.html#support-for-mixed-model",
    "title": "Appendix C — Case Studies",
    "section": "C.4 Support for mixed model",
    "text": "C.4 Support for mixed model\n\n## ---- echo=F, warning=F, message=F---------------------------------------\nset.seed(123)\nrm(list=ls(all=TRUE))\nlibrary(rjags)\nlibrary(runjags)\nlibrary(lme4)\n\nLoading required package: Matrix\n\n\n\nAttaching package: 'lme4'\n\n\nThe following object is masked from 'package:brms':\n\n    ngrps\n\nlibrary(effects)\nlibrary(R2jags)\n\n## ---- fig.width=5, fig.height=5------------------------------------------\na &lt;- 5\nb &lt;- 10\nsigma &lt;- 10\nrsigma = 30\ngroup = rep(1:11, each = 5)\nrandomEffect = rnorm(11, sd = rsigma)\n\nx &lt;- -27:27\ny &lt;- a * x + b + rnorm(55,0,sd = sigma) + randomEffect[group]\nplot(x,y, col = group, pch = 3)\n\n\n\n## ---- fig.width=5, fig.height=5------------------------------------------\nfit &lt;- lm(y ~ x)\nsummary(fit)\n\n\nCall:\nlm(formula = y ~ x)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-47.21 -21.53  -4.41  11.95  71.57 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  25.6079     4.0436   6.333 5.32e-08 ***\nx             4.5581     0.2547  17.894  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 29.99 on 53 degrees of freedom\nMultiple R-squared:  0.858, Adjusted R-squared:  0.8553 \nF-statistic: 320.2 on 1 and 53 DF,  p-value: &lt; 2.2e-16\n\nplot(allEffects(fit, partial.residuals = T))\n\n\n\n## ---- fig.width=5, fig.height=5------------------------------------------\nfit &lt;- lmer(y ~ x + (1|group))\nsummary(fit)\n\nLinear mixed model fit by REML ['lmerMod']\nFormula: y ~ x + (1 | group)\n\nREML criterion at convergence: 438.2\n\nScaled residuals: \n     Min       1Q   Median       3Q      Max \n-1.81013 -0.60007 -0.07453  0.70328  1.91655 \n\nRandom effects:\n Groups   Name        Variance Std.Dev.\n group    (Intercept) 925.64   30.424  \n Residual              89.63    9.467  \nNumber of obs: 55, groups:  group, 11\n\nFixed effects:\n            Estimate Std. Error t value\n(Intercept)  25.6079     9.2617   2.765\nx             4.6488     0.4914   9.461\n\nCorrelation of Fixed Effects:\n  (Intr)\nx 0.000 \n\nplot(x,y, col = group,  pch = 3)\nfor(i in 1:11){\n  abline(coef(fit)$group[i,1], coef(fit)$group[i,2], col = i)\n}\n\n\n\n## ------------------------------------------------------------------------\n  # 1) Model definition exactly how we created our data \n  modelCode = \"\n    model{\n      \n      # Likelihood\n      for(i in 1:i.max){\n        y[i] ~ dnorm(mu[i],tau)\n        mu[i] &lt;- a*x[i] + b\n      }\n\n      # Prior distributions\n      a ~ dnorm(0,0.001)\n      b ~ dnorm(0,0.001)\n      tau &lt;- 1/(sigma*sigma)\n      sigma ~ dunif(0,100)\n    }\n  \"\n  \n  # 2) Set up a list that contains all the necessary data (here, including parameters of the prior distribution)\n  Data = list(y = y, x = x, i.max = length(y))\n\n  # 3) Specify a function to generate inital values for the parameters\n  inits.fn &lt;- function() list(a = rnorm(1), b = rnorm(1), sigma = runif(1,1,100))\n\n\n## ---- fig.width=7, fig.height=7------------------------------------------\n  # Compile the model and run the MCMC for an adaptation (burn-in) phase\n  jagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, init = inits.fn, n.chains = 3, n.adapt= 1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 3\n   Total graph size: 230\n\nInitializing model\n\n  # Specify parameters for which posterior samples are saved\n  para.names &lt;- c(\"a\",\"b\",\"sigma\")\n\n  # Continue the MCMC runs with sampling\n  Samples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n  \n  # Plot the mcmc chain and the posterior sample for p\n  plot(Samples)\n\n\n\n  dic = dic.samples(jagsModel, n.iter = 5000)\n  dic\n\nMean deviance:  531.3 \npenalty 3.12 \nPenalized deviance: 534.4 \n\n## ------------------------------------------------------------------------\ngelman.diag(Samples)\n\nPotential scale reduction factors:\n\n      Point est. Upper C.I.\na              1          1\nb              1          1\nsigma          1          1\n\nMultivariate psrf\n\n1\n\n## ------------------------------------------------------------------------\nsummary(Samples)\n\n\nIterations = 1001:6000\nThinning interval = 1 \nNumber of chains = 3 \nSample size per chain = 5000 \n\n1. Empirical mean and standard deviation for each variable,\n   plus standard error of the mean:\n\n        Mean    SD Naive SE Time-series SE\na      4.556 0.261 0.002131       0.002111\nb     25.151 4.127 0.033694       0.033694\nsigma 30.703 3.038 0.024803       0.033976\n\n2. Quantiles for each variable:\n\n        2.5%    25%    50%   75%  97.5%\na      4.044  4.383  4.554  4.73  5.069\nb     16.946 22.427 25.159 27.93 33.239\nsigma 25.443 28.538 30.458 32.64 37.230\n\n## ---- fig.width=5, fig.height=5------------------------------------------\nplot(x,y)\nsampleMatrix &lt;- as.matrix(Samples)\nselection &lt;- sample(dim(sampleMatrix)[1], 1000)\nfor (i in selection) abline(sampleMatrix[i,1], sampleMatrix[i,1], col = \"#11111105\")\n\n\n\n\nAlternative: mixed model\n\n## ------------------------------------------------------------------------\n  # 1) Model definition exactly how we created our data \n  modelCode = \"\n    model{\n      \n      # Likelihood\n      for(i in 1:i.max){\n        y[i] ~ dnorm(mu[i],tau)\n        mu[i] &lt;- a*x[i] + b + r[group[i]]\n      }\n\n      # random effect\n      for(i in 1:nGroups){\n        r[i] ~ dnorm(0,rTau)\n      }\n\n      # Prior distributions\n      a ~ dnorm(0,0.001)\n      b ~ dnorm(0,0.001)\n\n      tau &lt;- 1/(sigma*sigma)\n      sigma ~ dunif(0,100)\n\n      rTau &lt;- 1/(rSigma*rSigma)\n      rSigma ~ dunif(0,100)\n    }\n  \"\n  \n  # 2) Set up a list that contains all the necessary data (here, including parameters of the prior distribution)\n  Data = list(y = y, x = x, i.max = length(y), group = group, nGroups = 11)\n\n  # 3) Specify a function to generate inital values for the parameters\n  inits.fn &lt;- function() list(a = rnorm(1), b = rnorm(1), sigma = runif(1,1,100), rSigma = runif(1,1,100))\n\n\n## ---- fig.width=7, fig.height=7------------------------------------------\n  # Compile the model and run the MCMC for an adaptation (burn-in) phase\n  jagsModel &lt;- jags.model(file= textConnection(modelCode), data=Data, init = inits.fn, n.chains = 3, n.adapt= 1000)\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 15\n   Total graph size: 300\n\nInitializing model\n\n  # Specify parameters for which posterior samples are saved\n  para.names &lt;- c(\"a\",\"b\",\"sigma\", \"rSigma\")\n\n  # Continue the MCMC runs with sampling\n  Samples &lt;- coda.samples(jagsModel, variable.names = para.names, n.iter = 5000)\n  \n  # Plot the mcmc chain and the posterior sample for p\n  plot(Samples)\n\n\n\n## ----  fig.width=18, fig.height=18---------------------------------------\nR2JagsResults &lt;- jags(data=Data, inits=inits.fn, parameters.to.save=c(\"a\",\"b\",\"sigma\", \"rSigma\", \"r\"), n.chains=3, n.iter=5000, model.file=textConnection(modelCode))\n\nCompiling model graph\n   Resolving undeclared variables\n   Allocating nodes\nGraph information:\n   Observed stochastic nodes: 55\n   Unobserved stochastic nodes: 15\n   Total graph size: 300\n\nInitializing model\n\nplot(R2JagsResults)\n\n\n\nprint(R2JagsResults)\n\nInference for Bugs model at \"8\", fit using jags,\n 3 chains, each with 5000 iterations (first 2500 discarded), n.thin = 2\n n.sims = 3750 iterations saved\n         mu.vect sd.vect    2.5%     25%     50%     75%   97.5%  Rhat n.eff\na          4.677   0.538   3.606   4.322   4.674   5.021   5.781 1.001  3800\nb         22.937  10.080   2.115  16.456  23.370  29.476  42.337 1.001  3800\nr[1]       6.099  17.237 -27.675  -5.190   5.841  17.011  41.579 1.001  3800\nr[2]      -9.529  15.256 -39.082 -19.577  -9.758   0.123  21.715 1.001  3800\nr[3]      47.123  13.504  21.074  38.305  46.654  55.792  74.498 1.001  3800\nr[4]     -35.103  12.021 -58.729 -42.698 -35.340 -27.654 -10.327 1.001  3800\nr[5]     -19.663  11.078 -40.985 -26.930 -19.774 -12.518   2.430 1.001  3800\nr[6]       4.013  10.884 -16.642  -3.142   3.759  10.816  26.540 1.001  3800\nr[7]      59.492  11.339  37.939  52.047  59.294  66.532  83.887 1.001  2700\nr[8]      -4.679  11.977 -28.086 -12.724  -4.851   2.696  19.435 1.001  3800\nr[9]       4.384  13.402 -21.492  -4.439   4.077  12.763  31.141 1.001  3800\nr[10]    -30.631  15.248 -61.410 -40.487 -30.964 -20.650  -0.380 1.001  3800\nr[11]      7.457  17.124 -27.732  -3.390   7.378  18.913  40.703 1.001  3800\nrSigma    34.947   9.763  21.354  28.142  33.184  39.484  59.266 1.001  3800\nsigma      9.760   1.096   7.902   8.986   9.654  10.408  12.220 1.003   820\ndeviance 405.020   5.778 395.856 400.952 404.168 408.333 418.513 1.002  1100\n\nFor each parameter, n.eff is a crude measure of effective sample size,\nand Rhat is the potential scale reduction factor (at convergence, Rhat=1).\n\nDIC info (using the rule, pD = var(deviance)/2)\npD = 16.7 and DIC = 421.7\nDIC is an estimate of expected predictive error (lower deviance is better).\n\ndic = dic.samples(jagsModel, n.iter = 5000)\ndic\n\nMean deviance:  405 \npenalty 12.78 \nPenalized deviance: 417.8 \n\n## ---- fig.width=14, fig.height=14, eval= F-------------------------------\n## R2JagsCoda &lt;- as.mcmc(R2JagsResults)\n## plot(R2JagsCoda)\n## summary(R2JagsCoda)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction to Bayesian Statistics with R",
    "section": "",
    "text": "Preface\nThis course provides a practical introduction to Bayesian inference covering both the theory and application of Bayesian methods using a number of examples motivated from the biological and environmental sciences, including"
  },
  {
    "objectID": "1A-GettingStarted.html#exercises-and-r-code-from-the-book",
    "href": "1A-GettingStarted.html#exercises-and-r-code-from-the-book",
    "title": "1  Getting Started",
    "section": "1.3 Exercises and R code from the book",
    "text": "1.3 Exercises and R code from the book\nIf you want to run code from the book, you can copy and paste it to your RStudio, but for your convenience, I also provide a plain R version of the code of each chapter is here.\nThe same folder holds some R scripts that are meant as classroom exercises, accompanying each chapter."
  },
  {
    "objectID": "6A-References.html",
    "href": "6A-References.html",
    "title": "Appendix A — References",
    "section": "",
    "text": "George, Edward I., and Robert McCulloch. 1993. “On Obtaining\nInvariant Prior Distributions.” Journal of Statistical\nPlanning and Inference 37 (2): 169–79. https://doi.org/10.1016/0378-3758(93)90086-L."
  },
  {
    "objectID": "index.html#what-is-the-goal-here",
    "href": "index.html#what-is-the-goal-here",
    "title": "Introduction to Bayesian Statistics with R",
    "section": "What is the goal here?",
    "text": "What is the goal here?\nThis course provides a practical introduction to Bayesian inference covering both the theory and application of Bayesian methods using a number of examples motivated from the biological and environmental sciences, including\n\nIntroduction to concepts of Bayesian statistics (Priors, Likelihoods, etc.)\nSampling methods (e.g. Markov Chain Monte Carlo) and model specification languages and frameworks (STAN, brms, BayesianTools)\nWorkflow of Bayesian inference, including model checks, model specification etc.\nBayesian model choice and model selection\nDiscussion of common hierarchical model structures, including mixed models, error in variable models, etc."
  },
  {
    "objectID": "index.html#organization-of-this-book",
    "href": "index.html#organization-of-this-book",
    "title": "Introduction to Bayesian Statistics with R",
    "section": "Organization of this book",
    "text": "Organization of this book\nThis book is organized in three parts:\n\nIntroduction and philosophy: The first part of this book provides a general introduction to Bayesian inference, starting with the internal logic (likelihood, prior, posterior), a short introduction on posterior estimation and interpretation, a section on Bayesian model selection and a overview of the Bayesian workflow\nBayesian GLMMs: The second part covers how standard GLMMs (which could also be fit in R packages lme4 or glmmTMB) would be implemented in a Bayesian worklow\nHierarchical models: The third part of the book shows examples of popular hierarchical model structures that may be the reason why you want to use Bayesian inference."
  },
  {
    "objectID": "index.html#assumed-knowledge",
    "href": "index.html#assumed-knowledge",
    "title": "Introduction to Bayesian Statistics with R",
    "section": "Assumed knowledge",
    "text": "Assumed knowledge\nThis material assumes prior knowledge of standard statistical methods and concepts (tests, regressions, p-value, power, CIs, …) and the ability to apply those in R. At the University of Regensburg, this knowledge would be taught in the Bachelors Biology Lecture “Statistik und Bioinformatik” (lecture notes in German here), and the block course “Introduction to statistics in R”. If you didn’t take those or comparable courses, you should at least try to get some basic understanding of R before proceeding with this book. On top of that, a good grasp of GLMMs would be helpful for following this course. As a reference, have at my lecture notes on for the course Advanced Regression Models."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Introduction to Bayesian Statistics with R",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nMuch of this material was inspired by a serious of courses and summer schools that I did in various combinations of people, but mostly with my colleagues Jörn Page, Joe Chipperfield and Björn Reineking. Those past courses inlcuded\n\nSept 2019 Münster, Germany\nFeb 2019 Bangkok, Thailand\nSept 2018 Bergen Norway\nApril 2018 Frankfurt, Germany\nSept 2017 Bergen Norway\nSept 2015 Bergen Norway\nLeipzig 2015,Germany\nBergen 2014, Norway\nFreiburg 2013, Germany\nGöttingen 2013, Germany\nBayreuth 2012, Germany\nBayreuth 2011, Germany\n\nIf you have comments, questions or suggestions regarding this book, please submit them here.\nThis work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License. Note that some elements of this work (embedded videos, graphics) may be under a seperate licence and are thus not included in this licence."
  },
  {
    "objectID": "2B-PosteriorEstimation.html#what-is-an-mcmc",
    "href": "2B-PosteriorEstimation.html#what-is-an-mcmc",
    "title": "3  Posterior estimation",
    "section": "3.1 What is an MCMC?",
    "text": "3.1 What is an MCMC?\nA Markov-Chain Monte-Carlo algorithm (MCMC) is an algorithm that jumps around in a density function (the so-called target function), in such a way that the probability to be at each point of the function is proportional to the target. To give you a simple example, let’s say we wouldn’t know how the normal distribution looks like. What you would then probably usually do is to calculate a value of the normal for a number of data points\n\nvalues = seq(-10,10,length.out = 100)\ndensity = dnorm(values)\nplot(density)\n\n\n\n\nTo produce the same picture with an MCMC sampler, we will use teh BayesianTools package. Here the code to sample from a normal distribution:\n\nlibrary(BayesianTools)\n\ndensity = function(x) dnorm(x, log = T) \nsetup = createBayesianSetup(density, lower = -10, upper = 10)\nout = runMCMC(setup, settings = list(iterations = 1000), sampler = \"Metropolis\")\n\nBT runMCMC: trying to find optimal start and covariance values \b\n\n\nBT runMCMC: Optimization finished, setting startValues to 1.4901160971803e-08  - Setting covariance to 0.99999713415048 \n\n\n\n Running Metropolis-MCMC, chain  iteration 100 of 1000 . Current logp:  -3.915425  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 200 of 1000 . Current logp:  -3.952539  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 300 of 1000 . Current logp:  -4.330099  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 400 of 1000 . Current logp:  -4.801967  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 500 of 1000 . Current logp:  -4.04227  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 600 of 1000 . Current logp:  -4.129271  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 700 of 1000 . Current logp:  -4.062493  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 800 of 1000 . Current logp:  -4.430568  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 900 of 1000 . Current logp:  -3.943009  Please wait! \n\n Running Metropolis-MCMC, chain  iteration 1000 of 1000 . Current logp:  -4.060668  Please wait! \n\n\nrunMCMC terminated after 0.181seconds\n\nplot(out)\n\n\n\n\nWhat we get as a result is the so-called trace plot to the left, which shows us how the sampler jumped around in parameter space over time, and the density plot to the right, which shows us results of sampling from the normal distribution.\nFor this simple case, this is not particularly impressive, and looks exactly like the plot that we coded above, using the seq approach. However, as discussed above, the first approach will break down if we have high-dimensional multivariate distributions, wheras MCMC sampling also works for high-dimensional problems.\n\n\n\n\n\n\nNote\n\n\n\nIf you are interested in how an MCMC sampler works internally, you can look at Appendix (appendix-BayesianNumerics?)."
  }
]